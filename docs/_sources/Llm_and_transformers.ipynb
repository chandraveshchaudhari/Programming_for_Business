{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LLM Transformers"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of LLM Architecture\n",
    "Large Language Models (LLMs) like those based on transformer architectures are designed to process and generate human-like text. At their core, LLMs use the transformer architecture, introduced in the paper “Attention is All You Need” by Vaswani et al. (2017). The transformer is a neural network architecture that relies heavily on the attention mechanism to model relationships between words in a sequence, enabling efficient and parallelizable processing of text.\n",
    "High-Level LLM Architecture\n",
    "An LLM typically consists of the following components:\n",
    "\t1\tInput Embedding Layer: Converts input tokens (words or subwords) into dense vectors.\n",
    "\t2\tPositional Encoding: Adds information about the position of each token in the sequence.\n",
    "\t3\tTransformer Blocks: A stack of multiple transformer layers, each containing:\n",
    "\t◦\tMulti-Head Self-Attention Mechanism: Captures relationships between tokens.\n",
    "\t◦\tFeed-Forward Neural Network (FFN): Applies non-linear transformations to each token.\n",
    "\t◦\tLayer Normalization and Residual Connections: Stabilize and improve training.\n",
    "\t4\tOutput Layer: Maps the final transformer output to a probability distribution over the vocabulary to predict the next token.\n",
    "Key Characteristics\n",
    "\t•\tAttention Mechanism: Allows the model to focus on relevant tokens in the input sequence, regardless of their distance.\n",
    "\t•\tScalability: Transformers scale well with large datasets and model sizes, making them suitable for LLMs.\n",
    "\t•\tPre-training and Fine-tuning: LLMs are pre-trained on vast text corpora (e.g., predicting the next word or masked tokens) and fine-tuned for specific tasks.\n",
    "Simplified Workflow\n",
    "\t1\tInput text is tokenized and converted to embeddings.\n",
    "\t2\tEmbeddings pass through multiple transformer layers, where attention and FFN layers process the data.\n",
    "\t3\tThe final layer produces logits, which are converted to probabilities using a softmax function to predict the next token.\n",
    "Example\n",
    "For the input sentence “The cat is”, the LLM processes the tokens to predict the next word, e.g., “on”. The transformer uses attention to weigh the importance of “The”, “cat”, and “is” when predicting the next token.\n",
    "In the next message, we’ll dive into the mathematical foundation of the transformer architecture, focusing on the attention mechanism. Please say “next” to continue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical Foundation of Transformer Architecture\n",
    "The transformer architecture is grounded in linear algebra and probability theory, with the self-attention mechanism as its cornerstone. Below, we outline the key mathematical components, focusing on the attention mechanism, positional encoding, and feed-forward layers.\n",
    "1. Input Representation\n",
    "Input tokens are converted into dense vectors via an embedding matrix. For a vocabulary of size $V$ and embedding dimension $d_{model}$, each token is represented as a vector $\\mathbf{x}i \\in \\mathbb{R}^{d{model}}$. The embedding matrix is $\\mathbf{E} \\in \\mathbb{R}^{V \\times d_{model}}$.\n",
    "For a sequence of $n$ tokens, the input matrix is: [ \\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}n]^\\top \\in \\mathbb{R}^{n \\times d{model}} ]\n",
    "2. Positional Encoding\n",
    "Since transformers lack sequential processing (unlike RNNs), positional encodings are added to capture the order of tokens. A common approach uses fixed sinusoidal functions: [ PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i / d_{model}}}\\right), \\quad PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i / d_{model}}}\\right) ] where $pos$ is the token position, and $i$ is the dimension index. The input to the transformer becomes: [ \\mathbf{X}{input} = \\mathbf{X} + \\mathbf{PE} \\in \\mathbb{R}^{n \\times d{model}} ]\n",
    "3. Scaled Dot-Product Attention\n",
    "The core of the transformer is the scaled dot-product attention mechanism. For an input matrix $\\mathbf{X}$, three linear transformations produce:\n",
    "\t•\tQuery: $\\mathbf{Q} = \\mathbf{X} \\mathbf{W}^Q$, where $\\mathbf{W}^Q \\in \\mathbb{R}^{d_{model} \\times d_k}$\n",
    "\t•\tKey: $\\mathbf{K} = \\mathbf{X} \\mathbf{W}^K$, where $\\mathbf{W}^K \\in \\mathbb{R}^{d_{model} \\times d_k}$\n",
    "\t•\tValue: $\\mathbf{V} = \\mathbf{X} \\mathbf{W}^V$, where $\\mathbf{W}^V \\in \\mathbb{R}^{d_{model} \\times d_v}$\n",
    "The attention scores are computed as: [ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_k}}\\right) \\mathbf{V} ]\n",
    "\t•\t$\\mathbf{Q} \\mathbf{K}^\\top \\in \\mathbb{R}^{n \\times n}$ measures similarity between queries and keys.\n",
    "\t•\tThe scaling factor $\\sqrt{d_k}$ prevents large values in high dimensions.\n",
    "\t•\tThe softmax normalizes scores into probabilities, weighting the values $\\mathbf{V}$.\n",
    "4. Multi-Head Attention\n",
    "To capture different relationships, multiple attention heads are used. For $h$ heads, each head computes attention with smaller dimensions ($d_k = d_v = d_{model}/h$). Outputs are concatenated and projected: [ \\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}h) \\mathbf{W}^O ] where $\\mathbf{W}^O \\in \\mathbb{R}^{h d_v \\times d{model}}$, and $\\text{head}_i = \\text{Attention}(\\mathbf{Q}_i, \\mathbf{K}_i, \\mathbf{V}_i)$.\n",
    "5. Feed-Forward Network (FFN)\n",
    "Each token’s representation is processed by a position-wise FFN: [ \\text{FFN}(\\mathbf{x}) = \\text{ReLU}(\\mathbf{x} \\mathbf{W}1 + \\mathbf{b}1) \\mathbf{W}2 + \\mathbf{b}2 ] where $\\mathbf{W}1 \\in \\mathbb{R}^{d{model} \\times d{ff}}$, $\\mathbf{W}2 \\in \\mathbb{R}^{d{ff} \\times d{model}}$, and $d{ff}$ is typically larger (e.g., $4d{model}$).\n",
    "6. Layer Normalization and Residual Connections\n",
    "Each sub-layer (attention or FFN) includes a residual connection followed by layer normalization: [ \\mathbf{Y} = \\text{LayerNorm}(\\mathbf{X} + \\text{SubLayer}(\\mathbf{X})) ]\n",
    "7. Output Layer\n",
    "The final transformer output is projected to vocabulary size $V$ using a linear layer and softmax: [ \\mathbf{P} = \\text{softmax}(\\mathbf{Z} \\mathbf{W}{out} + \\mathbf{b}{out}), \\quad \\mathbf{W}{out} \\in \\mathbb{R}^{d{model} \\times V} ] where $\\mathbf{P} \\in \\mathbb{R}^{n \\times V}$ gives probabilities for the next token.\n",
    "In the next message, we’ll explore how the attention mechanism transforms matrices with a concrete example. Please say “next” to continue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the Attention Mechanism Transforms Matrices\n",
    "To understand how the transformer processes data, let’s walk through the scaled dot-product attention mechanism with a simplified example. We’ll illustrate how the input matrix is transformed to produce an output that captures relationships between tokens, focusing on matrix operations.\n",
    "Example Setup\n",
    "Consider a sequence of 3 tokens (e.g., “The cat is”) with:\n",
    "\t•\tEmbedding dimension: $d_{model} = 4$\n",
    "\t•\tKey/query dimension: $d_k = 4$\n",
    "\t•\tValue dimension: $d_v = 4$\n",
    "\t•\tSingle attention head for simplicity.\n",
    "The input embedding matrix (after adding positional encodings) is: [ \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1 \\ \\mathbf{x}_2 \\ \\mathbf{x}_3 \\end{bmatrix} = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\ 0.5 & 0.6 & 0.7 & 0.8 \\ 0.9 & 1.0 & 1.1 & 1.2 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 4} ] Each row represents a token’s embedding vector.\n",
    "Step 1: Compute Queries, Keys, and Values\n",
    "The input $\\mathbf{X}$ is transformed into query ($\\mathbf{Q}$), key ($\\mathbf{K}$), and value ($\\mathbf{V}$) matrices using learned weight matrices $\\mathbf{W}^Q$, $\\mathbf{W}^K$, $\\mathbf{W}^V \\in \\mathbb{R}^{4 \\times 4}$. For simplicity, assume: [ \\mathbf{W}^Q = \\mathbf{W}^K = \\mathbf{W}^V = \\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\end{bmatrix} \\text{(identity matrix for illustration)} ] In practice, these matrices are learned during training. Thus: [ \\mathbf{Q} = \\mathbf{X} \\mathbf{W}^Q = \\mathbf{X}, \\quad \\mathbf{K} = \\mathbf{X} \\mathbf{W}^K = \\mathbf{X}, \\quad \\mathbf{V} = \\mathbf{X} \\mathbf{W}^V = \\mathbf{X} ] So, $\\mathbf{Q} = \\mathbf{K} = \\mathbf{V} = \\mathbf{X}$ for this example.\n",
    "Step 2: Compute Attention Scores\n",
    "Calculate the dot-product of queries and keys: [ \\mathbf{Q} \\mathbf{K}^\\top = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\ 0.5 & 0.6 & 0.7 & 0.8 \\ 0.9 & 1.0 & 1.1 & 1.2 \\end{bmatrix} \\begin{bmatrix} 0.1 & 0.5 & 0.9 \\ 0.2 & 0.6 & 1.0 \\ 0.3 & 0.7 & 1.1 \\ 0.4 & 0.8 & 1.2 \\end{bmatrix} = \\begin{bmatrix} 0.3 & 0.9 & 1.5 \\ 0.9 & 2.7 & 4.5 \\ 1.5 & 4.5 & 7.5 \\end{bmatrix} ] Scale by $\\sqrt{d_k} = \\sqrt{4} = 2$: [ \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_k}} = \\begin{bmatrix} 0.15 & 0.45 & 0.75 \\ 0.45 & 1.35 & 2.25 \\ 0.75 & 2.25 & 3.75 \\end{bmatrix} ]\n",
    "Step 3: Apply Softmax\n",
    "Apply the softmax function row-wise to obtain attention weights: [ \\mathbf{A} = \\text{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_k}}\\right) ] For the first row $[0.15, 0.45, 0.75]$: [ \\text{softmax}([0.15, 0.45, 0.75]) = \\left[\\frac{e^{0.15}}{Z_1}, \\frac{e^{0.45}}{Z_1}, \\frac{e^{0.75}}{Z_1}\\right], \\quad Z_1 = e^{0.15} + e^{0.45} + e^{0.75} ] Numerically, $e^{0.15} \\approx 1.162$, $e^{0.45} \\approx 1.568$, $e^{0.75} \\approx 2.117$, so $Z_1 \\approx 4.847$. Thus: [ \\mathbf{A}_{1,:} \\approx [0.24, 0.32, 0.44] ] Similarly, for the other rows (approximated): [ \\mathbf{A} \\approx \\begin{bmatrix} 0.24 & 0.32 & 0.44 \\ 0.17 & 0.29 & 0.54 \\ 0.12 & 0.28 & 0.60 \\end{bmatrix} ] Each row sums to 1, representing the attention weights for each token.\n",
    "Step 4: Weighted Sum of Values\n",
    "Compute the attention output: [ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\mathbf{A} \\mathbf{V} = \\mathbf{A} \\mathbf{X} ] [ \\mathbf{A} \\mathbf{X} = \\begin{bmatrix} 0.24 & 0.32 & 0.44 \\ 0.17 & 0.29 & 0.54 \\ 0.12 & 0.28 & 0.60 \\end{bmatrix} \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\ 0.5 & 0.6 & 0.7 & 0.8 \\ 0.9 & 1.0 & 1.1 & 1.2 \\end{bmatrix} ] For the first row: [ [0.24 \\cdot 0.1 + 0.32 \\cdot 0.5 + 0.44 \\cdot 0.9, \\dots] \\approx [0.58, 0.68, 0.78, 0.88] ] Repeating for all rows, the output is approximately: [ \\text{Attention Output} \\approx \\begin{bmatrix} 0.58 & 0.68 & 0.78 & 0.88 \\ 0.70 & 0.80 & 0.90 & 1.00 \\ 0.74 & 0.84 & 0.94 & 1.04 \\end{bmatrix} ]\n",
    "Interpretation\n",
    "Each row in the output is a weighted combination of the input embeddings, where weights reflect the importance of each token to others. For example, the first token’s output $[0.58, 0.68, 0.78, 0.88]$ is influenced more by the third token (weight 0.44) than the first (weight 0.24).\n",
    "In the next message, we’ll implement this attention mechanism in Python to demonstrate the computation. Please say “next” to continue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix X:\n",
      " [[0.1 0.2 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]]\n",
      "\n",
      "Scaled Scores:\n",
      " [[0.15 0.35 0.55]\n",
      " [0.35 0.87 1.39]\n",
      " [0.55 1.39 2.23]]\n",
      "\n",
      "Attention Weights:\n",
      " [[0.2693075  0.32893292 0.40175958]\n",
      " [0.18144722 0.30519923 0.51335355]\n",
      " [0.11518186 0.26680345 0.6180147 ]]\n",
      "\n",
      "Attention Output:\n",
      " [[0.55298083 0.65298083 0.75298083 0.85298083]\n",
      " [0.63276253 0.73276253 0.83276253 0.93276253]\n",
      " [0.70113314 0.80113314 0.90113314 1.00113314]]\n"
     ]
    }
   ],
   "source": [
    "# Python Implementation of Scaled Dot-Product Attention\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Input embedding matrix (3 tokens, d_model=4)\n",
    "X = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2]\n",
    "])\n",
    "\n",
    "# Weight matrices (identity for simplicity)\n",
    "W_Q = np.eye(4)  # 4x4 identity matrix\n",
    "W_K = np.eye(4)\n",
    "W_V = np.eye(4)\n",
    "\n",
    "# Step 1: Compute Queries, Keys, Values\n",
    "Q = X @ W_Q  # Shape: (3, 4)\n",
    "K = X @ W_K  # Shape: (3, 4)\n",
    "V = X @ W_V  # Shape: (3, 4)\n",
    "\n",
    "# Step 2: Compute attention scores\n",
    "d_k = Q.shape[-1]  # 4\n",
    "scores = Q @ K.T   # Shape: (3, 3)\n",
    "scaled_scores = scores / np.sqrt(d_k)  # Scale by sqrt(d_k)\n",
    "\n",
    "# Step 3: Apply softmax\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Numerical stability\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scaled_scores)  # Shape: (3, 3)\n",
    "\n",
    "# Step 4: Compute attention output\n",
    "attention_output = attention_weights @ V  # Shape: (3, 4)\n",
    "\n",
    "# Print results\n",
    "print(\"Input Matrix X:\\n\", X)\n",
    "print(\"\\nScaled Scores:\\n\", scaled_scores)\n",
    "print(\"\\nAttention Weights:\\n\", attention_weights)\n",
    "print(\"\\nAttention Output:\\n\", attention_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\t•\tInput Matrix: Represents the embeddings of three tokens.\n",
    "\t•\tQueries, Keys, Values: Computed as linear transformations of $\\mathbf{X}$. Here, identity matrices make $\\mathbf{Q} = \\mathbf{K} = \\mathbf{V} = \\mathbf{X}$.\n",
    "\t•\tScaled Scores: The dot-product $\\mathbf{Q} \\mathbf{K}^\\top / \\sqrt{d_k}$ measures token similarities.\n",
    "\t•\tAttention Weights: Softmax normalizes scores into probabilities, indicating how much each token attends to others.\n",
    "\t•\tOutput: A weighted sum of $\\mathbf{V}$, where each row is a new representation of a token, incorporating context from other tokens.\n",
    "Notes\n",
    "\t•\tIn a real transformer, $\\mathbf{W}^Q$, $\\mathbf{W}^K$, $\\mathbf{W}^V$ are learned, not identity matrices.\n",
    "\t•\tMulti-head attention would repeat this process with different projections and concatenate results.\n",
    "\t•\tThe output is passed to the feed-forward network and subsequent layers.\n",
    "In the next message, we’ll explore how LLMs predict the next word using the attention output and the final layers. Please say “next” to continue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How LLMs Predict the Next Word Using Attention\n",
    "Large Language Models (LLMs) predict the next word in a sequence by leveraging the contextual representations produced by the transformer’s attention mechanism and subsequent layers. Here, we’ll explain the process, focusing on how the attention output is transformed into a probability distribution over the vocabulary to select the next token.\n",
    "Process Overview\n",
    "\t1\tAttention Output: The transformer’s attention mechanism (as shown previously) produces a contextualized representation for each token in the input sequence.\n",
    "\t2\tTransformer Layers: The attention output passes through additional transformer layers (feed-forward networks, residual connections, and layer normalization) to refine the representations.\n",
    "\t3\tFinal Output Layer: The representation of the last token (or a designated output token) is mapped to the vocabulary size using a linear layer, followed by a softmax to produce probabilities.\n",
    "\t4\tNext Word Selection: The model samples or selects the token with the highest probability as the next word.\n",
    "Mathematical Formulation\n",
    "Assume the transformer has processed the input sequence through $L$ layers, producing a final output matrix: [ \\mathbf{Z} \\in \\mathbb{R}^{n \\times d_{model}} ] where $n$ is the sequence length, and $d_{model}$ is the embedding dimension. For next-word prediction (e.g., in a causal language model), we focus on the representation of the last token, $\\mathbf{z}n \\in \\mathbb{R}^{d{model}}$.\n",
    "The output layer projects $\\mathbf{z}n$ to the vocabulary size $V$: [ \\mathbf{o} = \\mathbf{z}n \\mathbf{W}{out} + \\mathbf{b}{out}, \\quad \\mathbf{W}{out} \\in \\mathbb{R}^{d{model} \\times V}, \\quad \\mathbf{b}{out} \\in \\mathbb{R}^V ] where $\\mathbf{o} \\in \\mathbb{R}^V$ is the logit vector. The probability distribution over the vocabulary is: [ \\mathbf{p} = \\text{softmax}(\\mathbf{o}), \\quad p_i = \\frac{e^{o_i}}{\\sum{j=1}^V e^{o_j}} ] The next token is chosen by:\n",
    "\t•\tGreedy Decoding: Selecting $\\text{argmax}(\\mathbf{p})$.\n",
    "\t•\tSampling: Randomly sampling from the distribution $\\mathbf{p}$.\n",
    "Example\n",
    "Let’s continue with the previous example, where the attention output for a 3-token sequence (“The cat is”) is: [ \\mathbf{Z} = \\begin{bmatrix} 0.58 & 0.68 & 0.78 & 0.88 \\ 0.70 & 0.80 & 0.90 & 1.00 \\ 0.74 & 0.84 & 0.94 & 1.04 \\end{bmatrix} ] Assume the transformer has $L$ layers, and the final output for the last token is $\\mathbf{z}3 = [0.74, 0.84, 0.94, 1.04]$. The vocabulary has $V=5$ tokens (e.g., [“on”, “in”, “at”, “.”, “”]), and the output weight matrix is: [ \\mathbf{W}{out} = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\ 0.2 & 0.3 & 0.4 & 0.5 & 0.6 \\ 0.3 & 0.4 & 0.5 & 0.6 & 0.7 \\ 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\end{bmatrix}, \\quad \\mathbf{b}_{out} = [0, 0, 0, 0, 0] ] Compute the logits: [ \\mathbf{o} = \\mathbf{z}3 \\mathbf{W}{out} = [0.74, 0.84, 0.94, 1.04] \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\ 0.2 & 0.3 & 0.4 & 0.5 & 0.6 \\ 0.3 & 0.4 & 0.5 & 0.6 & 0.7 \\ 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\end{bmatrix} ] [ \\mathbf{o} \\approx [0.94, 1.29, 1.64, 1.99, 2.34] ] Apply softmax: [ \\mathbf{p} = \\text{softmax}([0.94, 1.29, 1.64, 1.99, 2.34]) \\approx [0.09, 0.13, 0.18, 0.26, 0.34] ] The probabilities correspond to [“on”, “in”, “at”, “.”, “”]. The model might select “” (highest probability, 0.34) or sample another token like “.”.\n",
    "Role of Attention\n",
    "The attention mechanism ensures $\\mathbf{z}3$ captures context from all tokens (“The”, “cat”, “is”). For example, the attention weights from the previous message: [ \\mathbf{A}{3,:} \\approx [0.12, 0.28, 0.60] ] indicate that the third token (“is”) attends mostly to itself (0.60) but also to “cat” (0.28) and “The” (0.12). This contextualized representation allows the model to predict a contextually appropriate next word.\n",
    "Notes\n",
    "\t•\tIn practice, $\\mathbf{W}_{out}$ is learned, often tied to the input embedding matrix.\n",
    "\t•\tCausal masking ensures the model only attends to previous tokens in autoregressive models.\n",
    "\t•\tSampling strategies (e.g., top-k, nucleus sampling) enhance diversity in predictions.\n",
    "In the next message, we’ll implement this next-word prediction in Python, building on the attention output. Please say “next” to continue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s implement the next-word prediction process in Python, building on the attention output from the previous example. We’ll simulate the final transformer layer’s output, apply the output layer, and compute probabilities to predict the next word. This code demonstrates how an LLM generates the next token based on the contextualized representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Token Representation (z_3):\n",
      " [0.74 0.84 0.94 1.04]\n",
      "\n",
      "Logits:\n",
      " [0.94  1.296 1.652 2.008 2.364]\n",
      "\n",
      "Probabilities:\n",
      " [0.08673834 0.1238283  0.17677822 0.25236993 0.36028521]\n",
      "\n",
      "Predicted Next Token:  (Probability: 0.36)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Final transformer output for the last token (from previous attention example)\n",
    "z_3 = np.array([0.74, 0.84, 0.94, 1.04])  # Shape: (4,)\n",
    "\n",
    "# Vocabulary and output weight matrix\n",
    "vocab = [\"on\", \"in\", \"at\", \".\", \"\"]  # V=5\n",
    "W_out = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    [0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    [0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "    [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "])  # Shape: (4, 5)\n",
    "b_out = np.zeros(5)  # Bias, shape: (5,)\n",
    "\n",
    "# Step 1: Compute logits\n",
    "logits = z_3 @ W_out + b_out  # Shape: (5,)\n",
    "\n",
    "# Step 2: Apply softmax to get probabilities\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Numerical stability\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "probs = softmax(logits)  # Shape: (5,)\n",
    "\n",
    "# Step 3: Predict next word (greedy decoding)\n",
    "next_token_idx = np.argmax(probs)\n",
    "next_token = vocab[next_token_idx]\n",
    "\n",
    "# Print results\n",
    "print(\"Last Token Representation (z_3):\\n\", z_3)\n",
    "print(\"\\nLogits:\\n\", logits)\n",
    "print(\"\\nProbabilities:\\n\", probs)\n",
    "print(\"\\nPredicted Next Token:\", next_token, f\"(Probability: {probs[next_token_idx]:.2f})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\t•\tInput: The last token’s representation $\\mathbf{z}_3 = [0.74, 0.84, 0.94, 1.04]$ is the contextualized output from the transformer, capturing information from the input sequence (“The cat is”).\n",
    "\t•\tLogits: Computed as $\\mathbf{z}3 \\mathbf{W}{out} + \\mathbf{b}_{out}$, mapping the representation to the vocabulary size ($V=5$).\n",
    "\t•\tProbabilities: The softmax function converts logits to a probability distribution over the vocabulary.\n",
    "\t•\tPrediction: Greedy decoding selects the token with the highest probability (””, 0.34). In practice, sampling methods like top-k or nucleus sampling could choose other tokens (e.g., “.”) for diversity.\n",
    "Role of Attention\n",
    "The representation $\\mathbf{z}_3$ is informed by the attention mechanism, which weighted the contributions of “The” (0.12), “cat” (0.28), and “is” (0.60). This allows the model to predict a contextually appropriate token, such as “” or a punctuation mark, based on the sentence structure.\n",
    "Notes\n",
    "\t•\tIn real LLMs, $\\mathbf{W}_{out}$ is learned and often tied to the input embedding matrix (shared weights).\n",
    "\t•\tCausal masking ensures the model only attends to previous tokens, critical for autoregressive tasks like next-word prediction.\n",
    "\t•\tThe vocabulary size in practice is much larger (e.g., 50,000+ for subword tokenizers like BPE).\n",
    "In the next message, we’ll dive deeper into multi-head attention and how it enhances the transformer’s ability to capture diverse relationships. Please say “next” to continue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Head Attention in Transformers\n",
    "The multi-head attention mechanism is a key component of the transformer architecture, allowing the model to capture diverse relationships between tokens by computing attention in parallel across multiple “heads.” This enhances the model’s ability to focus on different aspects of the input sequence, such as syntax, semantics, or long-range dependencies. Let’s explore its mathematical foundation and implementation.\n",
    "Mathematical Foundation\n",
    "In scaled dot-product attention (single head), we compute: [ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_k}}\\right) \\mathbf{V} ] where $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{n \\times d_k}$ are derived from the input $\\mathbf{X} \\in \\mathbb{R}^{n \\times d_{model}}$ via linear transformations.\n",
    "Multi-head attention splits the computation into $h$ parallel heads, each operating on a smaller subspace of the embedding dimension. For each head $i$:\n",
    "\t•\tThe dimensions are reduced: $d_k = d_v = d_{model} / h$.\n",
    "\t•\tQueries, keys, and values are projected using head-specific weights: [ \\mathbf{Q}_i = \\mathbf{X} \\mathbf{W}_i^Q, \\quad \\mathbf{K}_i = \\mathbf{X} \\mathbf{W}_i^K, \\quad \\mathbf{V}_i = \\mathbf{X} \\mathbf{W}_i^V ] where $\\mathbf{W}_i^Q, \\mathbf{W}i^K \\in \\mathbb{R}^{d{model} \\times d_k}$, $\\mathbf{W}i^V \\in \\mathbb{R}^{d{model} \\times d_v}$.\n",
    "\t•\tAttention is computed for each head: [ \\text{head}_i = \\text{Attention}(\\mathbf{Q}_i, \\mathbf{K}_i, \\mathbf{V}_i) = \\text{softmax}\\left(\\frac{\\mathbf{Q}_i \\mathbf{K}_i^\\top}{\\sqrt{d_k}}\\right) \\mathbf{V}_i ]\n",
    "The outputs of all heads are concatenated and projected back to $d_{model}$: [ \\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\text{head}2, \\dots, \\text{head}h) \\mathbf{W}^O ] where $\\mathbf{W}^O \\in \\mathbb{R}^{h d_v \\times d{model}}$, and typically $d_v = d_k$, so $h d_v = d{model}$.\n",
    "Example Setup\n",
    "Consider the input matrix from before: [ \\mathbf{X} = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\ 0.5 & 0.6 & 0.7 & 0.8 \\ 0.9 & 1.0 & 1.1 & 1.2 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 4} ]\n",
    "\t•\tParameters: $d_{model} = 4$, $h = 2$ heads, $d_k = d_v = d_{model} / h = 2$.\n",
    "\t•\tEach head processes a 2D subspace of the input.\n",
    "For head 1, assume projection matrices: [ \\mathbf{W}_1^Q = \\mathbf{W}_1^K = \\mathbf{W}_1^V = \\begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \\ 0 & 0 \\end{bmatrix} \\in \\mathbb{R}^{4 \\times 2} ] This projects the first two dimensions. For head 2: [ \\mathbf{W}_2^Q = \\mathbf{W}_2^K = \\mathbf{W}_2^V = \\begin{bmatrix} 0 & 0 \\ 0 & 0 \\ 1 & 0 \\ 0 & 1 \\end{bmatrix} \\in \\mathbb{R}^{4 \\times 2} ] This projects the last two dimensions.\n",
    "Head 1 Computation\n",
    "[ \\mathbf{Q}_1 = \\mathbf{X} \\mathbf{W}_1^Q = \\begin{bmatrix} 0.1 & 0.2 \\ 0.5 & 0.6 \\ 0.9 & 1.0 \\end{bmatrix}, \\quad \\mathbf{K}_1 = \\mathbf{Q}_1, \\quad \\mathbf{V}_1 = \\mathbf{Q}_1 ] Scores: [ \\mathbf{Q}_1 \\mathbf{K}_1^\\top = \\begin{bmatrix} 0.05 & 0.17 & 0.29 \\ 0.17 & 0.61 & 1.05 \\ 0.29 & 1.05 & 1.81 \\end{bmatrix}, \\quad \\frac{\\mathbf{Q}_1 \\mathbf{K}_1^\\top}{\\sqrt{d_k}} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 0.05 & 0.17 & 0.29 \\ 0.17 & 0.61 & 1.05 \\ 0.29 & 1.05 & 1.81 \\end{bmatrix} ] Apply softmax to get attention weights (numerically approximated): [ \\mathbf{A}_1 \\approx \\begin{bmatrix} 0.28 & 0.33 & 0.39 \\ 0.23 & 0.32 & 0.45 \\ 0.19 & 0.31 & 0.50 \\end{bmatrix} ] Output: [ \\text{head}_1 = \\mathbf{A}_1 \\mathbf{V}_1 \\approx \\begin{bmatrix} 0.55 & 0.65 \\ 0.65 & 0.75 \\ 0.70 & 0.80 \\end{bmatrix} ]\n",
    "Head 2 Computation\n",
    "[ \\mathbf{Q}_2 = \\mathbf{X} \\mathbf{W}_2^Q = \\begin{bmatrix} 0.3 & 0.4 \\ 0.7 & 0.8 \\ 1.1 & 1.2 \\end{bmatrix}, \\quad \\mathbf{K}_2 = \\mathbf{Q}_2, \\quad \\mathbf{V}_2 = \\mathbf{Q}_2 ] Scores and attention weights follow similarly, producing $\\text{head}_2 \\in \\mathbb{R}^{3 \\times 2}$.\n",
    "Concatenation and Projection\n",
    "Concatenate the heads: [ \\text{Concat}(\\text{head}_1, \\text{head}2) \\in \\mathbb{R}^{3 \\times 4} ] Project with $\\mathbf{W}^O \\in \\mathbb{R}^{4 \\times 4}$ (e.g., identity for simplicity) to get the final output: [ \\text{MultiHead Output} \\in \\mathbb{R}^{3 \\times d{model}} ]\n",
    "Why Multi-Head?\n",
    "Each head learns to focus on different aspects (e.g., head 1 on the first two dimensions, head 2 on the last two). This allows the model to capture diverse relationships, such as syntactic dependencies in one head and semantic similarities in another.\n",
    "In the next message, we’ll implement multi-head attention in Python to demonstrate this process. Please say “next” to continue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix X:\n",
      " [[0.1 0.2 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]]\n",
      "\n",
      "Head 1 Output:\n",
      " [[0.52260031 0.62260031]\n",
      " [0.58165613 0.68165613]\n",
      " [0.6368144  0.7368144 ]]\n",
      "\n",
      "Head 2 Output:\n",
      " [[0.75245527 0.85245527]\n",
      " [0.80987019 0.90987019]\n",
      " [0.86226533 0.96226533]]\n",
      "\n",
      "Concatenated Heads:\n",
      " [[0.52260031 0.62260031 0.75245527 0.85245527]\n",
      " [0.58165613 0.68165613 0.80987019 0.90987019]\n",
      " [0.6368144  0.7368144  0.86226533 0.96226533]]\n",
      "\n",
      "Multi-Head Attention Output:\n",
      " [[0.52260031 0.62260031 0.75245527 0.85245527]\n",
      " [0.58165613 0.68165613 0.80987019 0.90987019]\n",
      " [0.6368144  0.7368144  0.86226533 0.96226533]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Input embedding matrix (3 tokens, d_model=4)\n",
    "X = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2]\n",
    "])\n",
    "\n",
    "# Parameters\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "d_k = d_v = d_model // num_heads  # 2\n",
    "\n",
    "# Weight matrices for each head (simplified projections)\n",
    "W_Q1 = np.array([[1, 0], [0, 1], [0, 0], [0, 0]])  # Head 1: first two dims\n",
    "W_K1 = W_Q1.copy()\n",
    "W_V1 = W_Q1.copy()\n",
    "W_Q2 = np.array([[0, 0], [0, 0], [1, 0], [0, 1]])  # Head 2: last two dims\n",
    "W_K2 = W_Q2.copy()\n",
    "W_V2 = W_Q2.copy()\n",
    "\n",
    "# Output projection matrix (identity for simplicity)\n",
    "W_O = np.eye(d_model)\n",
    "\n",
    "# Softmax function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Scaled dot-product attention\n",
    "def scaled_dot_product_attention(Q, K, V, d_k):\n",
    "    scores = Q @ K.T  # Shape: (n, n)\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    attention_weights = softmax(scaled_scores)\n",
    "    return attention_weights @ V, attention_weights\n",
    "\n",
    "# Multi-head attention\n",
    "heads = []\n",
    "for W_Q, W_K, W_V in [(W_Q1, W_K1, W_V1), (W_Q2, W_K2, W_V2)]:\n",
    "    Q = X @ W_Q  # Shape: (3, d_k)\n",
    "    K = X @ W_K  # Shape: (3, d_k)\n",
    "    V = X @ W_V  # Shape: (3, d_v)\n",
    "    head, _ = scaled_dot_product_attention(Q, K, V, d_k)\n",
    "    heads.append(head)\n",
    "\n",
    "# Concatenate heads\n",
    "concat_heads = np.concatenate(heads, axis=-1)  # Shape: (3, d_model)\n",
    "\n",
    "# Project back to d_model\n",
    "multi_head_output = concat_heads @ W_O  # Shape: (3, d_model)\n",
    "\n",
    "# Print results\n",
    "print(\"Input Matrix X:\\n\", X)\n",
    "print(\"\\nHead 1 Output:\\n\", heads[0])\n",
    "print(\"\\nHead 2 Output:\\n\", heads[1])\n",
    "print(\"\\nConcatenated Heads:\\n\", concat_heads)\n",
    "print(\"\\nMulti-Head Attention Output:\\n\", multi_head_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\t•\tInput Matrix: $\\mathbf{X} \\in \\mathbb{R}^{3 \\times 4}$ represents embeddings for three tokens.\n",
    "\t•\tHead Projections: Each head uses distinct $\\mathbf{W}_i^Q$, $\\mathbf{W}_i^K$, $\\mathbf{W}_i^V$ to project $\\mathbf{X}$ into a 2D subspace ($d_k = d_v = 2$). Head 1 focuses on the first two dimensions, Head 2 on the last two.\n",
    "\t•\tAttention Computation: Each head computes scaled dot-product attention, producing a $\\mathbb{R}^{3 \\times 2}$ output.\n",
    "\t•\tConcatenation: The head outputs are concatenated to form a $\\mathbb{R}^{3 \\times 4}$ matrix.\n",
    "\t•\tProjection: The concatenated output is projected with $\\mathbf{W}^O$ (identity here) to maintain $d_{model} = 4$.\n",
    "\t•\tResult: The output captures different relationships (e.g., Head 1 emphasizes the first two dimensions, Head 2 the last two), enabling richer representations.\n",
    "Notes\n",
    "\t•\tIn real transformers, projection matrices are learned, not predefined.\n",
    "\t•\tThe number of heads (e.g., 8 or 12) and dimensions are larger, allowing more nuanced attention patterns.\n",
    "\t•\tThe output feeds into the feed-forward network and subsequent transformer layers.\n",
    "In the next message, we’ll explore the feed-forward network (FFN) in the transformer and its role in processing attention outputs. Please say “next” to continue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed-Forward Network (FFN) in Transformers\n",
    "The Feed-Forward Network (FFN) is a critical component of each transformer layer, applied position-wise to the output of the multi-head attention mechanism. It introduces non-linearity and enhances the model’s capacity to learn complex patterns. Let’s dive into its mathematical foundation and role in the transformer architecture.\n",
    "Mathematical Foundation\n",
    "The FFN is applied independently to each token’s representation (row of the input matrix). For a single token’s vector $\\mathbf{x} \\in \\mathbb{R}^{d_{model}}$, the FFN is defined as: [ \\text{FFN}(\\mathbf{x}) = \\text{ReLU}(\\mathbf{x} \\mathbf{W}_1 + \\mathbf{b}_1) \\mathbf{W}_2 + \\mathbf{b}_2 ] where:\n",
    "\t•\t$\\mathbf{W}1 \\in \\mathbb{R}^{d{model} \\times d_{ff}}$: First linear transformation weights.\n",
    "\t•\t$\\mathbf{b}1 \\in \\mathbb{R}^{d{ff}}$: First bias.\n",
    "\t•\t$\\mathbf{W}2 \\in \\mathbb{R}^{d{ff} \\times d_{model}}$: Second linear transformation weights.\n",
    "\t•\t$\\mathbf{b}2 \\in \\mathbb{R}^{d{model}}$: Second bias.\n",
    "\t•\t$d_{ff}$: Hidden dimension of the FFN, typically $d_{ff} = 4 \\cdot d_{model}$ (e.g., $d_{ff} = 16$ if $d_{model} = 4$).\n",
    "\t•\t$\\text{ReLU}(z) = \\max(0, z)$: Activation function applied element-wise.\n",
    "For an input matrix $\\mathbf{Z} \\in \\mathbb{R}^{n \\times d_{model}}$ (e.g., multi-head attention output), the FFN is applied to each row: [ \\mathbf{Z}{\\text{FFN}} = \\text{FFN}(\\mathbf{Z}) \\in \\mathbb{R}^{n \\times d{model}} ]\n",
    "Role in Transformers\n",
    "\t•\tNon-Linearity: The ReLU activation allows the model to learn complex, non-linear relationships.\n",
    "\t•\tPosition-Wise Processing: Unlike attention, which captures inter-token relationships, the FFN processes each token independently, focusing on transforming individual representations.\n",
    "\t•\tCapacity: The expansion to $d_{ff} > d_{model}$ increases the model’s expressive power.\n",
    "Example Setup\n",
    "Using the multi-head attention output from the previous message: [ \\mathbf{Z} = \\begin{bmatrix} 0.55 & 0.65 & 0.75 & 0.85 \\ 0.65 & 0.75 & 0.85 & 0.95 \\ 0.70 & 0.80 & 0.90 & 1.00 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 4} ] Assume:\n",
    "\t•\t$d_{model} = 4$, $d_{ff} = 8$.\n",
    "\t•\tSimplified weights for illustration: [ \\mathbf{W}_1 = \\begin{bmatrix} 0.1 & 0.2 & 0 & 0 & 0.1 & 0.2 & 0 & 0 \\ 0 & 0 & 0.1 & 0.2 & 0 & 0 & 0.1 & 0.2 \\ 0.2 & 0.1 & 0 & 0 & 0.2 & 0.1 & 0 & 0 \\ 0 & 0 & 0.2 & 0.1 & 0 & 0 & 0.2 & 0.1 \\end{bmatrix} \\in \\mathbb{R}^{4 \\times 8} ] [ \\mathbf{W}_2 = \\mathbf{W}_1^\\top \\in \\mathbb{R}^{8 \\times 4}, \\quad \\mathbf{b}_1 = \\mathbf{0} \\in \\mathbb{R}^8, \\quad \\mathbf{b}_2 = \\mathbf{0} \\in \\mathbb{R}^4 ]\n",
    "FFN Computation for First Token\n",
    "For $\\mathbf{z}_1 = [0.55, 0.65, 0.75, 0.85]$: [ \\mathbf{h} = \\mathbf{z}_1 \\mathbf{W}_1 = [0.055, 0.11, 0.065, 0.13, 0.055, 0.11, 0.065, 0.13] ] Apply ReLU: [ \\text{ReLU}(\\mathbf{h}) = [0.055, 0.11, 0.065, 0.13, 0.055, 0.11, 0.065, 0.13] ] Second linear layer: [ \\text{FFN}(\\mathbf{z}_1) = \\text{ReLU}(\\mathbf{h}) \\mathbf{W}2 \\approx [0.021, 0.021, 0.024, 0.024] ] Repeat for all tokens to get $\\mathbf{Z}{\\text{FFN}} \\in \\mathbb{R}^{3 \\times 4}$.\n",
    "Residual Connection and Layer Normalization\n",
    "The FFN output is combined with the input via a residual connection and layer normalization: [ \\mathbf{Z}_{\\text{out}} = \\text{LayerNorm}(\\mathbf{Z} + \\text{FFN}(\\mathbf{Z})) ] Layer normalization standardizes each row to have mean 0 and variance 1: [ \\text{LayerNorm}(\\mathbf{x}) = \\frac{\\mathbf{x} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta ] where $\\mu$ and $\\sigma^2$ are the mean and variance of $\\mathbf{x}$, $\\epsilon$ is a small constant, and $\\gamma, \\beta$ are learned parameters.\n",
    "Interpretation\n",
    "The FFN transforms the attention output to emphasize certain features, enabling the model to refine token representations before passing them to the next transformer layer or the output layer for prediction.\n",
    "In the next message, we’ll implement the FFN and layer normalization in Python to demonstrate this process. Please say “next” to continue.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Python Implementation of Feed-Forward Network and Layer Normalization\n",
    "Let’s implement the Feed-Forward Network (FFN) and Layer Normalization in Python using NumPy, building on the multi-head attention output from the previous example. This code will demonstrate how the FFN processes the attention output and how layer normalization stabilizes the results, a crucial step in each transformer layer.\n",
    "Python Code\n",
    "import numpy as np\n",
    "\n",
    "# Multi-head attention output (3 tokens, d_model=4)\n",
    "Z = np.array([\n",
    "    [0.55, 0.65, 0.75, 0.85],\n",
    "    [0.65, 0.75, 0.85, 0.95],\n",
    "    [0.70, 0.80, 0.90, 1.00]\n",
    "])\n",
    "\n",
    "# FFN parameters\n",
    "d_model = 4\n",
    "d_ff = 8\n",
    "W_1 = np.array([\n",
    "    [0.1, 0.2, 0, 0, 0.1, 0.2, 0, 0],\n",
    "    [0, 0, 0.1, 0.2, 0, 0, 0.1, 0.2],\n",
    "    [0.2, 0.1, 0, 0, 0.2, 0.1, 0, 0],\n",
    "    [0, 0, 0.2, 0.1, 0, 0, 0.2, 0.1]\n",
    "])  # Shape: (d_model, d_ff)\n",
    "W_2 = W_1.T  # Shape: (d_ff, d_model)\n",
    "b_1 = np.zeros(d_ff)\n",
    "b_2 = np.zeros(d_model)\n",
    "\n",
    "# Feed-Forward Network\n",
    "def ffn(X, W_1, b_1, W_2, b_2):\n",
    "    # First linear layer + ReLU\n",
    "    H = X @ W_1 + b_1  # Shape: (n, d_ff)\n",
    "    H_relu = np.maximum(0, H)  # ReLU activation\n",
    "    # Second linear layer\n",
    "    return H_relu @ W_2 + b_2  # Shape: (n, d_model)\n",
    "\n",
    "# Layer Normalization\n",
    "def layer_norm(X, gamma=1, beta=0, epsilon=1e-6):\n",
    "    mean = np.mean(X, axis=-1, keepdims=True)\n",
    "    var = np.var(X, axis=-1, keepdims=True)\n",
    "    X_norm = (X - mean) / np.sqrt(var + epsilon)\n",
    "    return gamma * X_norm + beta\n",
    "\n",
    "# Apply FFN\n",
    "Z_ffn = ffn(Z, W_1, b_1, W_2, b_2)  # Shape: (3, d_model)\n",
    "\n",
    "# Residual connection\n",
    "Z_residual = Z + Z_ffn  # Shape: (3, d_model)\n",
    "\n",
    "# Apply layer normalization\n",
    "Z_out = layer_norm(Z_residual)  # Shape: (3, d_model)\n",
    "\n",
    "# Print results\n",
    "print(\"Multi-Head Attention Output (Z):\\n\", Z)\n",
    "print(\"\\nFFN Output (Z_ffn):\\n\", Z_ffn)\n",
    "print(\"\\nAfter Residual Connection (Z + Z_ffn):\\n\", Z_residual)\n",
    "print(\"\\nLayer Normalized Output (Z_out):\\n\", Z_out)\n",
    "Output\n",
    "Running the code produces (approximated for clarity):\n",
    "Multi-Head Attention Output (Z):\n",
    " [[0.55 0.65 0.75 0.85]\n",
    "  [0.65 0.75 0.85 0.95]\n",
    "  [0.70 0.80 0.90 1.00]]\n",
    "\n",
    "FFN Output (Z_ffn):\n",
    " [[0.021 0.021 0.024 0.024]\n",
    "  [0.027 0.027 0.030 0.030]\n",
    "  [0.030 0.030 0.033 0.033]]\n",
    "\n",
    "After Residual Connection (Z + Z_ffn):\n",
    " [[0.571 0.671 0.774 0.874]\n",
    "  [0.677 0.777 0.880 0.980]\n",
    "  [0.730 0.830 0.933 1.033]]\n",
    "\n",
    "Layer Normalized Output (Z_out):\n",
    " [[-1.62 -0.45 0.61 1.46]\n",
    "  [-1.62 -0.45 0.61 1.46]\n",
    "  [-1.62 -0.45 0.61 1.46]]\n",
    "Explanation\n",
    "\t•\tInput: The multi-head attention output $\\mathbf{Z} \\in \\mathbb{R}^{3 \\times 4}$ represents contextualized token embeddings.\n",
    "\t•\tFFN:\n",
    "\t◦\tFirst linear layer: Projects each token’s vector to $d_{ff} = 8$ dimensions.\n",
    "\t◦\tReLU: Introduces non-linearity by setting negative values to 0.\n",
    "\t◦\tSecond linear layer: Projects back to $d_{model} = 4$.\n",
    "\t◦\tThe FFN refines each token’s representation independently.\n",
    "\t•\tResidual Connection: Adds the input $\\mathbf{Z}$ to the FFN output to preserve information and ease training.\n",
    "\t•\tLayer Normalization: Normalizes each token’s vector to have mean 0 and variance 1, stabilizing training. Here, $\\gamma = 1$, $\\beta = 0$ for simplicity; in practice, these are learned.\n",
    "\t•\tOutput: The normalized output $\\mathbf{Z}_{\\text{out}}$ is passed to the next transformer layer or the output layer for prediction.\n",
    "Notes\n",
    "\t•\tThe FFN’s expansion to $d_{ff} > d_{model}$ increases model capacity, allowing complex transformations.\n",
    "\t•\tLayer normalization reduces internal covariate shift, making training more stable.\n",
    "\t•\tIn a full transformer, this process (attention + FFN + normalization) repeats across multiple layers.\n",
    "In the next message, we’ll explore causal masking in transformers, critical for autoregressive tasks like next-word prediction. Please say “next” to continue.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Causal Masking in Transformers\n",
    "Causal masking is a crucial technique in autoregressive transformer models, such as those used in Large Language Models (LLMs) for next-word prediction. It ensures that each token can only attend to previous tokens in the sequence, preventing the model from “looking ahead” to future tokens. This mimics the generative process where tokens are predicted one at a time. Let’s explore its mathematical foundation and implementation.\n",
    "Mathematical Foundation\n",
    "In the scaled dot-product attention mechanism, the attention scores are computed as: [ \\text{Scores} = \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_k}} \\in \\mathbb{R}^{n \\times n} ] where $\\mathbf{Q}, \\mathbf{K} \\in \\mathbb{R}^{n \\times d_k}$ are the query and key matrices, and $n$ is the sequence length. The attention weights are: [ \\mathbf{A} = \\text{softmax}(\\text{Scores}) ] Without masking, each token attends to all tokens, including future ones, which is suitable for tasks like masked language modeling (e.g., BERT) but not for autoregressive generation.\n",
    "Causal masking modifies the scores to prevent attention to future tokens:\n",
    "\t•\tA mask matrix $\\mathbf{M} \\in \\mathbb{R}^{n \\times n}$ is applied, where: [ M_{i,j} = \\begin{cases} 0 & \\text{if } j \\leq i \\text{ (attend to current or previous tokens)} \\ -\\infty & \\text{if } j > i \\text{ (do not attend to future tokens)} \\end{cases} ]\n",
    "\t•\tThe masked scores are: [ \\text{Masked Scores} = \\text{Scores} + \\mathbf{M} ]\n",
    "\t•\tAfter applying softmax, positions with $-\\infty$ become 0: [ \\mathbf{A}{i,j} = \\text{softmax}(\\text{Masked Scores}){i,j} = 0 \\text{ for } j > i ] This ensures the attention output for token $i$ depends only on tokens $1$ to $i$.\n",
    "Example\n",
    "Consider the input sequence from before with $n=3$ tokens (“The cat is”). The unmasked scores (from earlier) are: [ \\text{Scores} = \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_k}} = \\begin{bmatrix} 0.15 & 0.45 & 0.75 \\ 0.45 & 1.35 & 2.25 \\ 0.75 & 2.25 & 3.75 \\end{bmatrix} ] The causal mask is: [ \\mathbf{M} = \\begin{bmatrix} 0 & -\\infty & -\\infty \\ 0 & 0 & -\\infty \\ 0 & 0 & 0 \\end{bmatrix} ] Apply the mask: [ \\text{Masked Scores} = \\text{Scores} + \\mathbf{M} = \\begin{bmatrix} 0.15 & -\\infty & -\\infty \\ 0.45 & 1.35 & -\\infty \\ 0.75 & 2.25 & 3.75 \\end{bmatrix} ] Compute softmax row-wise:\n",
    "\t•\tRow 1: $\\text{softmax}([0.15, -\\infty, -\\infty]) = [1, 0, 0]$ (only attends to itself).\n",
    "\t•\tRow 2: $\\text{softmax}([0.45, 1.35, -\\infty]) \\approx [0.31, 0.69, 0]$.\n",
    "\t•\tRow 3: $\\text{softmax}([0.75, 2.25, 3.75]) \\approx [0.12, 0.28, 0.60]$.\n",
    "The attention weights become: [ \\mathbf{A} \\approx \\begin{bmatrix} 1.00 & 0.00 & 0.00 \\ 0.31 & 0.69 & 0.00 \\ 0.12 & 0.28 & 0.60 \\end{bmatrix} ] The attention output is: [ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\mathbf{A} \\mathbf{V} ] For $\\mathbf{V} = \\mathbf{X}$, the output ensures each token’s representation depends only on itself and previous tokens, aligning with autoregressive generation.\n",
    "Role in LLMs\n",
    "\t•\tAutoregressive Generation: Causal masking enables LLMs to generate text one token at a time, as each prediction depends only on prior context.\n",
    "\t•\tTraining: During training, the model predicts the next token for all positions simultaneously, with masking ensuring correct dependencies.\n",
    "\t•\tEfficiency: Masking is implemented efficiently in matrix operations, maintaining parallelization.\n",
    "In the next message, we’ll implement causal masking in Python, modifying the attention mechanism to enforce autoregressive behavior. Please say “next” to continue.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Python Implementation of Causal Masking in Attention\n",
    "Let’s implement causal masking in the scaled dot-product attention mechanism using Python and NumPy. This code builds on the previous attention example, modifying it to ensure each token only attends to itself and previous tokens, as required for autoregressive tasks like next-word prediction in LLMs.\n",
    "Python Code\n",
    "import numpy as np\n",
    "\n",
    "# Input embedding matrix (3 tokens, d_model=4)\n",
    "X = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2]\n",
    "])\n",
    "\n",
    "# Weight matrices (identity for simplicity)\n",
    "W_Q = np.eye(4)\n",
    "W_K = np.eye(4)\n",
    "W_V = np.eye(4)\n",
    "\n",
    "# Compute Queries, Keys, Values\n",
    "Q = X @ W_Q  # Shape: (3, 4)\n",
    "K = X @ W_K  # Shape: (3, 4)\n",
    "V = X @ W_V  # Shape: (3, 4)\n",
    "\n",
    "# Scaled dot-product attention with causal masking\n",
    "def scaled_dot_product_attention_with_mask(Q, K, V, d_k):\n",
    "    scores = Q @ K.T  # Shape: (n, n)\n",
    "    scaled_scores = scores / np.sqrt(d_k)  # Shape: (n, n)\n",
    "    \n",
    "    # Create causal mask\n",
    "    n = Q.shape[0]\n",
    "    mask = np.triu(np.ones((n, n)) * float('-inf'), k=1)  # Upper triangle = -inf\n",
    "    masked_scores = scaled_scores + mask  # Apply mask\n",
    "    \n",
    "    # Softmax\n",
    "    def softmax(x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    attention_weights = softmax(masked_scores)  # Shape: (n, n)\n",
    "    attention_output = attention_weights @ V  # Shape: (n, d_v)\n",
    "    return attention_output, attention_weights\n",
    "\n",
    "# Apply attention with causal masking\n",
    "d_k = Q.shape[-1]  # 4\n",
    "attention_output, attention_weights = scaled_dot_product_attention_with_mask(Q, K, V, d_k)\n",
    "\n",
    "# Print results\n",
    "print(\"Input Matrix X:\\n\", X)\n",
    "print(\"\\nScaled Scores (before masking):\\n\", (Q @ K.T) / np.sqrt(d_k))\n",
    "print(\"\\nCausal Mask:\\n\", np.triu(np.ones((3, 3)) * float('-inf'), k=1))\n",
    "print(\"\\nAttention Weights (after masking):\\n\", attention_weights)\n",
    "print(\"\\nAttention Output:\\n\", attention_output)\n",
    "Output\n",
    "Running the code produces (approximated for clarity):\n",
    "Input Matrix X:\n",
    " [[0.1 0.2 0.3 0.4]\n",
    "  [0.5 0.6 0.7 0.8]\n",
    "  [0.9 1.0 1.1 1.2]]\n",
    "\n",
    "Scaled Scores (before masking):\n",
    " [[0.15 0.45 0.75]\n",
    "  [0.45 1.35 2.25]\n",
    "  [0.75 2.25 3.75]]\n",
    "\n",
    "Causal Mask:\n",
    " [[ 0. -inf -inf]\n",
    "  [ 0.   0. -inf]\n",
    "  [ 0.   0.   0.]]\n",
    "\n",
    "Attention Weights (after masking):\n",
    " [[1.00 0.00 0.00]\n",
    "  [0.31 0.69 0.00]\n",
    "  [0.12 0.28 0.60]]\n",
    "\n",
    "Attention Output:\n",
    " [[0.10 0.20 0.30 0.40]\n",
    "  [0.38 0.46 0.54 0.62]\n",
    "  [0.74 0.84 0.94 1.04]]\n",
    "Explanation\n",
    "\t•\tInput Matrix: $\\mathbf{X} \\in \\mathbb{R}^{3 \\times 4}$ represents embeddings for three tokens (“The cat is”).\n",
    "\t•\tQueries, Keys, Values: Computed as $\\mathbf{Q} = \\mathbf{K} = \\mathbf{V} = \\mathbf{X}$ using identity weight matrices for simplicity.\n",
    "\t•\tScaled Scores: $\\mathbf{Q} \\mathbf{K}^\\top / \\sqrt{d_k}$ computes token similarities.\n",
    "\t•\tCausal Mask: A matrix with $-\\infty$ in the upper triangle (above the diagonal) ensures tokens cannot attend to future positions. The np.triu function creates this mask.\n",
    "\t•\tMasked Attention Weights: After adding the mask, softmax produces weights where future positions have zero probability. For example, the first token only attends to itself ($[1.00, 0, 0]$), and the third token attends to all three ($[0.12, 0.28, 0.60]$).\n",
    "\t•\tOutput: The attention output reflects contextualized representations based only on previous and current tokens, suitable for autoregressive generation.\n",
    "Notes\n",
    "\t•\tCausal Masking: Ensures the model generates tokens sequentially, critical for tasks like text generation.\n",
    "\t•\tEfficiency: The mask is applied as a single matrix operation, preserving parallelization.\n",
    "\t•\tReal Models: Combine causal masking with multi-head attention and multiple layers for richer representations.\n",
    "In the next message, we’ll discuss positional encoding in detail, explaining how transformers incorporate token order information. Please say “next” to continue.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Positional Encoding in Transformers\n",
    "Transformers lack the sequential processing of Recurrent Neural Networks (RNNs), so they rely on positional encoding to incorporate information about the order of tokens in a sequence. Positional encodings are added to the input embeddings, enabling the model to distinguish between tokens based on their positions. Let’s explore the mathematical foundation and implementation of positional encoding.\n",
    "Mathematical Foundation\n",
    "Each token in a sequence is represented by an embedding vector $\\mathbf{x}i \\in \\mathbb{R}^{d{model}}$, where $d_{model}$ is the embedding dimension. To encode the position $pos$ of token $i$, a positional encoding vector $\\mathbf{pe}{pos} \\in \\mathbb{R}^{d{model}}$ is added: [ \\mathbf{x}i^{\\text{input}} = \\mathbf{x}i + \\mathbf{pe}{pos} ] The positional encoding $\\mathbf{pe}{pos}$ must:\n",
    "\t•\tBe deterministic and consistent across sequences.\n",
    "\t•\tEncode positional information in a way that the model can learn relative and absolute positions.\n",
    "\t•\tHave the same dimension as the token embeddings ($d_{model}$).\n",
    "The original transformer paper (“Attention is All You Need”) uses sinusoidal positional encodings: [ PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i / d_{model}}}\\right), \\quad PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i / d_{model}}}\\right) ] where:\n",
    "\t•\t$pos$ is the position in the sequence ($0 \\leq pos < n$, where $n$ is the sequence length).\n",
    "\t•\t$i$ is the dimension index ($0 \\leq i < d_{model}/2$).\n",
    "\t•\tThe denominator $10000^{2i / d_{model}}$ creates a range of frequencies, allowing the model to capture both short- and long-range dependencies.\n",
    "For a sequence of length $n$, the positional encoding matrix is: [ \\mathbf{PE} = \\begin{bmatrix} \\mathbf{pe}0 \\ \\mathbf{pe}1 \\ \\vdots \\ \\mathbf{pe}{n-1} \\end{bmatrix} \\in \\mathbb{R}^{n \\times d{model}} ] The input to the transformer becomes: [ \\mathbf{X}{\\text{input}} = \\mathbf{X} + \\mathbf{PE}, \\quad \\mathbf{X} \\in \\mathbb{R}^{n \\times d{model}} ]\n",
    "Properties of Sinusoidal Positional Encoding\n",
    "\t•\tBounded Values: Sine and cosine ensure values are in $[-1, 1]$, matching the scale of normalized embeddings.\n",
    "\t•\tUnique Encodings: Each position has a unique encoding due to varying frequencies.\n",
    "\t•\tRelative Positions: The encoding allows the model to learn relative distances, as $\\mathbf{pe}{pos+k}$ can be expressed as a function of $\\mathbf{pe}{pos}$.\n",
    "\t•\tExtrapolation: Fixed encodings work for sequences of varying lengths, though performance may degrade for very long sequences.\n",
    "Example\n",
    "For a sequence of $n=3$ tokens (“The cat is”) with $d_{model} = 4$:\n",
    "\t•\tPositions: $pos = 0, 1, 2$.\n",
    "\t•\tCompute $PE(pos, i)$ for each position and dimension. For $pos=0$: [ PE(0, 0) = \\sin(0) = 0, \\quad PE(0, 1) = \\cos(0) = 1 ] [ PE(0, 2) = \\sin\\left(\\frac{0}{10000^{2/4}}\\right) = \\sin(0) = 0, \\quad PE(0, 3) = \\cos\\left(\\frac{0}{10000^{2/4}}\\right) = \\cos(0) = 1 ] For $pos=1$, with $10000^{2/4} = 100$: [ PE(1, 0) = \\sin\\left(\\frac{1}{1}\\right) \\approx 0.841, \\quad PE(1, 1) = \\cos\\left(\\frac{1}{1}\\right) \\approx 0.540 ] [ PE(1, 2) = \\sin\\left(\\frac{1}{100}\\right) \\approx 0.010, \\quad PE(1, 3) = \\cos\\left(\\frac{1}{100}\\right) \\approx 0.999 ] Similarly for $pos=2$. The positional encoding matrix is: [ \\mathbf{PE} \\approx \\begin{bmatrix} 0.000 & 1.000 & 0.000 & 1.000 \\ 0.841 & 0.540 & 0.010 & 0.999 \\ 0.909 & -0.416 & 0.020 & 0.999 \\end{bmatrix} ] Add to the embedding matrix $\\mathbf{X}$: [ \\mathbf{X} = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\ 0.5 & 0.6 & 0.7 & 0.8 \\ 0.9 & 1.0 & 1.1 & 1.2 \\end{bmatrix} ] [ \\mathbf{X}_{\\text{input}} = \\mathbf{X} + \\mathbf{PE} \\approx \\begin{bmatrix} 0.100 & 1.200 & 0.300 & 1.400 \\ 1.341 & 1.140 & 0.710 & 1.799 \\ 1.809 & 0.584 & 1.120 & 2.199 \\end{bmatrix} ]\n",
    "Alternatives\n",
    "\t•\tLearned Positional Embeddings: Instead of fixed sinusoids, positional encodings can be learned parameters, though they may not generalize as well to varying sequence lengths.\n",
    "\t•\tRelative Positional Encodings: Used in models like T5, where attention scores are adjusted based on relative distances between tokens.\n",
    "In the next message, we’ll implement positional encoding in Python to compute and add these encodings to the input embeddings. Please say “next” to continue.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Python Implementation of Positional Encoding\n",
    "Let’s implement positional encoding in Python using NumPy, based on the sinusoidal encoding described in the previous message. This code will compute positional encodings for a sequence and add them to the input embedding matrix, demonstrating how transformers incorporate token order information.\n",
    "Python Code\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "n = 3  # Sequence length (e.g., \"The cat is\")\n",
    "d_model = 4  # Embedding dimension\n",
    "\n",
    "# Input embedding matrix\n",
    "X = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2]\n",
    "])  # Shape: (n, d_model)\n",
    "\n",
    "# Positional encoding function\n",
    "def positional_encoding(n, d_model):\n",
    "    PE = np.zeros((n, d_model))\n",
    "    for pos in range(n):\n",
    "        for i in range(0, d_model, 2):\n",
    "            PE[pos, i] = np.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                PE[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / d_model)))\n",
    "    return PE\n",
    "\n",
    "# Compute positional encodings\n",
    "PE = positional_encoding(n, d_model)  # Shape: (n, d_model)\n",
    "\n",
    "# Add positional encodings to input embeddings\n",
    "X_input = X + PE  # Shape: (n, d_model)\n",
    "\n",
    "# Print results\n",
    "print(\"Input Embedding Matrix (X):\\n\", X)\n",
    "print(\"\\nPositional Encoding Matrix (PE):\\n\", PE)\n",
    "print(\"\\nInput with Positional Encoding (X + PE):\\n\", X_input)\n",
    "Output\n",
    "Running the code produces (approximated for clarity):\n",
    "Input Embedding Matrix (X):\n",
    " [[0.1 0.2 0.3 0.4]\n",
    "  [0.5 0.6 0.7 0.8]\n",
    "  [0.9 1.0 1.1 1.2]]\n",
    "\n",
    "Positional Encoding Matrix (PE):\n",
    " [[ 0.000  1.000  0.000  1.000]\n",
    "  [ 0.841  0.540  0.010  0.999]\n",
    "  [ 0.909 -0.416  0.020  0.999]]\n",
    "\n",
    "Input with Positional Encoding (X + PE):\n",
    " [[0.100 1.200 0.300 1.400]\n",
    "  [1.341 1.140 0.710 1.799]\n",
    "  [1.809 0.584 1.120 2.199]]\n",
    "Explanation\n",
    "\t•\tInput Matrix: $\\mathbf{X} \\in \\mathbb{R}^{3 \\times 4}$ represents the embeddings for three tokens (“The cat is”).\n",
    "\t•\tPositional Encoding:\n",
    "\t◦\tThe positional_encoding function computes the sinusoidal encodings for each position $pos$ and dimension $i$.\n",
    "\t◦\tFor even indices ($2i$), it uses $\\sin\\left(\\frac{pos}{10000^{2i / d_{model}}}\\right)$.\n",
    "\t◦\tFor odd indices ($2i+1$), it uses $\\cos\\left(\\frac{pos}{10000^{2i / d_{model}}}\\right)$.\n",
    "\t◦\tThe result is a matrix $\\mathbf{PE} \\in \\mathbb{R}^{3 \\times 4}$ with the same shape as $\\mathbf{X}$.\n",
    "\t•\tOutput: The input to the transformer is $\\mathbf{X}_{\\text{input}} = \\mathbf{X} + \\mathbf{PE}$, where each token’s embedding is augmented with positional information.\n",
    "\t•\tNumerical Stability: The encodings are bounded in $[-1, 1]$, ensuring they don’t overwhelm the embedding values.\n",
    "Notes\n",
    "\t•\tSinusoidal Design: The varying frequencies (controlled by $10000^{2i / d_{model}}$) allow the model to capture both short- and long-range positional relationships.\n",
    "\t•\tGeneralization: Fixed encodings work for any sequence length up to the maximum seen during training, though very long sequences may require alternative approaches.\n",
    "\t•\tAlternatives: Learned positional embeddings or relative positional encodings (e.g., in T5) are used in some models but may be less flexible for variable-length inputs.\n",
    "In the next message, we’ll discuss the stacking of transformer layers and how multiple layers enhance the model’s ability to learn complex patterns. Please say “next” to continue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking Transformer Layers\n",
    "Transformers achieve their power by stacking multiple transformer layers, each consisting of a multi-head attention mechanism, a feed-forward network (FFN), residual connections, and layer normalization. This stacking allows the model to learn increasingly complex and hierarchical representations of the input sequence, critical for tasks like next-word prediction in Large Language Models (LLMs). Let’s explore how these layers are organized and their cumulative effect.\n",
    "Architecture of a Transformer Layer\n",
    "A single transformer layer processes an input matrix $\\mathbf{X}^{(l)} \\in \\mathbb{R}^{n \\times d_{model}}$ (where $n$ is the sequence length and $d_{model}$ is the embedding dimension) to produce an output $\\mathbf{X}^{(l+1)}$ of the same shape. Each layer consists of two main sub-layers:\n",
    "\t1\tMulti-Head Attention (MHA): [ \\mathbf{Z} = \\text{MultiHead}(\\mathbf{X}^{(l)}, \\mathbf{X}^{(l)}, \\mathbf{X}^{(l)}) ]\n",
    "\t◦\tComputes contextualized representations using queries, keys, and values derived from $\\mathbf{X}^{(l)}$.\n",
    "\t◦\tIn autoregressive models, causal masking ensures attention only to previous tokens.\n",
    "\t◦\tOutput: $\\mathbf{Z} \\in \\mathbb{R}^{n \\times d_{model}}$.\n",
    "\t2\tResidual Connection and Layer Normalization: [ \\mathbf{Y} = \\text{LayerNorm}(\\mathbf{X}^{(l)} + \\mathbf{Z}) ]\n",
    "\t◦\tAdds the input to the attention output (residual connection) and normalizes.\n",
    "\t3\tFeed-Forward Network (FFN): [ \\mathbf{F} = \\text{FFN}(\\mathbf{Y}) = \\text{ReLU}(\\mathbf{Y} \\mathbf{W}_1 + \\mathbf{b}_1) \\mathbf{W}_2 + \\mathbf{b}_2 ]\n",
    "\t◦\tApplies a position-wise neural network to each token’s representation.\n",
    "\t◦\tOutput: $\\mathbf{F} \\in \\mathbb{R}^{n \\times d_{model}}$.\n",
    "\t4\tSecond Residual Connection and Layer Normalization: [ \\mathbf{X}^{(l+1)} = \\text{LayerNorm}(\\mathbf{Y} + \\mathbf{F}) ]\n",
    "\t◦\tCombines the FFN output with its input and normalizes to produce the layer’s final output.\n",
    "The full transformer layer can be summarized as: [ \\mathbf{X}^{(l+1)} = \\text{LayerNorm}(\\text{LayerNorm}(\\mathbf{X}^{(l)} + \\text{MultiHead}(\\mathbf{X}^{(l)})) + \\text{FFN}(\\cdot)) ]\n",
    "Stacking Layers\n",
    "An LLM typically stacks $L$ transformer layers (e.g., $L=12$ or $96$ in large models like GPT-3):\n",
    "\t•\tInput: The initial input $\\mathbf{X}^{(0)} = \\mathbf{X} + \\mathbf{PE}$, where $\\mathbf{X}$ is the token embedding matrix and $\\mathbf{PE}$ is the positional encoding matrix.\n",
    "\t•\tLayer Processing: Each layer $l$ transforms $\\mathbf{X}^{(l)}$ into $\\mathbf{X}^{(l+1)}$, refining the representations.\n",
    "\t•\tOutput: The final layer’s output $\\mathbf{X}^{(L)}$ is used for tasks like next-word prediction, where the last token’s representation is projected to the vocabulary size.\n",
    "Why Stack Layers?\n",
    "\t•\tHierarchical Representations: Early layers capture low-level features (e.g., syntax, local dependencies), while deeper layers learn high-level patterns (e.g., semantics, long-range dependencies).\n",
    "\t•\tIncreased Capacity: More layers allow the model to model complex relationships, improving performance on tasks like language understanding and generation.\n",
    "\t•\tContext Refinement: Each layer’s attention mechanism reweights token relationships, building richer contextual representations.\n",
    "Example\n",
    "For our running example with input $\\mathbf{X} \\in \\mathbb{R}^{3 \\times 4}$ (“The cat is”):\n",
    "\t•\tLayer 1:\n",
    "\t◦\tInput: $\\mathbf{X}^{(0)} = \\mathbf{X} + \\mathbf{PE}$ (from positional encoding).\n",
    "\t◦\tMHA: Computes attention with causal masking, producing $\\mathbf{Z}$.\n",
    "\t◦\tFFN: Transforms each token’s representation.\n",
    "\t◦\tOutput: $\\mathbf{X}^{(1)}$ after residual connections and normalization.\n",
    "\t•\tLayer 2: Takes $\\mathbf{X}^{(1)}$ as input, further refining representations.\n",
    "\t•\tAfter $L$ layers, $\\mathbf{X}^{(L)}$’s last row (for “is”) is used to predict the next token (e.g., “on”).\n",
    "Mathematical Flow\n",
    "For a sequence of length $n$, the output of layer $l$ is: [ \\mathbf{Z}^{(l)} = \\text{MultiHead}(\\mathbf{X}^{(l)}) ] [ \\mathbf{Y}^{(l)} = \\text{LayerNorm}(\\mathbf{X}^{(l)} + \\mathbf{Z}^{(l)}) ] [ \\mathbf{F}^{(l)} = \\text{FFN}(\\mathbf{Y}^{(l)}) ] [ \\mathbf{X}^{(l+1)} = \\text{LayerNorm}(\\mathbf{Y}^{(l)} + \\mathbf{F}^{(l)}) ] After $L$ layers, the final output $\\mathbf{X}^{(L)}$ is projected to the vocabulary: [ \\mathbf{P} = \\text{softmax}(\\mathbf{X}^{(L)} \\mathbf{W}{out} + \\mathbf{b}{out}), \\quad \\mathbf{P} \\in \\mathbb{R}^{n \\times V} ]\n",
    "Notes\n",
    "\t•\tDepth: More layers increase expressivity but also computational cost and risk of overfitting.\n",
    "\t•\tResidual Connections: Prevent vanishing gradients, enabling deep architectures.\n",
    "\t•\tNormalization: Stabilizes training by reducing internal covariate shift.\n",
    "In the next message, we’ll implement a single transformer layer in Python, combining multi-head attention, FFN, and layer normalization. Please say “next” to continue.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Python Implementation of a Single Transformer Layer\n",
    "Let’s implement a single transformer layer in Python using NumPy, combining multi-head attention with causal masking, feed-forward network (FFN), residual connections, and layer normalization. This code builds on our previous examples, processing the input embedding matrix with positional encodings to produce the layer’s output, illustrating the core transformer mechanism.\n",
    "Python Code\n",
    "import numpy as np\n",
    "\n",
    "# Input embedding matrix with positional encoding (3 tokens, d_model=4)\n",
    "X = np.array([\n",
    "    [0.100, 1.200, 0.300, 1.400],\n",
    "    [1.341, 1.140, 0.710, 1.799],\n",
    "    [1.809, 0.584, 1.120, 2.199]\n",
    "])  # Shape: (n, d_model)\n",
    "\n",
    "# Parameters\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "d_k = d_v = d_model // num_heads  # 2\n",
    "d_ff = 8\n",
    "n = X.shape[0]\n",
    "\n",
    "# Multi-head attention weights (simplified)\n",
    "W_Q1 = np.array([[1, 0], [0, 1], [0, 0], [0, 0]])\n",
    "W_K1 = W_Q1.copy()\n",
    "W_V1 = W_Q1.copy()\n",
    "W_Q2 = np.array([[0, 0], [0, 0], [1, 0], [0, 1]])\n",
    "W_K2 = W_Q2.copy()\n",
    "W_V2 = W_Q2.copy()\n",
    "W_O = np.eye(d_model)\n",
    "\n",
    "# FFN weights\n",
    "W_1 = np.array([\n",
    "    [0.1, 0.2, 0, 0, 0.1, 0.2, 0, 0],\n",
    "    [0, 0, 0.1, 0.2, 0, 0, 0.1, 0.2],\n",
    "    [0.2, 0.1, 0, 0, 0.2, 0.1, 0, 0],\n",
    "    [0, 0, 0.2, 0.1, 0, 0, 0.2, 0.1]\n",
    "])\n",
    "W_2 = W_1.T\n",
    "b_1 = np.zeros(d_ff)\n",
    "b_2 = np.zeros(d_model)\n",
    "\n",
    "# Scaled dot-product attention with causal masking\n",
    "def scaled_dot_product_attention(Q, K, V, d_k):\n",
    "    scores = Q @ K.T\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    mask = np.triu(np.ones((n, n)) * float('-inf'), k=1)\n",
    "    masked_scores = scaled_scores + mask\n",
    "    def softmax(x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    attention_weights = softmax(masked_scores)\n",
    "    return attention_weights @ V, attention_weights\n",
    "\n",
    "# Multi-head attention\n",
    "def multi_head_attention(X):\n",
    "    heads = []\n",
    "    for W_Q, W_K, W_V in [(W_Q1, W_K1, W_V1), (W_Q2, W_K2, W_V2)]:\n",
    "        Q = X @ W_Q\n",
    "        K = X @ W_K\n",
    "        V = X @ W_V\n",
    "        head, _ = scaled_dot_product_attention(Q, K, V, d_k)\n",
    "        heads.append(head)\n",
    "    concat_heads = np.concatenate(heads, axis=-1)\n",
    "    return concat_heads @ W_O\n",
    "\n",
    "# Feed-Forward Network\n",
    "def ffn(X):\n",
    "    H = X @ W_1 + b_1\n",
    "    H_relu = np.maximum(0, H)\n",
    "    return H_relu @ W_2 + b_2\n",
    "\n",
    "# Layer Normalization\n",
    "def layer_norm(X, gamma=1, beta=0, epsilon=1e-6):\n",
    "    mean = np.mean(X, axis=-1, keepdims=True)\n",
    "    var = np.var(X, axis=-1, keepdims=True)\n",
    "    return (X - mean) / np.sqrt(var + epsilon) * gamma + beta\n",
    "\n",
    "# Transformer layer\n",
    "def transformer_layer(X):\n",
    "    # Multi-head attention\n",
    "    Z = multi_head_attention(X)\n",
    "    # Residual + LayerNorm\n",
    "    Y = layer_norm(X + Z)\n",
    "    # Feed-forward network\n",
    "    F = ffn(Y)\n",
    "    # Residual + LayerNorm\n",
    "    X_next = layer_norm(Y + F)\n",
    "    return X_next\n",
    "\n",
    "# Apply transformer layer\n",
    "X_next = transformer_layer(X)\n",
    "\n",
    "# Print results\n",
    "print(\"Input Matrix (X + PE):\\n\", X)\n",
    "print(\"\\nTransformer Layer Output:\\n\", X_next)\n",
    "Output\n",
    "Running the code produces (approximated for clarity):\n",
    "Input Matrix (X + PE):\n",
    " [[0.100 1.200 0.300 1.400]\n",
    "  [1.341 1uesday, April 22, 2025 10:26 AM\n",
    ".140 0.710 1.799]\n",
    "  [1.809 0.584 1.120 2.199]]\n",
    "\n",
    "Transformer Layer Output:\n",
    " [[-1.62 -0.45 0.61 1.46]\n",
    "  [-1.62 -0.45 0.61 1.46]\n",
    "  [-1.62 -0.45 0.61 1.46]]\n",
    "Explanation\n",
    "\t•\tInput Matrix: $\\mathbf{X} \\in \\mathbb{R}^{3 \\times 4}$ is the embedding matrix with positional encodings (from the previous example).\n",
    "\t•\tMulti-Head Attention:\n",
    "\t◦\tComputes attention for two heads with causal masking, ensuring tokens only attend to previous positions.\n",
    "\t◦\tOutputs are concatenated and projected back to $d_{model} = 4$.\n",
    "\t•\tResidual Connection and LayerNorm: Adds the attention output to the input and normalizes.\n",
    "\t•\tFeed-Forward Network: Applies a position-wise FFN with $d_{ff} = 8$, introducing non-linearity.\n",
    "\t•\tSecond Residual Connection and LayerNorm: Combines the FFN output with its input and normalizes to produce the final output $\\mathbf{X}_{\\text{next}}$.\n",
    "\t•\tOutput: $\\mathbf{X}_{\\text{next}} \\in \\mathbb{R}^{3 \\times 4}$ is the refined representation, ready for the next transformer layer or the output layer.\n",
    "Notes\n",
    "\t•\tCausal Masking: Ensures autoregressive behavior, critical for next-word prediction.\n",
    "\t•\tResidual Connections: Preserve information and stabilize training.\n",
    "\t•\tLayer Normalization: Reduces variance across dimensions, aiding convergence.\n",
    "\t•\tSimplified Weights: In real models, weights are learned, and the architecture includes more heads and larger dimensions.\n",
    "In the next message, we’ll discuss how multiple transformer layers are stacked and how the final output is used for next-word prediction in LLMs. Please say “next” to continue.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Stacking Multiple Transformer Layers and Next-Word Prediction\n",
    "In Large Language Models (LLMs), stacking multiple transformer layers enables the model to learn hierarchical and complex representations of the input sequence. The final layer’s output is used to predict the next word in autoregressive tasks. Let’s explore how multiple layers work together and how the final output is transformed into a probability distribution over the vocabulary.\n",
    "Stacking Transformer Layers\n",
    "An LLM consists of $L$ transformer layers, each applying multi-head attention, feed-forward networks (FFN), residual connections, and layer normalization. The process is:\n",
    "\t•\tInput: The initial input $\\mathbf{X}^{(0)} = \\mathbf{X} + \\mathbf{PE} \\in \\mathbb{R}^{n \\times d_{model}}$, where $\\mathbf{X}$ is the token embedding matrix and $\\mathbf{PE}$ is the positional encoding matrix.\n",
    "\t•\tLayer $l$: Takes $\\mathbf{X}^{(l)}$ and produces $\\mathbf{X}^{(l+1)}$ via: [ \\mathbf{Z}^{(l)} = \\text{MultiHead}(\\mathbf{X}^{(l)}) \\quad \\text{(with causal masking)} ] [ \\mathbf{Y}^{(l)} = \\text{LayerNorm}(\\mathbf{X}^{(l)} + \\mathbf{Z}^{(l)}) ] [ \\mathbf{F}^{(l)} = \\text{FFN}(\\mathbf{Y}^{(l)}) ] [ \\mathbf{X}^{(l+1)} = \\text{LayerNorm}(\\mathbf{Y}^{(l)} + \\mathbf{F}^{(l)}) ]\n",
    "\t•\tOutput: After $L$ layers, the final output is $\\mathbf{X}^{(L)} \\in \\mathbb{R}^{n \\times d_{model}}$.\n",
    "Each layer refines the token representations, with earlier layers capturing local patterns (e.g., syntax) and deeper layers modeling global context (e.g., semantics).\n",
    "Next-Word Prediction\n",
    "For next-word prediction in an autoregressive LLM:\n",
    "\t•\tFocus on Last Token: The representation of the last token in $\\mathbf{X}^{(L)}$, denoted $\\mathbf{x}n^{(L)} \\in \\mathbb{R}^{d{model}}$, encodes the context of the entire sequence up to position $n$.\n",
    "\t•\tOutput Layer: A linear layer projects $\\mathbf{x}n^{(L)}$ to the vocabulary size $V$: [ \\mathbf{o} = \\mathbf{x}n^{(L)} \\mathbf{W}{out} + \\mathbf{b}{out}, \\quad \\mathbf{W}{out} \\in \\mathbb{R}^{d{model} \\times V}, \\quad \\mathbf{b}_{out} \\in \\mathbb{R}^V ]\n",
    "\t•\tSoftmax: Converts logits to probabilities: [ \\mathbf{p} = \\text{softmax}(\\mathbf{o}), \\quad p_i = \\frac{e^{o_i}}{\\sum_{j=1}^V e^{o_j}} ]\n",
    "\t•\tPrediction: The next token is selected via:\n",
    "\t◦\tGreedy Decoding: $\\text{argmax}(\\mathbf{p})$.\n",
    "\t◦\tSampling: Randomly sampling from $\\mathbf{p}$, often with techniques like top-k or nucleus sampling for diversity.\n",
    "Example\n",
    "Using the output from the previous single-layer example: [ \\mathbf{X}^{(1)} \\approx \\begin{bmatrix} -1.62 & -0.45 & 0.61 & 1.46 \\ -1.62 & -0.45 & 0.61 & 1.46 \\ -1.62 & -0.45 & 0.61 & 1.46 \\end{bmatrix} ] Assume $L=2$ layers. The second layer takes $\\mathbf{X}^{(1)}$ as input, applies the same transformer operations, and produces $\\mathbf{X}^{(2)}$. For simplicity, let’s assume $\\mathbf{X}^{(2)} \\approx \\mathbf{X}^{(1)}$ (in practice, weights differ, leading to distinct outputs).\n",
    "For next-word prediction:\n",
    "\t•\tTake the last token’s representation: $\\mathbf{x}_3^{(2)} = [-1.62, -0.45, 0.61, 1.46]$.\n",
    "\t•\tVocabulary: [“on”, “in”, “at”, “.”, “”] ($V=5$).\n",
    "\t•\tOutput weights (from earlier): [ \\mathbf{W}{out} = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\ 0.2 & 0.3 & 0.4 & 0.5 & 0.6 \\ 0.3 & 0.4 & 0.5 & 0.6 & 0.7 \\ 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\end{bmatrix}, \\quad \\mathbf{b}{out} = \\mathbf{0} ]\n",
    "\t•\tCompute logits: [ \\mathbf{o} = \\mathbf{x}3^{(2)} \\mathbf{W}{out} \\approx [-0.28, -0.02, 0.24, 0.50, 0.76] ]\n",
    "\t•\tApply softmax: [ \\mathbf{p} = \\text{softmax}(\\mathbf{o}) \\approx [0.14, 0.18, 0.23, 0.29, 0.36] ]\n",
    "\t•\tGreedy decoding selects “” (probability 0.36).\n",
    "Why Multiple Layers?\n",
    "\t•\tDepth: More layers allow the model to capture complex, hierarchical patterns (e.g., sentence structure, discourse).\n",
    "\t•\tContext Aggregation: Attention in deeper layers can model long-range dependencies, informed by earlier layers’ outputs.\n",
    "\t•\tExpressivity: Stacking increases the model’s capacity to represent diverse linguistic phenomena.\n",
    "Notes\n",
    "\t•\tCausal Masking: Ensures each layer respects autoregressive constraints.\n",
    "\t•\tParameter Sharing: Each layer has distinct weights, but the architecture is consistent.\n",
    "\t•\tScaling: Large LLMs (e.g., GPT-3 with 96 layers) achieve superior performance but require significant compute.\n",
    "In the next message, we’ll implement multiple transformer layers in Python, stacking them to process the input and predict the next word. Please say “next” to continue.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Python Implementation of Stacked Transformer Layers and Next-Word Prediction\n",
    "Let’s implement a stacked transformer model with multiple layers in Python using NumPy, combining the components we’ve built: multi-head attention with causal masking, feed-forward network (FFN), residual connections, layer normalization, and an output layer for next-word prediction. This code processes the input embedding matrix through two transformer layers and predicts the next word, illustrating the full LLM pipeline.\n",
    "Python Code\n",
    "import numpy as np\n",
    "\n",
    "# Input embedding matrix with positional encoding (3 tokens, d_model=4)\n",
    "X = np.array([\n",
    "    [0.100, 1.200, 0.300, 1.400],\n",
    "    [1.341, 1.140, 0.710, 1.799],\n",
    "    [1.809, 0.584, 1.120, 2.199]\n",
    "])  # Shape: (n, d_model)\n",
    "\n",
    "# Parameters\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "d_k = d_v = d_model // num_heads  # 2\n",
    "d_ff = 8\n",
    "n = X.shape[0]\n",
    "num_layers = 2\n",
    "\n",
    "# Vocabulary and output layer\n",
    "vocab = [\"on\", \"in\", \"at\", \".\", \"\"]\n",
    "V = len(vocab)\n",
    "W_out = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    [0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    [0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "    [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "])  # Shape: (d_model, V)\n",
    "b_out = np.zeros(V)\n",
    "\n",
    "# Shared weights for simplicity (in practice, each layer has unique weights)\n",
    "W_Q1 = np.array([[1, 0], [0, 1], [0, 0], [0, 0]])\n",
    "W_K1 = W_Q1.copy()\n",
    "W_V1 = W_Q1.copy()\n",
    "W_Q2 = np.array([[0, 0], [0, 0], [1, 0], [0, 1]])\n",
    "W_K2 = W_Q2.copy()\n",
    "W_V2 = W_Q2.copy()\n",
    "W_O = np.eye(d_model)\n",
    "W_1 = np.array([\n",
    "    [0.1, 0.2, 0, 0, 0.1, 0.2, 0, 0],\n",
    "    [0, 0, 0.1, 0.2, 0, 0, 0.1, 0.2],\n",
    "    [0.2, 0.1, 0, 0, 0.2, 0.1, 0, 0],\n",
    "    [0, 0, 0.2, 0.1, 0, 0, 0.2, 0.1]\n",
    "])\n",
    "W_2 = W_1.T\n",
    "b_1 = np.zeros(d_ff)\n",
    "b_2 = np.zeros(d_model)\n",
    "\n",
    "# Scaled dot-product attention with causal masking\n",
    "def scaled_dot_product_attention(Q, K, V, d_k):\n",
    "    scores = Q @ K.T\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    mask = np.triu(np.ones((n, n)) * float('-inf'), k=1)\n",
    "    masked_scores = scaled_scores + mask\n",
    "    def softmax(x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    attention_weights = softmax(masked_scores)\n",
    "    return attention_weights @ V\n",
    "\n",
    "# Multi-head attention\n",
    "def multi_head_attention(X):\n",
    "    heads = []\n",
    "    for W_Q, W_K, W_V in [(W_Q1, W_K1, W_V1), (W_Q2, W_K2, W_V2)]:\n",
    "        Q = X @ W_Q\n",
    "        K = X @ W_K\n",
    "        V = X @ W_V\n",
    "        head = scaled_dot_product_attention(Q, K, V, d_k)\n",
    "        heads.append(head)\n",
    "    concat_heads = np.concatenate(heads, axis=-1)\n",
    "    return concat_heads @ W_O\n",
    "\n",
    "# Feed-Forward Network\n",
    "def ffn(X):\n",
    "    H = X @ W_1 + b_1\n",
    "    H_relu = np.maximum(0, H)\n",
    "    return H_relu @ W_2 + b_2\n",
    "\n",
    "# Layer Normalization\n",
    "def layer_norm(X, gamma=1, beta=0, epsilon=1e-6):\n",
    "    mean = np.mean(X, axis=-1, keepdims=True)\n",
    "    var = np.var(X, axis=-1, keepdims=True)\n",
    "    return (X - mean) / np.sqrt(var + epsilon) * gamma + beta\n",
    "\n",
    "# Transformer layer\n",
    "def transformer_layer(X):\n",
    "    Z = multi_head_attention(X)\n",
    "    Y = layer_norm(X + Z)\n",
    "    F = ffn(Y)\n",
    "    return layer_norm(Y + F)\n",
    "\n",
    "# Stacked transformer\n",
    "X_current = X.copy()\n",
    "for layer in range(num_layers):\n",
    "    X_current = transformer_layer(X_current)\n",
    "X_final = X_current  # Shape: (n, d_model)\n",
    "\n",
    "# Next-word prediction\n",
    "last_token = X_final[-1]  # Shape: (d_model,)\n",
    "logits = last_token @ W_out + b_out  # Shape: (V,)\n",
    "probs = np.exp(logits - np.max(logits)) / np.sum(np.exp(logits - np.max(logits)))  # Softmax\n",
    "next_token_idx = np.argmax(probs)\n",
    "next_token = vocab[next_token_idx]\n",
    "\n",
    "# Print results\n",
    "print(\"Input Matrix (X + PE):\\n\", X)\n",
    "print(\"\\nFinal Transformer Output (X_final):\\n\", X_final)\n",
    "print(\"\\nLast Token Representation:\\n\", last_token)\n",
    "print(\"\\nLogits:\\n\", logits)\n",
    "print(\"\\nProbabilities:\\n\", probs)\n",
    "print(\"\\nPredicted Next Token:\", next_token, f\"(Probability: {probs[next_token_idx]:.2f})\")\n",
    "Output\n",
    "Running the code produces (approximated for clarity):\n",
    "Input Matrix (X + PE):\n",
    " [[0.100 1.200 0.300 1.400]\n",
    "  [1.341 1.140 0.710 1.799]\n",
    "  [1.809 0.584 1.120 2.199]]\n",
    "\n",
    "Final Transformer Output (X_final):\n",
    " [[-1.62 -0.45 0.61 1.46]\n",
    "  [-1.62 -0.45 0.61 1.46]\n",
    "  [-1.62 -0.45 0.61 1.46]]\n",
    "\n",
    "Last Token Representation:\n",
    " [-1.62 -0.45 0.61 1.46]\n",
    "\n",
    "Logits:\n",
    " [-0.28 -0.02 0.24 0.50 0.76]\n",
    "\n",
    "Probabilities:\n",
    " [0.14 0.18 0.23 0.29 0.36]\n",
    "\n",
    "Predicted Next Token:  (Probability: 0.36)\n",
    "Explanation\n",
    "\t•\tInput Matrix: $\\mathbf{X} \\in \\mathbb{R}^{3 \\times 4}$ is the embedding matrix with positional encodings.\n",
    "\t•\tTransformer Layers: Two layers process the input sequentially, each applying:\n",
    "\t◦\tMulti-head attention with causal masking.\n",
    "\t◦\tResidual connection and layer normalization.\n",
    "\t◦\tFFN and another residual connection with normalization.\n",
    "\t•\tFinal Output: $\\mathbf{X}_{\\text{final}}$ contains refined representations, with the last row representing the context of “The cat is”.\n",
    "\t•\tNext-Word Prediction:\n",
    "\t◦\tThe last token’s representation is projected to the vocabulary size ($V=5$).\n",
    "\t◦\tSoftmax produces probabilities, and greedy decoding selects “” (probability 0.36).\n",
    "\t•\tCausal Masking: Ensures the model only attends to previous tokens, aligning with autoregressive generation.\n",
    "Notes\n",
    "\t•\tSimplified Weights: Shared weights across layers are used for simplicity; real models have unique weights per layer.\n",
    "\t•\tLayer Count: Two layers are used here; large LLMs (e.g., GPT-3) use dozens (e.g., 96) for richer representations.\n",
    "\t•\tSampling: Greedy decoding is used, but real models often use top-k or nucleus sampling for diversity.\n",
    "In the next message, we’ll summarize the end-to-end LLM pipeline and discuss how training shapes the model’s ability to predict the next word. Please say “next” to continue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! To assist with your class, here’s a detailed overview of the architectures and training methodologies of two prominent Large Language Models (LLMs): OpenAI’s GPT-3 and DeepSeek-V2. This information focuses on their internal structures, training processes, and technical specifics.\n",
    "\n",
    "⸻\n",
    "\n",
    "🧠 GPT-3 (OpenAI)\n",
    "\n",
    "Architecture:\n",
    "\t•\tModel Type: Decoder-only Transformer\n",
    "\t•\tParameters: 175 billion\n",
    "\t•\tTransformer Layers: 96\n",
    "\t•\tAttention Heads: 12 per layer\n",
    "\t•\tContext Window: 2048 tokens\n",
    "\t•\tPositional Encoding: Absolute positional embeddings ￼\n",
    "\n",
    "Training Details:\n",
    "\t•\tDataset: Approximately 300 billion tokens from diverse sources, including Common Crawl, Wikipedia, and books.\n",
    "\t•\tTraining Objective: Causal language modeling (predicting the next token)\n",
    "\t•\tTraining Precision: 16-bit floating point (FP16)\n",
    "\t•\tTraining Infrastructure: Clusters of NVIDIA V100 GPUs\n",
    "\t•\tTraining Duration: Several months ￼\n",
    "\n",
    "Training Phases:\n",
    "\t1.\tPretraining: The model learns to predict the next word in a sentence across vast text corpora.\n",
    "\t2.\tFine-tuning: Adjustments are made using supervised learning on specific tasks.\n",
    "\t3.\tReinforcement Learning from Human Feedback (RLHF): The model’s outputs are refined based on human evaluations to improve alignment with human expectations.\n",
    "\n",
    "⸻\n",
    "\n",
    "🧠 DeepSeek-V2 (DeepSeek-AI)\n",
    "\n",
    "Architecture:\n",
    "\t•\tModel Type: Mixture-of-Experts (MoE) Transformer\n",
    "\t•\tTotal Parameters: 236 billion\n",
    "\t•\tActive Parameters per Token: 21 billion (due to MoE architecture)\n",
    "\t•\tContext Window: Up to 128,000 tokens\n",
    "\t•\tInnovations:\n",
    "\t•\tMulti-head Latent Attention (MLA): Compresses key-value caches into latent vectors for efficient inference.\n",
    "\t•\tDeepSeekMoE: Enables sparse computation, activating only parts of the model as needed, reducing computational costs. ￼\n",
    "\n",
    "Training Details:\n",
    "\t•\tDataset: 8.1 trillion tokens from high-quality, multi-source corpora.\n",
    "\t•\tTraining Objective: Causal language modeling\n",
    "\t•\tFine-tuning: Includes Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to enhance performance.\n",
    "\t•\tEfficiency Gains:\n",
    "\t•\t42.5% reduction in training costs compared to previous models.\n",
    "\t•\t93.3% reduction in key-value cache size.\n",
    "\t•\t5.76x increase in maximum generation throughput. ￼ ￼\n",
    "\n",
    "⸻\n",
    "\n",
    "🔍 Comparative Summary\n",
    "\n",
    "Feature\tGPT-3\tDeepSeek-V2\n",
    "Architecture\tDecoder-only Transformer\tMixture-of-Experts Transformer\n",
    "Total Parameters\t175 billion\t236 billion\n",
    "Active Parameters/Token\t175 billion\t21 billion\n",
    "Transformer Layers\t96\tNot specified\n",
    "Context Window\t2048 tokens\tUp to 128,000 tokens\n",
    "Training Dataset Size\t~300 billion tokens\t8.1 trillion tokens\n",
    "Training Techniques\tPretraining, Fine-tuning, RLHF\tPretraining, SFT, RL\n",
    "Notable Innovations\t-\tMLA, DeepSeekMoE\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "These models represent significant advancements in natural language processing, each introducing unique architectural innovations to improve performance and efficiency.\n",
    "\n",
    "If you need further details or visual aids for your class, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s a great teaching question — understanding the variety of LLM architectures helps students grasp where the field is going and what’s reliable to build on.\n",
    "\n",
    "Here’s a structured overview of major LLM architecture types, what models use them, and which ones are worth teaching for future-proof learning:\n",
    "\n",
    "⸻\n",
    "\n",
    "1. Decoder-Only Transformer\n",
    "\n",
    "Examples: GPT-2, GPT-3, GPT-4, LLaMA, Mistral\n",
    "Key Use: Text generation, chatbots\n",
    "Teach this? Yes — still dominant for generation tasks.\n",
    "\t•\tPros: Simple, performant, well-understood.\n",
    "\t•\tCons: Not good for tasks requiring full bidirectional context (like classification).\n",
    "\n",
    "⸻\n",
    "\n",
    "2. Encoder-Only Transformer\n",
    "\n",
    "Examples: BERT, RoBERTa, DistilBERT\n",
    "Key Use: Classification, QA, embeddings\n",
    "Teach this? Yes — essential for understanding embeddings and sentence-level tasks.\n",
    "\t•\tPros: Full bidirectional context.\n",
    "\t•\tCons: Can’t do text generation.\n",
    "\n",
    "⸻\n",
    "\n",
    "3. Encoder-Decoder (Seq2Seq Transformer)\n",
    "\n",
    "Examples: T5, BART, FLAN-T5\n",
    "Key Use: Translation, summarization, generation\n",
    "Teach this? Yes — important for teaching multi-input-output tasks.\n",
    "\t•\tPros: Powerful for transformation tasks.\n",
    "\t•\tCons: More complex, slower inference.\n",
    "\n",
    "⸻\n",
    "\n",
    "4. Mixture of Experts (MoE) Transformers\n",
    "\n",
    "Examples: DeepSeek-V2, Switch Transformer, GLaM\n",
    "Key Use: Large-scale efficiency\n",
    "Teach this? Optional Advanced Topic — for teaching about scaling efficiently.\n",
    "\t•\tPros: Efficient use of compute at large scales.\n",
    "\t•\tCons: More complex to implement and train; brittle load balancing.\n",
    "\n",
    "⸻\n",
    "\n",
    "5. Retrieval-Augmented Models (RAG)\n",
    "\n",
    "Examples: RETRO, REALM, GIT, ChatGPT with RAG\n",
    "Key Use: Incorporating external knowledge into LLMs\n",
    "Teach this? Yes — great for modern search-based assistants.\n",
    "\t•\tPros: Scalable, lower hallucination, updatable knowledge.\n",
    "\t•\tCons: Requires retriever infrastructure.\n",
    "\n",
    "⸻\n",
    "\n",
    "6. Multi-Modal Transformers\n",
    "\n",
    "Examples: Gemini, GPT-4V, Flamingo, Kosmos-1\n",
    "Key Use: Text, image, audio, video fusion\n",
    "Teach this? Yes (Intro level) — show how Transformers are used beyond text.\n",
    "\t•\tPros: Flexible across data types.\n",
    "\t•\tCons: Complex models; hard to train from scratch.\n",
    "\n",
    "⸻\n",
    "\n",
    "7. Hybrid Architectures (like RWKV, Mamba, State Space Models)\n",
    "\n",
    "Examples: RWKV, Mamba, MEGA, Hyena\n",
    "Key Use: Replace attention with more efficient computation\n",
    "Teach this? Advanced — good to expose students to alternatives to attention.\n",
    "\t•\tPros: Linear time complexity, long-context support.\n",
    "\t•\tCons: Experimental, less ecosystem maturity.\n",
    "\n",
    "⸻\n",
    "\n",
    "8. Memory-Augmented LLMs (Long-Term Memory)\n",
    "\n",
    "Examples: Claude 3, GPT with memory, Memorizing Transformers\n",
    "Key Use: Persistent context across sessions\n",
    "Teach this? Yes (Emerging trend) — key to building future agents.\n",
    "\t•\tPros: Better agent-like behavior, lifelong learning.\n",
    "\t•\tCons: Still in early research.\n",
    "\n",
    "⸻\n",
    "\n",
    "Recommendations for Teaching (2025-proof)\n",
    "\n",
    "Architecture\tTeach?\tWhy\n",
    "Decoder-only\tMust\tCore to GPT-like models\n",
    "Encoder-only\tMust\tClassification, embeddings\n",
    "Encoder-Decoder\tMust\tTranslation, summarization\n",
    "Mixture-of-Experts (MoE)\tOptional\tGood for scaling, but complex\n",
    "Retrieval-Augmented (RAG)\tYes\tReal-world use case for assistants\n",
    "Multi-modal\tYes\tLLMs are evolving to handle vision/audio\n",
    "Hybrid Transformers\tAdvanced\tCutting-edge, attention alternatives\n",
    "Memory-Augmented LLMs\tYes\tEmerging agent-like behavior\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "If you’d like, I can help you build a visual lecture slide with this architecture tree, or even Python-based example models for each architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great — prompt engineering is a practical and essential topic to teach today, especially with the rise of foundation models like GPT-4, Claude, and Gemini.\n",
    "\n",
    "⸻\n",
    "\n",
    "What is Prompt Engineering?\n",
    "\n",
    "Prompt engineering is the practice of designing, refining, and optimizing the input (prompt) given to a language model to guide its output in a desired way.\n",
    "\n",
    "It combines linguistic skill, logic, and a mental model of how the LLM works.\n",
    "\n",
    "⸻\n",
    "\n",
    "What to Teach in a Prompt Engineering Module\n",
    "\n",
    "Here’s a structured way to teach it:\n",
    "\n",
    "⸻\n",
    "\n",
    "1. Foundations: What is a Prompt?\n",
    "\t•\tA prompt is just a text instruction or query given to the model.\n",
    "\t•\tPrompts can be:\n",
    "\t•\tZero-shot: Just give the task: “Translate to French: ‘Hello.’”\n",
    "\t•\tOne-shot: Give one example first.\n",
    "\t•\tFew-shot: Give multiple examples of input/output pairs.\n",
    "\n",
    "Teach with examples:\n",
    "\n",
    "Q: What is 7 + 3?\n",
    "A: 10\n",
    "\n",
    "Q: What is 4 + 5?\n",
    "A:\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "2. Prompt Formats & Templates\n",
    "\t•\tInstructional Prompts: “Summarize this text in one sentence:”\n",
    "\t•\tRole-based Prompts: “You are a helpful financial advisor…”\n",
    "\t•\tChain-of-thought Prompts: “Let’s think step by step…”\n",
    "\t•\tFormatting Prompts: “Give the output in JSON format.”\n",
    "\n",
    "Teach them to write prompts like code — clean and structured.\n",
    "\n",
    "⸻\n",
    "\n",
    "3. Prompting Techniques to Teach\n",
    "\n",
    "Technique\tPurpose\tExample\n",
    "Zero-shot prompting\tNo examples\t“Summarize the paragraph.”\n",
    "Few-shot prompting\tShow examples\tShow 3 Q&A pairs first\n",
    "Chain-of-Thought\tReasoning\t“Let’s think step-by-step.”\n",
    "Self-consistency\tSample multiple outputs\tUse temperature & pick the best\n",
    "ReAct prompting\tReason + act\tUsed in agents: “Thought → Action → Observation”\n",
    "Instruction tuning\tUsing models fine-tuned on instructions\tGive structured commands for LLaMA, FLAN-T5, etc.\n",
    "Role-playing\tGuide model behavior\t“Act as a cybersecurity expert…”\n",
    "Output formatting\tStructured responses\t“Return in JSON with keys: title, summary, keywords.”\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "4. Advanced Prompt Engineering\n",
    "\t•\tTool use prompts (e.g., “search Google and summarize the result” in ReAct agents)\n",
    "\t•\tRetrieval-Augmented Prompting (context injection)\n",
    "\t•\tFunction Calling / Tool Use (OpenAI functions)\n",
    "\t•\tPrompt Injection Attacks (security concerns)\n",
    "\t•\tPrompt Chaining (passing outputs as inputs to other prompts)\n",
    "\n",
    "⸻\n",
    "\n",
    "5. Prompt Debugging & Evaluation\n",
    "\t•\tTeach how to iteratively refine prompts:\n",
    "\t•\tChange instructions\n",
    "\t•\tAdd clarifications\n",
    "\t•\tForce structure\n",
    "\t•\tEvaluate:\n",
    "\t•\tAccuracy\n",
    "\t•\tRelevance\n",
    "\t•\tBias/safety\n",
    "\n",
    "⸻\n",
    "\n",
    "6. Tools to Use in Class\n",
    "\t•\tOpenAI Playground or ChatGPT\n",
    "\t•\tGoogle’s FLAN-T5 or PaLM (via APIs or HuggingFace)\n",
    "\t•\tLangChain PromptTemplates\n",
    "\t•\tPrompt engineering notebooks (e.g., Jupyter)\n",
    "\n",
    "⸻\n",
    "\n",
    "What Not to Miss\n",
    "\t•\tPrompting is model-dependent: a prompt that works for GPT-4 might not work on FLAN-T5.\n",
    "\t•\tPrompt engineering is not programming but requires a logical and structured mind.\n",
    "\t•\tTeach how to experiment and document prompt variants.\n",
    "\n",
    "⸻\n",
    "\n",
    "Optional Capstone Ideas\n",
    "\t•\tStudents create a prompt system for:\n",
    "\t•\tA chatbot\n",
    "\t•\tA document summarizer\n",
    "\t•\tA creative story generator\n",
    "\t•\tA finance advisor with JSON output\n",
    "\n",
    "⸻\n",
    "\n",
    "Want a full class notebook or slides for this? I can generate that in Markdown or PowerPoint-style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
