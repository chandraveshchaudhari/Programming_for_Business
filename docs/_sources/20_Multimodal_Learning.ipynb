{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Multimodal Learning\n",
    "\n",
    "⸻\n",
    "\n",
    "1. What Is Multimodal Learning?\n",
    "\n",
    "Multimodal learning combines information from multiple data modalities, such as:\n",
    "\t•\tText (e.g., captions, documents)\n",
    "\t•\tImage (e.g., pixels, features)\n",
    "\t•\tAudio (e.g., speech, sounds)\n",
    "\t•\tVideo, sensor, or tabular data\n",
    "\n",
    "Goal: Learn joint representations or complementary features for better prediction, classification, or generation.\n",
    "\n",
    "⸻\n",
    "\n",
    "2. Fusion Strategies\n",
    "\n",
    "Early Fusion:\n",
    "\t•\tCombine raw features (e.g., concatenate vectors)\n",
    "\t•\tExample: [text features | image features]\n",
    "\n",
    "Late Fusion:\n",
    "\t•\tTrain separate models for each modality\n",
    "\t•\tCombine decisions (e.g., ensemble of outputs)\n",
    "\n",
    "Hybrid Fusion:\n",
    "\t•\tIntermediate representations are combined\n",
    "\t•\tOften seen in attention-based models (e.g., CLIP, ViLBERT)\n",
    "\n",
    "⸻\n",
    "\n",
    "3. Applications\n",
    "\n",
    "Modality Pair\tTask Example\n",
    "Text + Image\tImage Captioning, VQA\n",
    "Text + Audio\tSpeech Recognition\n",
    "Video + Audio\tEmotion Recognition\n",
    "Image + Sensor\tMedical Diagnosis\n",
    "Text + Table\tFinancial Report Analysis\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "4. Architecture Example (Text + Image)\n",
    "\n",
    "Image → CNN → Img Features ┐\n",
    "                            ├→ Concatenate → MLP → Output\n",
    "Text  → BERT → Txt Features ┘\n",
    "\n",
    "Or using attention:\n",
    "\n",
    "Image → CNN → Keys/Values ┐\n",
    "                           ├→ Cross Attention\n",
    "Text → BERT → Queries     ┘\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "5. Python: Simple Text + Image Fusion (Early Fusion)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, txt_dim=768, img_dim=2048, hidden=512, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.img_fc = nn.Linear(img_dim, txt_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(txt_dim + txt_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, text_input_ids, text_attention_mask, img_features):\n",
    "        txt_out = self.bert(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
    "        txt_vec = txt_out.pooler_output  # [B, 768]\n",
    "        img_vec = self.img_fc(img_features)  # [B, 768]\n",
    "        x = torch.cat([txt_vec, img_vec], dim=1)  # [B, 1536]\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "6. Challenges in Multimodal Learning\n",
    "\t•\tMissing modalities: Not all inputs always available\n",
    "\t•\tAlignment: Matching time/space between modalities\n",
    "\t•\tNoise: One modality may dominate or be noisy\n",
    "\t•\tDimensional mismatch: Vectors of vastly different sizes\n",
    "\n",
    "⸻\n",
    "\n",
    "7. Large Multimodal Models\n",
    "\n",
    "Model\tModality\tTask\n",
    "CLIP\tText + Image\tRetrieval, Zero-shot vision\n",
    "Flamingo\tText + Image\tVQA, Captioning\n",
    "Whisper\tAudio + Text\tTranscription, Translation\n",
    "Gemini/GPT-4V\tText + Image/Code\tMultimodal reasoning\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "Would you like an example of Multimodal Contrastive Learning like CLIP, or Vision + Sensor fusion for time-series prediction?"
   ],
   "id": "b03adb7cb24dd2b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e57b5675cf7640f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
