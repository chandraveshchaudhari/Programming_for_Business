{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0750a014-fe70-4c76-8baf-33bcfd5510d6",
   "metadata": {},
   "source": [
    "# Transformers and LLMs\n",
    "\n",
    "\t• Self-Attention Mechanism\n",
    "\t• Positional Encoding, Multi-Head Attention\n",
    "\t• Feedforward + LayerNorm + Residual\n",
    "\t• Python: Mini-Transformer from Scratch (PyTorch)\n",
    "How DeepSeek Rewrote the Transformer [MLA] https://youtu.be/0VLAoVGf_74?si=AecUx9gtvM-ETDMR \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c5e1d-128a-4fb5-a139-233c427533ec",
   "metadata": {},
   "source": [
    "Deepseek R1\n",
    "\n",
    "Agents\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here’s your complete module on Transformers and Large Language Models (LLMs), including essential theory and a Mini-Transformer implemented from scratch in PyTorch.\n",
    "\n",
    "⸻\n",
    "\n",
    "Transformers and LLMs\n",
    "\n",
    "⸻\n",
    "\n",
    "1. Self-Attention Mechanism\n",
    "\n",
    "Each token attends to others via scaled dot-product attention:\n",
    "\n",
    "Given query $Q$, key $K$, and value $V$:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Where $d_k$ is the dimensionality of the key vectors.\n",
    "\n",
    "⸻\n",
    "\n",
    "2. Positional Encoding & Multi-Head Attention\n",
    "\n",
    "Positional Encoding adds order info (since attention is permutation-invariant):\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right), \\quad PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "Multi-Head Attention:\n",
    "\t•\tProjects input into multiple Q/K/V spaces\n",
    "\t•\tRuns attention in parallel\n",
    "\t•\tConcatenates and reprojects\n",
    "\n",
    "⸻\n",
    "\n",
    "3. Transformer Block\n",
    "\n",
    "Each block includes:\n",
    "\t1.\tMulti-Head Attention\n",
    "\t2.\tAdd + LayerNorm\n",
    "\t3.\tFeedforward Network\n",
    "\t4.\tAdd + LayerNorm\n",
    "\n",
    "Each with residual connections:\n",
    "\n",
    "x → Attention → Add & Norm → FFN → Add & Norm\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "4. Python: Mini-Transformer from Scratch (PyTorch)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim ** -0.5\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.reshape(B, T, self.heads, C // self.heads).transpose(1, 2), qkv)\n",
    "        dots = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = torch.softmax(dots, dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, T, C)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, ff_hidden):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention(dim, heads)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, ff_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden, dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2) * (-math.log(10000.0) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # shape (1, max_len, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, dim, depth, heads, ff_hidden, max_len):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_encoding = PositionalEncoding(dim, max_len)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(dim, heads, ff_hidden) for _ in range(depth)\n",
    "        ])\n",
    "        self.to_logits = nn.Linear(dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.blocks(x)\n",
    "        return self.to_logits(x)\n",
    "\n",
    "# Sample usage\n",
    "model = MiniTransformer(vocab_size=1000, dim=64, depth=2, heads=4, ff_hidden=256, max_len=50)\n",
    "dummy_input = torch.randint(0, 1000, (8, 50))  # Batch of 8 sequences\n",
    "out = model(dummy_input)\n",
    "print(out.shape)  # (8, 50, 1000)\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "Would you like to extend this to causal masking for language modeling or plug it into a training loop with tokenized data?"
   ],
   "id": "9a54b01dc4e5b266"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
