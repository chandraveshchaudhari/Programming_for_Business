{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2794070-9cf3-4053-8c7b-8da9de05c44f",
   "metadata": {},
   "source": [
    "# Neural Networks: From Perceptron to MLP\n",
    "\n",
    "\t• Linear Threshold Units\n",
    "\t• Backpropagation Algorithm (Gradient Chains)\n",
    "\t• Activation Functions (Sigmoid, ReLU, Tanh)\n",
    "\t• Python: Manual Backprop on 2-Layer Network\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here’s a complete Neural Networks: From Perceptron to MLP module with key theory and a Python implementation of manual backpropagation for a 2-layer network.\n",
    "\n",
    "⸻\n",
    "\n",
    "Neural Networks: From Perceptron to MLP\n",
    "\n",
    "⸻\n",
    "\n",
    "1. Linear Threshold Units (LTUs)\n",
    "\n",
    "A perceptron is the simplest neural unit:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "1 & \\text{if } \\mathbf{w}^\\top \\mathbf{x} + b > 0 \\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\t•\tWorks only for linearly separable problems.\n",
    "\t•\tNo hidden layers → limited expressiveness.\n",
    "\n",
    "⸻\n",
    "\n",
    "2. Activation Functions\n",
    "\n",
    "Function\tFormula\tNotes\n",
    "Sigmoid\t$\\sigma(z) = \\frac{1}{1 + e^{-z}}$\tSmooth, bounded, but saturates\n",
    "Tanh\t$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\tZero-centered\n",
    "ReLU\t$\\text{ReLU}(z) = \\max(0, z)$\tSparse, fast, not bounded\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "3. Backpropagation Algorithm\n",
    "\n",
    "Goal: Minimize loss by adjusting weights using gradient descent.\n",
    "\t1.\tForward Pass: Compute outputs layer by layer.\n",
    "\t2.\tLoss: Typically MSE or Cross-Entropy.\n",
    "\t3.\tBackward Pass:\n",
    "\t•\tUse chain rule to compute gradients.\n",
    "\t•\tUpdate weights:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "⸻\n",
    "\n",
    "4. Python: Manual Backprop on a 2-Layer MLP\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Activation and derivatives\n",
    "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "def d_sigmoid(x): return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Data: XOR Problem\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Initialize weights\n",
    "np.random.seed(42)\n",
    "input_size, hidden_size, output_size = 2, 2, 1\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Training\n",
    "lr = 0.1\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    z1 = X @ W1 + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    y_pred = sigmoid(z2)\n",
    "\n",
    "    # Loss (MSE)\n",
    "    loss = np.mean((y - y_pred) ** 2)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    d_loss = 2 * (y_pred - y) / y.size\n",
    "    d_z2 = d_loss * d_sigmoid(z2)\n",
    "    d_W2 = a1.T @ d_z2\n",
    "    d_b2 = np.sum(d_z2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = d_z2 @ W2.T\n",
    "    d_z1 = d_a1 * d_sigmoid(z1)\n",
    "    d_W1 = X.T @ d_z1\n",
    "    d_b1 = np.sum(d_z1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update weights\n",
    "    W2 -= lr * d_W2\n",
    "    b2 -= lr * d_b2\n",
    "    W1 -= lr * d_W1\n",
    "    b1 -= lr * d_b1\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nFinal Predictions:\")\n",
    "print(np.round(y_pred, 3))\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "Would you like me to extend this to multi-class classification, or add ReLU and tanh variants in the network?"
   ],
   "id": "460dfbe9c8f13954"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
