
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Logistic Regression (Mathematical Explanation) &#8212; Machine Learning for Business</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '4_Logistic_Regression';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Dimensionality Reduction: PCA" href="5_Dimensionality_Reduction.html" />
    <link rel="prev" title="Naive Bayes" href="3_Naive_Bayes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Machine Learning for Business - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Machine Learning for Business - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="symbols.html">Common Math Symbols and Notations in Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_Linear_regression.html">Linear Regression</a></li>

















<li class="toctree-l1"><a class="reference internal" href="2_Regularization.html">Generalization, Overfitting, Regularization</a></li>



<li class="toctree-l1"><a class="reference internal" href="3_Naive_Bayes.html">Naive Bayes</a></li>



<li class="toctree-l1 current active"><a class="current reference internal" href="#"><strong>Logistic Regression (Mathematical Explanation)</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="5_Dimensionality_Reduction.html">Dimensionality Reduction: PCA</a></li>






<li class="toctree-l1"><a class="reference internal" href="6_Clustering.html">K-Means and Clustering</a></li>




<li class="toctree-l1"><a class="reference internal" href="7_Gaussian_Mixture_Models.html">Gaussian Mixture Models and EM</a></li>


<li class="toctree-l1"><a class="reference internal" href="8_Nearest_Neighbour_Algorithm.html">Nearest Neighbour Algorithm</a></li>







<li class="toctree-l1"><a class="reference internal" href="10_Support_Vector_Machines.html">Support Vector Machines</a></li>



<li class="toctree-l1"><a class="reference internal" href="11_Decision_Trees.html">Decision Trees</a></li>

<li class="toctree-l1"><a class="reference internal" href="12_Ensemble_Methods.html">Ensemble Methods</a></li>

<li class="toctree-l1"><a class="reference internal" href="13_Neural_Networks.html">Neural Networks: From Perceptron to MLP</a></li>





<li class="toctree-l1"><a class="reference internal" href="14_Arima.html">ARIMA</a></li>





<li class="toctree-l1"><a class="reference internal" href="15_LSTM.html">Advanced DL: ResNets, RNNs</a></li>

<li class="toctree-l1"><a class="reference internal" href="16_CNN.html">Convolutional Neural Networks</a></li>


<li class="toctree-l1"><a class="reference internal" href="17_Resnet.html">1D-ResNet (or TCN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_LLM.html">Transformers and LLMs</a></li>

<li class="toctree-l1"><a class="reference internal" href="19_Reinforcement_Learning.html">Reinforcement Learning</a></li>

<li class="toctree-l1"><a class="reference internal" href="20_Multimodal_Learning.html">Multimodal Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Generative_Models.html">Generative Models</a></li>




<li class="toctree-l1"><a class="reference internal" href="Performance_Metrics.html">Performance Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="Visualization.html">Visualisation</a></li>





</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/chandraveshchaudhari/BusinessML_web/blob/main/notebooks//./4_Logistic_Regression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>

<li>
  <a href="https://chandraveshchaudhari.github.io/BusinessML_web/jupyterlite/lab/index.html?path=4_Logistic_Regression.ipynb" target="_blank"
     class="btn btn-sm dropdown-item"
     title="Launch on JupyterLite"
     data-bs-placement="left" data-bs-toggle="tooltip">
    <span class="btn__icon-container" style="display:inline-block; width:20px; height:20px;">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 256">
        <circle cx="128" cy="128" r="128" fill="#f37726"/>
        <ellipse cx="128" cy="128" rx="110" ry="40" fill="white" transform="rotate(-25, 128, 128)"/>
        <ellipse cx="128" cy="128" rx="110" ry="40" fill="white" transform="rotate(25, 128, 128)"/>
        <circle cx="200" cy="60" r="18" fill="white"/>
        <circle cx="60" cy="200" r="18" fill="white"/>
      </svg>
    </span>
    <span class="btn__text-container">JupyterLite</span>
  </a>
</li>

</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/chandraveshchaudhari/BusinessML_web" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/chandraveshchaudhari/BusinessML_web/issues/new?title=Issue%20on%20page%20%2F4_Logistic_Regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/4_Logistic_Regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Logistic Regression (Mathematical Explanation)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts"><strong>1. Key Concepts</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sigmoid-function"><strong>2. The Sigmoid Function</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-representation"><strong>3. Hypothesis Representation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-function-log-loss"><strong>4. Cost Function (Log Loss)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-optimization"><strong>5. Gradient Descent Optimization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary"><strong>6. Decision Boundary</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiclass-logistic-regression-softmax"><strong>7. Multiclass Logistic Regression (Softmax)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-limitations"><strong>8. Assumptions &amp; Limitations</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-in-python-scikit-learn"><strong>Example in Python (Scikit-learn)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-coin-flipping-with-maximum-likelihood-estimation-mle-and-maximum-a-posteriori-map">Example: Coin Flipping with Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-trust-only-the-data"><strong>MLE: Trust Only the Data</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#map-combine-data-with-a-prior-belief"><strong>MAP: Combine Data with a Prior Belief</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-difference">Key Difference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-recap"><strong>Linear Regression Recap</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-for-classification"><strong>Motivation for Classification</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-components">Key Components</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-function">1. Sigmoid Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties">Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-e-x-in-the-sigmoid">Why Use <span class="math notranslate nohighlight">\(e^x\)</span> in the Sigmoid?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2. Decision Boundary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-formulation">3. Model Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-with-maximum-likelihood-estimation-mle">4. Training with Maximum Likelihood Estimation (MLE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data">Training Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-function">Likelihood Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-logarithm-rules">Basic Logarithm Rules</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rules-involving-e-x-and-ln-x-natural-logarithm">Rules Involving <span class="math notranslate nohighlight">\(e^x\)</span> and <span class="math notranslate nohighlight">\(\ln(x)\)</span> (Natural Logarithm)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-calculating-e-z-or-e-z">Examples of Calculating <span class="math notranslate nohighlight">\(e(z)\)</span> or <span class="math notranslate nohighlight">\(e^z\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-e-0">Example 1: <span class="math notranslate nohighlight">\(e(0)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-e-1">Example 2: <span class="math notranslate nohighlight">\(e(1)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-e-2">Example 3: <span class="math notranslate nohighlight">\(e(2)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-4-e-1">Example 4: <span class="math notranslate nohighlight">\(e(-1)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-5-e-0-5">Example 5: <span class="math notranslate nohighlight">\(e(0.5)\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notes">Notes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood">Log-Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivation-of-the-sigmoid-function">5. Derivation of the Sigmoid Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-derivation">Step-by-Step Derivation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-the-sigmoid-function">6. Gradient of the Sigmoid Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivation">Derivation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-function-and-optimization-objective-for-logistic-regression">Cost Function and Optimization Objective for Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-a-specific-cost-function">Why a Specific Cost Function?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-function">1. Cost Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compact-form">Compact Form</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#full-cost-function">Full Cost Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-measure-of-uncertainty">1. <strong>Entropy: Measure of Uncertainty</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#log-loss-binary-cross-entropy">2. <strong>Log-Loss / Binary Cross-Entropy</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#formula">Formula:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-maximum-likelihood-estimation-mle">2. Connection to Maximum Likelihood Estimation (MLE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Likelihood Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Log-Likelihood</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-and-parameter-updates">3. Gradient Descent and Parameter Updates</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-the-sigmoid">Gradient of the Sigmoid</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-the-cost-function">Gradient of the Cost Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-update-rule">Gradient Descent Update Rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-to-linear-regression">Similarity to Linear Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-weights">4. Interpretation of Weights</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-a-posteriori-map-estimation-and-regularization">5. Maximum A Posteriori (MAP) Estimation and Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#map-estimation">MAP Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-prior-and-l2-regularization">Gaussian Prior and L2 Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#map-estimation-incorporating-prior-knowledge">1. <strong>MAP Estimation: Incorporating Prior Knowledge</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2. <strong>Gaussian Prior and L2 Regularization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-l2-regularization-works">3. <strong>Why L2 Regularization Works</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#small-example-logistic-regression-with-map-and-l2-regularization">Small Example: Logistic Regression with MAP and L2 Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-setup">Problem Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-log-likelihood-for-logistic-regression">Step 1: Log-Likelihood for Logistic Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-gaussian-prior">Step 2: Gaussian Prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-map-objective">Step 3: MAP Objective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-interpretation">Step 4: Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-numerical-intuition">Step 5: Numerical Intuition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-and-map-in-context">6. MLE and MAP in Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-a-posteriori-map">Maximum A Posteriori (MAP)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-vs-map">MLE vs. MAP</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-biased-coin">7. Example: Biased Coin</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood">Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Log-Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-solution">MLE Solution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-maximum-likelihood">8. Conditional Maximum Likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-from-scratch">Logistic Regression from Scratch</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-code">Explanation of Code:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Notes:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-regression-explained">Softmax Regression Explained</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Key Components</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">1. Model Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-score-computation">Step 1: Score Computation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-softmax-function">Step 2: Softmax Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#identifiability-note">Identifiability Note</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deeper-explanation-of-the-softmax-function">Deeper Explanation of the Softmax Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-softmax-function">1. <strong>What is the Softmax Function?</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-the-softmax-function">2. <strong>Why Use the Softmax Function?</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-in-multi-class-classification">3. <strong>Softmax in Multi-Class Classification</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-softmax">4. <strong>Properties of Softmax</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#small-example-applying-the-softmax-function">Small Example: Applying the Softmax Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Problem Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-compute-scores-logits">Step 1: Compute Scores (Logits)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-apply-the-softmax-function">Step 2: Apply the Softmax Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-interpretation">Step 3: Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-numerical-stability-optional">Step 4: Numerical Stability (Optional)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-interpretation">2. Probabilistic Interpretation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-rule">Decision Rule</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objective-maximum-likelihood-estimation-mle">3. Learning Objective: Maximum Likelihood Estimation (MLE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Likelihood Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Log-Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Cost Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">4. Gradient of the Cost Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-derivation">Gradient Derivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-update">Gradient Descent Update</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-logistic-regression">5. Connection to Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">6. Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#information-theoretic-view-entropy-kl-divergence-in-classification">Information-Theoretic View: Entropy &amp; KL Divergence in Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-in-classification"><strong>1. Entropy in Classification</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kl-divergence-in-classification"><strong>2. KL Divergence in Classification</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-implications-in-classification"><strong>3. Practical Implications in Classification</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-binary-classification"><strong>4. Example: Binary Classification</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-and-considerations"><strong>5. Limitations and Considerations</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#broader-context"><strong>6. Broader Context</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-cross-entropy-loss-multi-class">1. <strong>Multinomial Cross-Entropy Loss (Multi-Class)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Formula</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation">Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-features">Key Features</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-cross-entropy-loss-two-classes">2. <strong>Binary Cross-Entropy Loss (Two Classes)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Formula</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Key Features</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison">3. <strong>Comparison</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insight-binary-case-as-a-special-case">Key Insight: Binary Case as a Special Case</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#small-examples">4. <strong>Small Examples</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-binary-cross-entropy-2-classes">Example 1: Binary Cross-Entropy (2 Classes)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-multinomial-cross-entropy-3-classes">Example 2: Multinomial Cross-Entropy (3 Classes)</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="logistic-regression-mathematical-explanation">
<h1><strong>Logistic Regression (Mathematical Explanation)</strong><a class="headerlink" href="#logistic-regression-mathematical-explanation" title="Link to this heading">#</a></h1>
<section id="key-concepts">
<h2><strong>1. Key Concepts</strong><a class="headerlink" href="#key-concepts" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Binary Classification</strong>: Predicts <span class="math notranslate nohighlight">\(P(y=1 | \mathbf{x})\)</span>, the probability that <span class="math notranslate nohighlight">\(y=1\)</span> given input features <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p></li>
<li><p><strong>Sigmoid Function</strong>: Maps linear combinations of inputs to a probability between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="the-sigmoid-function">
<h2><strong>2. The Sigmoid Function</strong><a class="headerlink" href="#the-sigmoid-function" title="Link to this heading">#</a></h2>
<p>The core of logistic regression is the <strong>sigmoid (logistic) function</strong>:</p>
<div class="math notranslate nohighlight">
\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(z = \mathbf{w}^T \mathbf{x} + b\)</span> (linear combination of weights <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and features <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, with bias <span class="math notranslate nohighlight">\(b\)</span>).</p></li>
<li><p>Output <span class="math notranslate nohighlight">\(\sigma(z) \in (0,1)\)</span>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="hypothesis-representation">
<h2><strong>3. Hypothesis Representation</strong><a class="headerlink" href="#hypothesis-representation" title="Link to this heading">#</a></h2>
<p>The predicted probability <span class="math notranslate nohighlight">\(\hat{y} = P(y=1 | \mathbf{x})\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = \sigma(\mathbf{w}^T \mathbf{x} + b) = \frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{x} + b)}}
\]</div>
</section>
<hr class="docutils" />
<section id="cost-function-log-loss">
<h2><strong>4. Cost Function (Log Loss)</strong><a class="headerlink" href="#cost-function-log-loss" title="Link to this heading">#</a></h2>
<p>Logistic regression uses <strong>cross-entropy loss</strong>:</p>
<div class="math notranslate nohighlight">
\[
J(\mathbf{w}, b) = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(m\)</span> = number of training examples.</p></li>
<li><p><span class="math notranslate nohighlight">\(y^{(i)}\)</span> = true label (<span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}^{(i)}\)</span> = predicted probability.</p></li>
</ul>
<p><strong>Intuition</strong>:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(y=1\)</span>, cost <span class="math notranslate nohighlight">\(\rightarrow \infty\)</span> when <span class="math notranslate nohighlight">\(\hat{y} \rightarrow 0\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(y=0\)</span>, cost <span class="math notranslate nohighlight">\(\rightarrow \infty\)</span> when <span class="math notranslate nohighlight">\(\hat{y} \rightarrow 1\)</span>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="gradient-descent-optimization">
<h2><strong>5. Gradient Descent Optimization</strong><a class="headerlink" href="#gradient-descent-optimization" title="Link to this heading">#</a></h2>
<p>Update weights <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and bias <span class="math notranslate nohighlight">\(b\)</span> to minimize <span class="math notranslate nohighlight">\(J(\mathbf{w}, b)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
w_j := w_j - \alpha \frac{\partial J}{\partial w_j}
\]</div>
<div class="math notranslate nohighlight">
\[
b := b - \alpha \frac{\partial J}{\partial b}
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> = learning rate.</p></li>
<li><p>The gradients are:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)}) x_j^{(i)}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})
\]</div>
</section>
<hr class="docutils" />
<section id="decision-boundary">
<h2><strong>6. Decision Boundary</strong><a class="headerlink" href="#decision-boundary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\hat{y} \geq 0.5\)</span>, predict <span class="math notranslate nohighlight">\(y=1\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\hat{y} &lt; 0.5\)</span>, predict <span class="math notranslate nohighlight">\(y=0\)</span>.</p></li>
<li><p>The boundary is where <span class="math notranslate nohighlight">\(\mathbf{w}^T \mathbf{x} + b = 0\)</span>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="multiclass-logistic-regression-softmax">
<h2><strong>7. Multiclass Logistic Regression (Softmax)</strong><a class="headerlink" href="#multiclass-logistic-regression-softmax" title="Link to this heading">#</a></h2>
<p>For <span class="math notranslate nohighlight">\(K &gt; 2\)</span> classes, use <strong>softmax regression</strong>:</p>
<div class="math notranslate nohighlight">
\[
P(y=k | \mathbf{x}) = \frac{e^{\mathbf{w}_k^T \mathbf{x}}}{\sum_{j=1}^K e^{\mathbf{w}_j^T \mathbf{x}}}
\]</div>
</section>
<hr class="docutils" />
<section id="assumptions-limitations">
<h2><strong>8. Assumptions &amp; Limitations</strong><a class="headerlink" href="#assumptions-limitations" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Assumes a <strong>linear decision boundary</strong> (use feature engineering for non-linear cases).</p></li>
<li><p>Sensitive to <strong>outliers</strong> (but less than linear regression).</p></li>
<li><p>Requires <strong>large datasets</strong> for stable weight estimates.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="example-in-python-scikit-learn">
<h2><strong>Example in Python (Scikit-learn)</strong><a class="headerlink" href="#example-in-python-scikit-learn" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>  
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>  
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>  
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>  
</pre></div>
</div>
<section id="example-coin-flipping-with-maximum-likelihood-estimation-mle-and-maximum-a-posteriori-map">
<h3>Example: Coin Flipping with Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP)<a class="headerlink" href="#example-coin-flipping-with-maximum-likelihood-estimation-mle-and-maximum-a-posteriori-map" title="Link to this heading">#</a></h3>
<p>Imagine you flip a coin 10 times and get 7 heads. You want to estimate the probability of getting heads, denoted as <span class="math notranslate nohighlight">\(\theta\)</span>. Is the coin fair (<span class="math notranslate nohighlight">\(\theta = 0.5\)</span>) or biased? We’ll use MLE and MAP to find <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<hr class="docutils" />
<section id="mle-trust-only-the-data">
<h4><strong>MLE: Trust Only the Data</strong><a class="headerlink" href="#mle-trust-only-the-data" title="Link to this heading">#</a></h4>
<p>MLE picks the <span class="math notranslate nohighlight">\(\theta\)</span> that makes the observed data (7 heads in 10 flips) most likely.</p>
<ul class="simple">
<li><p>You got 7 heads and 3 tails. The probability of this happening is modeled as:
$<span class="math notranslate nohighlight">\(
P(\text{data}) = \theta^7 (1 - \theta)^3
\)</span>$</p></li>
<li><p>MLE finds the <span class="math notranslate nohighlight">\(\theta\)</span> that maximizes this. Intuitively, it’s like asking: “What <span class="math notranslate nohighlight">\(\theta\)</span> best explains 7 heads out of 10 flips?”</p></li>
<li><p>Without diving into calculus, the answer is the proportion of heads:
$<span class="math notranslate nohighlight">\(
\theta_{\text{MLE}} = \frac{\text{number of heads}}{\text{total flips}} = \frac{7}{10} = 0.7
\)</span>$</p></li>
<li><p><strong>Interpretation</strong>: MLE says the probability of heads is 0.7, based purely on the data. If you got no prior beliefs, this is your best guess.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="map-combine-data-with-a-prior-belief">
<h4><strong>MAP: Combine Data with a Prior Belief</strong><a class="headerlink" href="#map-combine-data-with-a-prior-belief" title="Link to this heading">#</a></h4>
<p>MAP also uses the data but adds a “prior belief” about <span class="math notranslate nohighlight">\(\theta\)</span>. Let’s assume you think the coin is probably fair (<span class="math notranslate nohighlight">\(\theta \approx 0.5\)</span>) but you’re open to it being slightly biased. We model this with a simple prior called a <strong>Beta(2, 2)</strong> prior, which favors <span class="math notranslate nohighlight">\(\theta\)</span> near 0.5 but allows flexibility.</p>
<ul class="simple">
<li><p>The Beta(2, 2) prior is like saying, “Before flipping, I believe the coin is fair, as if I’ve seen 2 heads and 2 tails in the past.”</p></li>
<li><p>MAP combines:</p>
<ul>
<li><p><strong>Data</strong>: 7 heads, 3 tails from 10 flips.</p></li>
<li><p><strong>Prior</strong>: 2 “imaginary” heads, 2 “imaginary” tails.</p></li>
</ul>
</li>
<li><p>Add them together:</p>
<ul>
<li><p>Total heads = 7 (data) + 2 (prior) = 9</p></li>
<li><p>Total flips = 10 (data) + 4 (prior) = 14</p></li>
</ul>
</li>
<li><p>MAP estimate:
$<span class="math notranslate nohighlight">\(
\theta_{\text{MAP}} = \frac{\text{total heads}}{\text{total flips}} = \frac{9}{14} \approx 0.643
\)</span>$</p></li>
<li><p><strong>Interpretation</strong>: MAP says the probability of heads is about 0.643. It’s closer to 0.5 than MLE (0.7) because the prior belief in a fair coin “pulls” the estimate toward 0.5.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="key-difference">
<h3>Key Difference<a class="headerlink" href="#key-difference" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>MLE</strong>: Only uses the data (7 heads, 10 flips) → <span class="math notranslate nohighlight">\(\theta = 0.7\)</span>.</p></li>
<li><p><strong>MAP</strong>: Combines data with a prior belief (like 2 heads, 2 tails) → <span class="math notranslate nohighlight">\(\theta \approx 0.643\)</span>, which is smoothed toward a fair coin.</p></li>
</ul>
<p>This smoothing is helpful when you have little data. For example, if you flipped the coin once and got 1 head, MLE would say <span class="math notranslate nohighlight">\(\theta = 1.0\)</span> (100% heads), which is extreme. MAP, with the Beta(2, 2) prior, would give <span class="math notranslate nohighlight">\(\theta = \frac{1 + 2}{1 + 4} = \frac{3}{5} = 0.6\)</span>, a more cautious estimate.</p>
</section>
<hr class="docutils" />
<section id="linear-regression-recap">
<h3><strong>Linear Regression Recap</strong><a class="headerlink" href="#linear-regression-recap" title="Link to this heading">#</a></h3>
<p>Linear regression models a continuous target value:</p>
<div class="math notranslate nohighlight">
\[
y = \vec{\theta}^\intercal \vec{x} + b
\]</div>
<ul class="simple">
<li><p>The cost function is usually <strong>Mean Squared Error (MSE)</strong>.</p></li>
<li><p>We use <strong>regularization</strong> (like L2/Ridge or L1/Lasso) to reduce overfitting.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="motivation-for-classification">
<h3><strong>Motivation for Classification</strong><a class="headerlink" href="#motivation-for-classification" title="Link to this heading">#</a></h3>
<blockquote>
<div><p>What if our target <span class="math notranslate nohighlight">\(y\)</span> is not a continuous number, but a <strong>class label</strong> — like “spam” or “not spam”?</p>
</div></blockquote>
<ul class="simple">
<li><p>Linear regression doesn’t work well for classification:</p>
<ul>
<li><p>It can predict values outside the <span class="math notranslate nohighlight">\([0, 1]\)</span> range.</p></li>
<li><p>It doesn’t model probabilities naturally.</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
</section>
</section>
<hr class="docutils" />
<section id="logistic-regression">
<h2>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Link to this heading">#</a></h2>
<p>Logistic regression is a binary classification algorithm used to predict the probability that an instance belongs to one of two classes (e.g., 0 or 1). It models the relationship between input features and the probability of a specific outcome using the <strong>sigmoid function</strong>, which maps any real-valued number to the range <span class="math notranslate nohighlight">\((0, 1)\)</span>. The model is trained by maximizing the likelihood of the observed data, typically using gradient-based optimization.</p>
<section id="key-components">
<h3>Key Components<a class="headerlink" href="#key-components" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Sigmoid Function</strong>: Converts a linear combination of features into a probability.</p></li>
<li><p><strong>Decision Boundary</strong>: Separates the two classes based on a probability threshold (usually 0.5).</p></li>
<li><p><strong>Maximum Likelihood Estimation (MLE)</strong>: Optimizes the model parameters to best fit the training data.</p></li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="sigmoid-function">
<h2>1. Sigmoid Function<a class="headerlink" href="#sigmoid-function" title="Link to this heading">#</a></h2>
<p>The sigmoid function is defined as:</p>
<div class="math notranslate nohighlight">
\[\sigma(z) = \frac{1}{1 + e^{-z}}\]</div>
<p>where <span class="math notranslate nohighlight">\(z = \vec{\theta}^\intercal \vec{x} + b\)</span> is a linear combination of:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\vec{x}\)</span>: Input feature vector.</p></li>
<li><p><span class="math notranslate nohighlight">\(\vec{\theta}\)</span>: Weight vector (model parameters).</p></li>
<li><p><span class="math notranslate nohighlight">\(b\)</span>: Bias term.</p></li>
</ul>
<section id="properties">
<h3>Properties<a class="headerlink" href="#properties" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Range</strong>: <span class="math notranslate nohighlight">\(\sigma(z) \in (0, 1)\)</span>, making it ideal for modeling probabilities.</p></li>
<li><p><strong>Interpretation</strong>: <span class="math notranslate nohighlight">\(\sigma(z) = P(y=1|\vec{x})\)</span>, the probability of class 1 given input <span class="math notranslate nohighlight">\(\vec{x}\)</span>.</p></li>
<li><p><strong>Complement</strong>: <span class="math notranslate nohighlight">\(P(y=0|\vec{x}) = 1 - \sigma(z)\)</span>.</p></li>
</ul>
</section>
<section id="why-use-e-x-in-the-sigmoid">
<h3>Why Use <span class="math notranslate nohighlight">\(e^x\)</span> in the Sigmoid?<a class="headerlink" href="#why-use-e-x-in-the-sigmoid" title="Link to this heading">#</a></h3>
<p>The exponential function <span class="math notranslate nohighlight">\(e^x\)</span> is chosen because:</p>
<ol class="arabic simple">
<li><p><strong>Non-linearity</strong>: Introduces non-linear behavior, allowing the model to capture complex patterns.</p></li>
<li><p><strong>Differentiability</strong>: <span class="math notranslate nohighlight">\(e^x\)</span> has a simple derivative, facilitating gradient-based optimization.</p></li>
<li><p><strong>Convexity</strong>: The resulting log-likelihood is convex, ensuring a single global minimum during optimization.</p></li>
</ol>
<p>Other bases (e.g., <span class="math notranslate nohighlight">\(2^x\)</span>) could work but lack the mathematical convenience of <span class="math notranslate nohighlight">\(e^x\)</span>.</p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">division</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">grad_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">grad_sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;derivative of sigmoid&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Sigmoid&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2958f018cec914cbbf62a193e9b9a9b2e52d1461d8304f2053fc016826e63da3.png" src="_images/2958f018cec914cbbf62a193e9b9a9b2e52d1461d8304f2053fc016826e63da3.png" />
</div>
</div>
</section>
</section>
<section id="id1">
<h2>2. Decision Boundary<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>The decision boundary is the set of points where the predicted probability is 0.5:</p>
<div class="math notranslate nohighlight">
\[\sigma(z) = 0.5 \implies z = 0 \implies \vec{\theta}^\intercal \vec{x} + b = 0\]</div>
<p>This equation defines a <strong>hyperplane</strong> in the feature space:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(z &gt; 0\)</span>, then <span class="math notranslate nohighlight">\(\sigma(z) &gt; 0.5\)</span>, and the instance is classified as class 1.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(z &lt; 0\)</span>, then <span class="math notranslate nohighlight">\(\sigma(z) &lt; 0.5\)</span>, and the instance is classified as class 0.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="model-formulation">
<h2>3. Model Formulation<a class="headerlink" href="#model-formulation" title="Link to this heading">#</a></h2>
<p>Logistic regression assumes the class posterior probabilities follow the sigmoid form:</p>
<div class="math notranslate nohighlight">
\[P(C_1|\vec{x}) = \sigma(\vec{\theta}^\intercal \vec{x} + b)\]</div>
<div class="math notranslate nohighlight">
\[P(C_2|\vec{x}) = 1 - P(C_1|\vec{x}) = 1 - \sigma(\vec{\theta}^\intercal \vec{x} + b)\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\vec{x}\)</span> may be raw features or transformed features <span class="math notranslate nohighlight">\(\phi(\vec{x})\)</span> (e.g., polynomial features).</p>
</section>
<hr class="docutils" />
<section id="training-with-maximum-likelihood-estimation-mle">
<h2>4. Training with Maximum Likelihood Estimation (MLE)<a class="headerlink" href="#training-with-maximum-likelihood-estimation-mle" title="Link to this heading">#</a></h2>
<p>The goal is to find the parameters <span class="math notranslate nohighlight">\(\vec{\theta}\)</span> and <span class="math notranslate nohighlight">\(b\)</span> that maximize the likelihood of the observed data.</p>
<section id="training-data">
<h3>Training Data<a class="headerlink" href="#training-data" title="Link to this heading">#</a></h3>
<p>The training set is:</p>
<div class="math notranslate nohighlight">
\[\Psi = \{(\vec{x}_i, t_i)\}_{i=1}^m, \quad t_i \in \{0, 1\}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\vec{x}_i\)</span>: Feature vector for the <span class="math notranslate nohighlight">\(i\)</span>-th instance.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_i\)</span>: Target label (0 or 1).</p></li>
</ul>
</section>
<section id="likelihood-function">
<h3>Likelihood Function<a class="headerlink" href="#likelihood-function" title="Link to this heading">#</a></h3>
<p>The likelihood of the data given the parameters is:</p>
<div class="math notranslate nohighlight">
\[P(\Psi|\vec{\theta}, b) = \prod_{i=1}^m P(t_i|\vec{x}_i)\]</div>
<p>Since <span class="math notranslate nohighlight">\(t_i \in \{0, 1\}\)</span>, we can write:</p>
<div class="math notranslate nohighlight">
\[P(t_i|\vec{x}_i) = \sigma(\vec{\theta}^\intercal \vec{x}_i + b)^{t_i} \cdot (1 - \sigma(\vec{\theta}^\intercal \vec{x}_i + b))^{1 - t_i}\]</div>
<p>Thus, the likelihood is:</p>
<div class="math notranslate nohighlight">
\[P(\Psi|\vec{\theta}, b) = \prod_{i=1}^m \sigma(\vec{\theta}^\intercal \vec{x}_i + b)^{t_i} \cdot (1 - \sigma(\vec{\theta}^\intercal \vec{x}_i + b))^{1 - t_i}\]</div>
</section>
<hr class="docutils" />
<section id="basic-logarithm-rules">
<h3>Basic Logarithm Rules<a class="headerlink" href="#basic-logarithm-rules" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(b\)</span> be the base (<span class="math notranslate nohighlight">\(b &gt; 0\)</span> and <span class="math notranslate nohighlight">\(b \neq 1\)</span>), and let <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> be positive numbers.</p>
<ol class="arabic simple">
<li><p><strong>Definition:</strong>
$<span class="math notranslate nohighlight">\(\log_b(x) = y \iff b^y = x\)</span>$</p></li>
<li><p><strong>Product Rule:</strong>
$<span class="math notranslate nohighlight">\(\log_b(xy) = \log_b(x) + \log_b(y)\)</span>$</p></li>
<li><p><strong>Quotient Rule:</strong>
$<span class="math notranslate nohighlight">\(\log_b\left(\frac{x}{y}\right) = \log_b(x) - \log_b(y)\)</span>$</p></li>
<li><p><strong>Power Rule:</strong>
$<span class="math notranslate nohighlight">\(\log_b(x^p) = p \cdot \log_b(x)\)</span>$</p></li>
<li><p><strong>Change of Base Rule:</strong>
$<span class="math notranslate nohighlight">\(\log_b(x) = \frac{\log_c(x)}{\log_c(b)}\)</span><span class="math notranslate nohighlight">\(
where \)</span>c<span class="math notranslate nohighlight">\( is any valid base (\)</span>c &gt; 0<span class="math notranslate nohighlight">\( and \)</span>c \neq 1<span class="math notranslate nohighlight">\(). A common form uses the natural logarithm (\)</span>\ln<span class="math notranslate nohighlight">\() or the common logarithm (\)</span>\log_{10}<span class="math notranslate nohighlight">\():
\)</span><span class="math notranslate nohighlight">\(\log_b(x) = \frac{\ln(x)}{\ln(b)} = \frac{\log_{10}(x)}{\log_{10}(b)}\)</span>$</p></li>
<li><p><strong>Logarithm of the Base:</strong>
$<span class="math notranslate nohighlight">\(\log_b(b) = 1\)</span>$</p></li>
<li><p><strong>Logarithm of One:</strong>
$<span class="math notranslate nohighlight">\(\log_b(1) = 0\)</span>$</p></li>
</ol>
</section>
<section id="rules-involving-e-x-and-ln-x-natural-logarithm">
<h3>Rules Involving <span class="math notranslate nohighlight">\(e^x\)</span> and <span class="math notranslate nohighlight">\(\ln(x)\)</span> (Natural Logarithm)<a class="headerlink" href="#rules-involving-e-x-and-ln-x-natural-logarithm" title="Link to this heading">#</a></h3>
<p>The natural logarithm (<span class="math notranslate nohighlight">\(\ln\)</span>) has base <span class="math notranslate nohighlight">\(e \approx 2.71828\)</span>.</p>
<ol class="arabic simple">
<li><p><strong>Definition:</strong>
$<span class="math notranslate nohighlight">\(\ln(x) = y \iff e^y = x\)</span>$</p></li>
<li><p><strong>Inverse Relationships:</strong>
$<span class="math notranslate nohighlight">\(e^{\ln(x)} = x\)</span><span class="math notranslate nohighlight">\(   \)</span><span class="math notranslate nohighlight">\(\ln(e^x) = x\)</span>$</p></li>
</ol>
<p>These rules are fundamental for working with logarithmic and exponential functions.</p>
</section>
</section>
<hr class="docutils" />
<section id="examples-of-calculating-e-z-or-e-z">
<h2>Examples of Calculating <span class="math notranslate nohighlight">\(e(z)\)</span> or <span class="math notranslate nohighlight">\(e^z\)</span><a class="headerlink" href="#examples-of-calculating-e-z-or-e-z" title="Link to this heading">#</a></h2>
<section id="example-1-e-0">
<h3>Example 1: <span class="math notranslate nohighlight">\(e(0)\)</span><a class="headerlink" href="#example-1-e-0" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Calculation</strong>: <span class="math notranslate nohighlight">\(e(0) = e^0 = 1\)</span></p></li>
<li><p><strong>Result</strong>: <span class="math notranslate nohighlight">\(e(0) = 1\)</span></p></li>
<li><p><strong>Explanation</strong>: Any number raised to the power of 0 is 1.</p></li>
</ul>
</section>
<section id="example-2-e-1">
<h3>Example 2: <span class="math notranslate nohighlight">\(e(1)\)</span><a class="headerlink" href="#example-2-e-1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Calculation</strong>: <span class="math notranslate nohighlight">\(e(1) = e^1 = e \approx 2.71828\)</span></p></li>
<li><p><strong>Result</strong>: <span class="math notranslate nohighlight">\(e(1) \approx 2.718\)</span></p></li>
<li><p><strong>Explanation</strong>: This is the value of <span class="math notranslate nohighlight">\(e\)</span> itself, approximated to three decimal places.</p></li>
</ul>
</section>
<section id="example-3-e-2">
<h3>Example 3: <span class="math notranslate nohighlight">\(e(2)\)</span><a class="headerlink" href="#example-3-e-2" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Calculation</strong>: <span class="math notranslate nohighlight">\(e(2) = e^2 \approx 2.71828^2 \approx 7.38906\)</span></p></li>
<li><p><strong>Result</strong>: <span class="math notranslate nohighlight">\(e(2) \approx 7.389\)</span></p></li>
<li><p><strong>Explanation</strong>: Square the value of <span class="math notranslate nohighlight">\(e\)</span>. Using a calculator, <span class="math notranslate nohighlight">\(2.71828^2 \approx 7.389\)</span>.</p></li>
</ul>
</section>
<section id="example-4-e-1">
<h3>Example 4: <span class="math notranslate nohighlight">\(e(-1)\)</span><a class="headerlink" href="#example-4-e-1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Calculation</strong>: <span class="math notranslate nohighlight">\(e(-1) = e^{-1} = \frac{1}{e} \approx \frac{1}{2.71828} \approx 0.36788\)</span></p></li>
<li><p><strong>Result</strong>: <span class="math notranslate nohighlight">\(e(-1) \approx 0.368\)</span></p></li>
<li><p><strong>Explanation</strong>: A negative exponent means the reciprocal of <span class="math notranslate nohighlight">\(e^1\)</span>. So, <span class="math notranslate nohighlight">\(e(-1) = 1/e\)</span>.</p></li>
</ul>
</section>
<section id="example-5-e-0-5">
<h3>Example 5: <span class="math notranslate nohighlight">\(e(0.5)\)</span><a class="headerlink" href="#example-5-e-0-5" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Calculation</strong>: <span class="math notranslate nohighlight">\(e(0.5) = e^{0.5} \approx 2.71828^{0.5} \approx \sqrt{2.71828} \approx 1.64872\)</span></p></li>
<li><p><strong>Result</strong>: <span class="math notranslate nohighlight">\(e(0.5) \approx 1.649\)</span></p></li>
<li><p><strong>Explanation</strong>: The exponent 0.5 means the square root of <span class="math notranslate nohighlight">\(e\)</span>. Using a calculator, <span class="math notranslate nohighlight">\(\sqrt{2.71828} \approx 1.649\)</span>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="notes">
<h2>Notes<a class="headerlink" href="#notes" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>Approximations</strong>: The results are rounded to three decimal places for simplicity. In practice, you’d use a calculator or software (e.g., Python’s <code class="docutils literal notranslate"><span class="pre">math.exp()</span></code> or NumPy’s <code class="docutils literal notranslate"><span class="pre">np.exp()</span></code>) for precise values.</p></li>
<li><p><strong>Context</strong>: These calculations are common in machine learning (e.g., softmax function) and other fields like statistics and physics.</p></li>
<li><p><strong>How to Compute</strong>: If you don’t have a calculator, you can use software like Python:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="nb">print</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># Outputs ~7.389</span>
</pre></div>
</div>
</li>
</ul>
<p>Let me know if you want more examples or help with a specific value!</p>
<section id="log-likelihood">
<h3>Log-Likelihood<a class="headerlink" href="#log-likelihood" title="Link to this heading">#</a></h3>
<p>To simplify optimization, we maximize the <strong>log-likelihood</strong> (since the log is monotonic):</p>
<div class="math notranslate nohighlight">
\[\ell(\vec{\theta}, b) = \log P(\Psi|\vec{\theta}, b) = \sum_{i=1}^m \left[ t_i \log \sigma(\vec{\theta}^\intercal \vec{x}_i + b) + (1 - t_i) \log (1 - \sigma(\vec{\theta}^\intercal \vec{x}_i + b)) \right]\]</div>
</section>
<section id="optimization">
<h3>Optimization<a class="headerlink" href="#optimization" title="Link to this heading">#</a></h3>
<p>We maximize <span class="math notranslate nohighlight">\(\ell(\vec{\theta}, b)\)</span> (or equivalently, minimize <span class="math notranslate nohighlight">\(-\ell(\vec{\theta}, b)\)</span>) using gradient-based methods (e.g., gradient ascent). The gradients are computed with respect to <span class="math notranslate nohighlight">\(\vec{\theta}\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p>
</section>
</section>
<hr class="docutils" />
<section id="derivation-of-the-sigmoid-function">
<h2>5. Derivation of the Sigmoid Function<a class="headerlink" href="#derivation-of-the-sigmoid-function" title="Link to this heading">#</a></h2>
<p>The sigmoid function arises naturally from modeling the log-odds (logit) as a linear function.</p>
<section id="step-by-step-derivation">
<h3>Step-by-Step Derivation<a class="headerlink" href="#step-by-step-derivation" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p><strong>Odds</strong>: The odds of class 1 are:</p>
<div class="math notranslate nohighlight">
\[\text{Odds} = \frac{P(y=1|\vec{x})}{P(y=0|\vec{x})} = \frac{p}{1 - p}\]</div>
</li>
<li><p><strong>Logit</strong>: The log-odds (logit) is assumed to be a linear function of the features:</p>
<div class="math notranslate nohighlight">
\[\ln \left( \frac{p}{1 - p} \right) = \vec{\theta}^\intercal \vec{x} + b\]</div>
</li>
<li><p><strong>Exponentiate Both Sides</strong>:</p>
<div class="math notranslate nohighlight">
\[\frac{p}{1 - p} = e^{\vec{\theta}^\intercal \vec{x} + b}\]</div>
</li>
<li><p><strong>Solve for <span class="math notranslate nohighlight">\(p\)</span></strong>:</p>
<p>Let <span class="math notranslate nohighlight">\(z = \vec{\theta}^\intercal \vec{x} + b\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[p = e^z (1 - p)\]</div>
<div class="math notranslate nohighlight">
\[p = e^z - p e^z\]</div>
<div class="math notranslate nohighlight">
\[p + p e^z = e^z\]</div>
<div class="math notranslate nohighlight">
\[p (1 + e^z) = e^z\]</div>
<div class="math notranslate nohighlight">
\[p = \frac{e^z}{1 + e^z} = \frac{1}{1 + e^{-z}}\]</div>
</li>
</ol>
<p>This is the sigmoid function: <span class="math notranslate nohighlight">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span>.</p>
</section>
</section>
<hr class="docutils" />
<section id="gradient-of-the-sigmoid-function">
<h2>6. Gradient of the Sigmoid Function<a class="headerlink" href="#gradient-of-the-sigmoid-function" title="Link to this heading">#</a></h2>
<p>The derivative of the sigmoid function is used in gradient-based optimization.</p>
<section id="derivation">
<h3>Derivation<a class="headerlink" href="#derivation" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\sigma(x) = \frac{1}{1 + e^{-x}}\)</span>. Compute the derivative:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{dx} \sigma(x) = \frac{d}{dx} \left( 1 + e^{-x} \right)^{-1}\]</div>
<p>Using the chain rule:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{dx} \left( 1 + e^{-x} \right)^{-1} = - \left( 1 + e^{-x} \right)^{-2} \cdot \frac{d}{dx} \left( 1 + e^{-x} \right)\]</div>
<p>The derivative of the inner function is:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{dx} \left( 1 + e^{-x} \right) = 0 + (-1) \cdot e^{-x}  = -e^{-x}\]</div>
<p>So:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{dx} \sigma(x) = - \left( 1 + e^{-x} \right)^{-2} \cdot (-e^{-x}) = \frac{e^{-x}}{\left( 1 + e^{-x} \right)^2}\]</div>
<p>Simplify:</p>
<div class="math notranslate nohighlight">
\[\frac{e^{-x}}{\left( 1 + e^{-x} \right)^2} = \frac{1}{1 + e^{-x}} \cdot \frac{e^{-x}}{1 + e^{-x}}\]</div>
<p>Notice that:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{1 + e^{-x}} = \sigma(x)\]</div>
<p>And:</p>
<div class="math notranslate nohighlight">
\[\frac{e^{-x}}{1 + e^{-x}} = \frac{(1 + e^{-x}) - 1}{1 + e^{-x}} = 1 - \frac{1}{1 + e^{-x}} = 1 - \sigma(x)\]</div>
<p>Thus:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{dx} \sigma(x) = \sigma(x) \cdot (1 - \sigma(x))\]</div>
<p>This derivative is crucial for computing gradients during optimization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Log(X)&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sigmoid(X)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Log(Sigmoid(X))&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Plot for y = 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3fed128352657425bc22a9ed38768d7914976bad3930272cda8622b10396882f.png" src="_images/3fed128352657425bc22a9ed38768d7914976bad3930272cda8622b10396882f.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Log(Sigmoid(X))&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sigmoid(X)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;-Log(1 - Sigmoid(X))&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Plot for y = 0&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8e9f0b0b0c37412739a61b67ffdb4e4c2287255ba26eaae6cdffcad24a171478.png" src="_images/8e9f0b0b0c37412739a61b67ffdb4e4c2287255ba26eaae6cdffcad24a171478.png" />
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="cost-function-and-optimization-objective-for-logistic-regression">
<h2>Cost Function and Optimization Objective for Logistic Regression<a class="headerlink" href="#cost-function-and-optimization-objective-for-logistic-regression" title="Link to this heading">#</a></h2>
<p>Logistic regression aims to find parameters that best predict the probability of an instance belonging to one of two classes. To achieve this, we define a <strong>cost function</strong> (or loss function) that measures how well the model’s predictions match the observed data. The cost function is optimized using gradient-based methods to update the model parameters.</p>
<section id="why-a-specific-cost-function">
<h3>Why a Specific Cost Function?<a class="headerlink" href="#why-a-specific-cost-function" title="Link to this heading">#</a></h3>
<p>Unlike linear regression, which uses mean squared error, logistic regression uses a <strong>log-loss</strong> (or <strong>binary cross-entropy</strong>) derived from the negative log-likelihood. This choice is motivated by:</p>
<ol class="arabic simple">
<li><p><strong>Mathematical Convenience</strong>: Logarithms convert products to sums, simplifying differentiation.</p></li>
<li><p><strong>Smoothness</strong>: The function is smooth and differentiable, aiding optimization.</p></li>
<li><p><strong>Numerical Stability</strong>: Logarithms handle very small probabilities (e.g., <span class="math notranslate nohighlight">\(4 \times 10^{-45}\)</span>) effectively.</p></li>
<li><p><strong>Convexity</strong>: The negative log-likelihood is convex, ensuring a single global minimum and no local optima.</p></li>
<li><p><strong>Probabilistic Interpretation</strong>: It aligns with maximum likelihood estimation (MLE), directly modeling the likelihood of the data.</p></li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="cost-function">
<h2>1. Cost Function<a class="headerlink" href="#cost-function" title="Link to this heading">#</a></h2>
<p>The cost function measures the error for a single prediction. For a given instance with features <span class="math notranslate nohighlight">\(\vec{x}\)</span>, true label <span class="math notranslate nohighlight">\(y \in \{0, 1\}\)</span>, and predicted probability <span class="math notranslate nohighlight">\(h_\theta(\vec{x}) = \sigma(\vec{\theta}^\intercal \vec{x})\)</span>, the cost is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{Cost}(h_\theta(\vec{x}), y) =
\begin{cases}
-\log(h_\theta(\vec{x})), &amp; \text{if } y = 1 \\
-\log(1 - h_\theta(\vec{x})), &amp; \text{if } y = 0
\end{cases}
\end{split}\]</div>
<p>where the sigmoid function is:</p>
<div class="math notranslate nohighlight">
\[h_\theta(\vec{x}) = \sigma(\vec{\theta}^\intercal \vec{x}) = \frac{1}{1 + e^{-\vec{\theta}^\intercal \vec{x}}}\]</div>
<section id="intuition">
<h3>Intuition<a class="headerlink" href="#intuition" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(y = 1\)</span>, we want <span class="math notranslate nohighlight">\(h_\theta(\vec{x}) \approx 1\)</span>. If <span class="math notranslate nohighlight">\(h_\theta(\vec{x})\)</span> is close to 1, <span class="math notranslate nohighlight">\(-\log(h_\theta(\vec{x}))\)</span> is small (low penalty). If <span class="math notranslate nohighlight">\(h_\theta(\vec{x})\)</span> is close to 0, <span class="math notranslate nohighlight">\(-\log(h_\theta(\vec{x}))\)</span> is large (high penalty).</p></li>
<li><p>If <span class="math notranslate nohighlight">\(y = 0\)</span>, we want <span class="math notranslate nohighlight">\(h_\theta(\vec{x}) \approx 0\)</span>. If <span class="math notranslate nohighlight">\(h_\theta(\vec{x})\)</span> is close to 0, <span class="math notranslate nohighlight">\(-\log(1 - h_\theta(\vec{x}))\)</span> is small. If <span class="math notranslate nohighlight">\(h_\theta(\vec{x})\)</span> is close to 1, the penalty is large.</p></li>
</ul>
</section>
<section id="compact-form">
<h3>Compact Form<a class="headerlink" href="#compact-form" title="Link to this heading">#</a></h3>
<p>The piecewise cost can be combined into a single expression:</p>
<div class="math notranslate nohighlight">
\[\text{Cost}(h_\theta(\vec{x}), y) = -y \log(h_\theta(\vec{x})) - (1 - y) \log(1 - h_\theta(\vec{x}))\]</div>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(y = 1\)</span>, the equation becomes <span class="math notranslate nohighlight">\(-\log(h_\theta(\vec{x}))\)</span>.</p></li>
<li><p>When <span class="math notranslate nohighlight">\(y = 0\)</span>, it becomes <span class="math notranslate nohighlight">\(-\log(1 - h_\theta(\vec{x}))\)</span>.</p></li>
</ul>
<p>This is the <strong>binary cross-entropy loss</strong> for a single instance.</p>
</section>
<section id="full-cost-function">
<h3>Full Cost Function<a class="headerlink" href="#full-cost-function" title="Link to this heading">#</a></h3>
<p>For a dataset with <span class="math notranslate nohighlight">\(m\)</span> instances <span class="math notranslate nohighlight">\(\{(\vec{x}^{(i)}, y^{(i)})\}_{i=1}^m\)</span>, the total cost function (average loss) is:</p>
<div class="math notranslate nohighlight">
\[J(\vec{\theta}) = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log(h_\theta(\vec{x}^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(\vec{x}^{(i)})) \right]\]</div>
<p>This is the <strong>negative log-likelihood</strong> averaged over all instances. Minimizing <span class="math notranslate nohighlight">\(J(\vec{\theta})\)</span> is equivalent to maximizing the likelihood of the observed data.</p>
</section>
</section>
<hr class="docutils" />
<section id="entropy-measure-of-uncertainty">
<h2>1. <strong>Entropy: Measure of Uncertainty</strong><a class="headerlink" href="#entropy-measure-of-uncertainty" title="Link to this heading">#</a></h2>
<p>Entropy comes from information theory. It quantifies the <strong>uncertainty</strong> or <strong>impurity</strong> in a probability distribution.</p>
<section id="definition">
<h3>Definition:<a class="headerlink" href="#definition" title="Link to this heading">#</a></h3>
<p>For a binary classification problem with a probability <span class="math notranslate nohighlight">\(p\)</span> of success (e.g., class 1), the <strong>entropy</strong> is:</p>
<div class="math notranslate nohighlight">
\[
H(p) = -p \log_2(p) - (1 - p) \log_2(1 - p)
\]</div>
<ul class="simple">
<li><p>Entropy is <strong>0</strong> when the outcome is certain (i.e., <span class="math notranslate nohighlight">\(p = 0\)</span> or <span class="math notranslate nohighlight">\(p = 1\)</span>).</p></li>
<li><p>Entropy is <strong>maximum (1 bit)</strong> when <span class="math notranslate nohighlight">\(p = 0.5\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="c1"># Clip to avoid log(0)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>

<span class="c1"># Plot entropy for p in [0, 1]</span>
<span class="n">p_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">entropy_vals</span> <span class="o">=</span> <span class="n">entropy</span><span class="p">(</span><span class="n">p_vals</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_vals</span><span class="p">,</span> <span class="n">entropy_vals</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Probability (p)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Entropy (bits)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Entropy of a Binary Distribution&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/28becb34143850ea9295ea520b60ac4e2c9932cfc865b4833b385bb2bc1e379a.png" src="_images/28becb34143850ea9295ea520b60ac4e2c9932cfc865b4833b385bb2bc1e379a.png" />
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="log-loss-binary-cross-entropy">
<h2>2. <strong>Log-Loss / Binary Cross-Entropy</strong><a class="headerlink" href="#log-loss-binary-cross-entropy" title="Link to this heading">#</a></h2>
<p>While entropy measures uncertainty of a <strong>distribution</strong>, <strong>log-loss</strong> (or binary cross-entropy) measures the <strong>error</strong> of a predicted probability <span class="math notranslate nohighlight">\(\hat{y}\)</span> compared to the true label <span class="math notranslate nohighlight">\(y \in \{0, 1\}\)</span>.</p>
<section id="formula">
<h3>Formula:<a class="headerlink" href="#formula" title="Link to this heading">#</a></h3>
<p>For a single prediction:</p>
<div class="math notranslate nohighlight">
\[
\text{LogLoss}(y, \hat{y}) = -[y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]
\]</div>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(y = 1\)</span>, only the <span class="math notranslate nohighlight">\(\log(\hat{y})\)</span> term remains.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(y = 0\)</span>, only the <span class="math notranslate nohighlight">\(\log(1 - \hat{y})\)</span> term remains.</p></li>
</ul>
<p>This is the <strong>same formula</strong> as entropy, but with <span class="math notranslate nohighlight">\(y\)</span> as the true class and <span class="math notranslate nohighlight">\(\hat{y}\)</span> as the predicted probability.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">log_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="c1"># Clip to avoid log(0)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span> <span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="c1"># Example: true label is 1</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">y_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">y_preds</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_preds</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;True Label = 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Predicted Probability ($\hat</span><span class="si">{y}</span><span class="s2">$)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Log-Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Binary Cross-Entropy Loss for $y = 1$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/207fafc972ea5640b9692f3f59ce51f340d39e49f11772c13938b3344ea5f48e.png" src="_images/207fafc972ea5640b9692f3f59ce51f340d39e49f11772c13938b3344ea5f48e.png" />
</div>
</div>
</section>
</section>
<section id="connection-to-maximum-likelihood-estimation-mle">
<h2>2. Connection to Maximum Likelihood Estimation (MLE)<a class="headerlink" href="#connection-to-maximum-likelihood-estimation-mle" title="Link to this heading">#</a></h2>
<section id="id2">
<h3>Likelihood Function<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>For a binary classification dataset, the probability of the observed label <span class="math notranslate nohighlight">\(y_i\)</span> given features <span class="math notranslate nohighlight">\(\vec{x}_i\)</span> is:</p>
<div class="math notranslate nohighlight">
\[P(y_i|\vec{x}_i, \vec{\theta}) = h_\theta(\vec{x}_i)^{y_i} \cdot (1 - h_\theta(\vec{x}_i))^{1 - y_i}\]</div>
<p>Assuming independence of instances, the likelihood of the entire dataset is:</p>
<div class="math notranslate nohighlight">
\[L(\vec{\theta}) = \prod_{i=1}^m P(y_i|\vec{x}_i, \vec{\theta}) = \prod_{i=1}^m h_\theta(\vec{x}_i)^{y_i} \cdot (1 - h_\theta(\vec{x}_i))^{1 - y_i}\]</div>
</section>
<section id="id3">
<h3>Log-Likelihood<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>To simplify optimization, we take the logarithm:</p>
<div class="math notranslate nohighlight">
\[\ell(\vec{\theta}) = \log L(\vec{\theta}) = \sum_{i=1}^m \left[ y_i \log h_\theta(\vec{x}_i) + (1 - y_i) \log (1 - h_\theta(\vec{x}_i)) \right]\]</div>
<p>The cost function <span class="math notranslate nohighlight">\(J(\vec{\theta})\)</span> is the negative log-likelihood scaled by <span class="math notranslate nohighlight">\(\frac{1}{m}\)</span>:</p>
<div class="math notranslate nohighlight">
\[J(\vec{\theta}) = -\frac{1}{m} \ell(\vec{\theta})\]</div>
<p>Maximizing <span class="math notranslate nohighlight">\(\ell(\vec{\theta})\)</span> (likelihood) is equivalent to minimizing <span class="math notranslate nohighlight">\(J(\vec{\theta})\)</span> (loss).</p>
</section>
</section>
<hr class="docutils" />
<section id="gradient-descent-and-parameter-updates">
<h2>3. Gradient Descent and Parameter Updates<a class="headerlink" href="#gradient-descent-and-parameter-updates" title="Link to this heading">#</a></h2>
<p>To minimize <span class="math notranslate nohighlight">\(J(\vec{\theta})\)</span>, we use <strong>gradient descent</strong>. We need the gradient of <span class="math notranslate nohighlight">\(J(\vec{\theta})\)</span> with respect to each parameter <span class="math notranslate nohighlight">\(\theta_j\)</span>.</p>
<section id="gradient-of-the-sigmoid">
<h3>Gradient of the Sigmoid<a class="headerlink" href="#gradient-of-the-sigmoid" title="Link to this heading">#</a></h3>
<p>Recall the sigmoid function and its derivative:</p>
<div class="math notranslate nohighlight">
\[\sigma(z) = \frac{1}{1 + e^{-z}}, \quad \frac{d}{dz} \sigma(z) = \sigma(z) \cdot (1 - \sigma(z))\]</div>
<p>For <span class="math notranslate nohighlight">\(h_\theta(\vec{x}) = \sigma(\vec{\theta}^\intercal \vec{x})\)</span>, the partial derivative with respect to <span class="math notranslate nohighlight">\(\theta_j\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_j} h_\theta(\vec{x}) = \frac{\partial}{\partial \theta_j} \sigma(\vec{\theta}^\intercal \vec{x}) = \sigma(\vec{\theta}^\intercal \vec{x}) \cdot (1 - \sigma(\vec{\theta}^\intercal \vec{x})) \cdot \frac{\partial}{\partial \theta_j} (\vec{\theta}^\intercal \vec{x})\]</div>
<p>Since <span class="math notranslate nohighlight">\(\vec{\theta}^\intercal \vec{x} = \theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_j} (\vec{\theta}^\intercal \vec{x}) = x_j\]</div>
<p>Thus:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_j} h_\theta(\vec{x}) = h_\theta(\vec{x}) \cdot (1 - h_\theta(\vec{x})) \cdot x_j\]</div>
</section>
<section id="gradient-of-the-cost-function">
<h3>Gradient of the Cost Function<a class="headerlink" href="#gradient-of-the-cost-function" title="Link to this heading">#</a></h3>
<p>Now, compute the partial derivative of <span class="math notranslate nohighlight">\(J(\vec{\theta})\)</span>:</p>
<div class="math notranslate nohighlight">
\[J(\vec{\theta}) = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log(h_\theta(\vec{x}^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(\vec{x}^{(i)})) \right]\]</div>
<p>Differentiate with respect to <span class="math notranslate nohighlight">\(\theta_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\vec{\theta})}{\partial \theta_j} = -\frac{1}{m} \sum_{i=1}^m \frac{\partial}{\partial \theta_j} \left[ y^{(i)} \log(h_\theta(\vec{x}^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(\vec{x}^{(i)})) \right]\]</div>
<p>For a single term:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_j} \left[ y^{(i)} \log(h_\theta(\vec{x}^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(\vec{x}^{(i)})) \right]\]</div>
<p>Apply the chain rule:</p>
<ul class="simple">
<li><p>First term: <span class="math notranslate nohighlight">\(y^{(i)} \log(h_\theta(\vec{x}^{(i)}))\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_j} \left[ y^{(i)} \log(h_\theta(\vec{x}^{(i)})) \right] = y^{(i)} \cdot \frac{1}{h_\theta(\vec{x}^{(i)})} \cdot \frac{\partial}{\partial \theta_j} h_\theta(\vec{x}^{(i)})\]</div>
<ul class="simple">
<li><p>Second term: <span class="math notranslate nohighlight">\((1 - y^{(i)}) \log(1 - h_\theta(\vec{x}^{(i)}))\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_j} \left[ (1 - y^{(i)}) \log(1 - h_\theta(\vec{x}^{(i)})) \right] = (1 - y^{(i)}) \cdot \frac{1}{1 - h_\theta(\vec{x}^{(i)})} \cdot \frac{\partial}{\partial \theta_j} (1 - h_\theta(\vec{x}^{(i)}))\]</div>
<p>Since <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \theta_j} (1 - h_\theta(\vec{x}^{(i)})) = -\frac{\partial}{\partial \theta_j} h_\theta(\vec{x}^{(i)})\)</span>, we get:</p>
<div class="math notranslate nohighlight">
\[= (1 - y^{(i)}) \cdot \frac{1}{1 - h_\theta(\vec{x}^{(i)})} \cdot \left( -\frac{\partial}{\partial \theta_j} h_\theta(\vec{x}^{(i)}) \right)\]</div>
<p>Combine:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_j} = \frac{y^{(i)}}{h_\theta(\vec{x}^{(i)})} \cdot \frac{\partial h_\theta(\vec{x}^{(i)})}{\partial \theta_j} - \frac{1 - y^{(i)}}{1 - h_\theta(\vec{x}^{(i)})} \cdot \frac{\partial h_\theta(\vec{x}^{(i)})}{\partial \theta_j}\]</div>
<p>Factor out <span class="math notranslate nohighlight">\(\frac{\partial h_\theta(\vec{x}^{(i)})}{\partial \theta_j}\)</span>:</p>
<div class="math notranslate nohighlight">
\[= \left( \frac{y^{(i)}}{h_\theta(\vec{x}^{(i)})} - \frac{1 - y^{(i)}}{1 - h_\theta(\vec{x}^{(i)})} \right) \cdot \frac{\partial h_\theta(\vec{x}^{(i)})}{\partial \theta_j}\]</div>
<p>Simplify the expression inside:</p>
<div class="math notranslate nohighlight">
\[\frac{y^{(i)}}{h_\theta(\vec{x}^{(i)})} - \frac{1 - y^{(i)}}{1 - h_\theta(\vec{x}^{(i)})} = \frac{y^{(i)} (1 - h_\theta(\vec{x}^{(i)})) - (1 - y^{(i)}) h_\theta(\vec{x}^{(i)})}{h_\theta(\vec{x}^{(i)}) (1 - h_\theta(\vec{x}^{(i)}))}\]</div>
<p>Numerator:</p>
<div class="math notranslate nohighlight">
\[y^{(i)} (1 - h_\theta(\vec{x}^{(i)})) - (1 - y^{(i)}) h_\theta(\vec{x}^{(i)}) = y^{(i)} - y^{(i)} h_\theta(\vec{x}^{(i)}) - h_\theta(\vec{x}^{(i)}) + y^{(i)} h_\theta(\vec{x}^{(i)}) = y^{(i)} - h_\theta(\vec{x}^{(i)})\]</div>
<p>So:</p>
<div class="math notranslate nohighlight">
\[\frac{y^{(i)} - h_\theta(\vec{x}^{(i)})}{h_\theta(\vec{x}^{(i)}) (1 - h_\theta(\vec{x}^{(i)}))}\]</div>
<p>Now substitute <span class="math notranslate nohighlight">\(\frac{\partial h_\theta(\vec{x}^{(i)})}{\partial \theta_j} = h_\theta(\vec{x}^{(i)}) (1 - h_\theta(\vec{x}^{(i)})) x_j^{(i)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_j} = \frac{y^{(i)} - h_\theta(\vec{x}^{(i)})}{h_\theta(\vec{x}^{(i)}) (1 - h_\theta(\vec{x}^{(i)}))} \cdot h_\theta(\vec{x}^{(i)}) (1 - h_\theta(\vec{x}^{(i)})) x_j^{(i)}\]</div>
<p>The denominator cancels, leaving:</p>
<div class="math notranslate nohighlight">
\[= (y^{(i)} - h_\theta(\vec{x}^{(i)})) x_j^{(i)}\]</div>
<p>Thus, the full gradient is:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\vec{\theta})}{\partial \theta_j} = -\frac{1}{m} \sum_{i=1}^m (y^{(i)} - h_\theta(\vec{x}^{(i)})) x_j^{(i)}\]</div>
<p>For gradient descent, we update in the direction of the negative gradient, so:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\vec{\theta})}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^m (h_\theta(\vec{x}^{(i)}) - y^{(i)}) x_j^{(i)}\]</div>
</section>
<section id="gradient-descent-update-rule">
<h3>Gradient Descent Update Rule<a class="headerlink" href="#gradient-descent-update-rule" title="Link to this heading">#</a></h3>
<p>The update rule for each parameter <span class="math notranslate nohighlight">\(\theta_j\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\theta_j := \theta_j - \alpha \cdot \frac{\partial J(\vec{\theta})}{\partial \theta_j} = \theta_j - \alpha \cdot \frac{1}{m} \sum_{i=1}^m (h_\theta(\vec{x}^{(i)}) - y^{(i)}) x_j^{(i)}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate. All <span class="math notranslate nohighlight">\(\theta_j\)</span> are updated simultaneously until convergence.</p>
</section>
<section id="similarity-to-linear-regression">
<h3>Similarity to Linear Regression<a class="headerlink" href="#similarity-to-linear-regression" title="Link to this heading">#</a></h3>
<p>This update rule resembles linear regression’s gradient descent, but here, <span class="math notranslate nohighlight">\(h_\theta(\vec{x}) = \sigma(\vec{\theta}^\intercal \vec{x})\)</span> is the sigmoid function, not a linear function.</p>
</section>
</section>
<hr class="docutils" />
<section id="interpretation-of-weights">
<h2>4. Interpretation of Weights<a class="headerlink" href="#interpretation-of-weights" title="Link to this heading">#</a></h2>
<p>In logistic regression, the model predicts the log-odds:</p>
<div class="math notranslate nohighlight">
\[\ln \left( \frac{P(y=1|\vec{x})}{P(y=0|\vec{x})} \right) = \vec{\theta}^\intercal \vec{x} = \theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n\]</div>
<p>The weight <span class="math notranslate nohighlight">\(\theta_j\)</span> represents the change in the <strong>log-odds</strong> for a unit increase in feature <span class="math notranslate nohighlight">\(x_j\)</span>, holding other features constant. Exponentiating <span class="math notranslate nohighlight">\(\theta_j\)</span> gives the odds ratio:</p>
<div class="math notranslate nohighlight">
\[e^{\theta_j} = \text{change in odds for a unit increase in } x_j\]</div>
<p>This is different from linear regression, where <span class="math notranslate nohighlight">\(\theta_j\)</span> directly represents the change in the output <span class="math notranslate nohighlight">\(y\)</span>.</p>
</section>
<hr class="docutils" />
<section id="maximum-a-posteriori-map-estimation-and-regularization">
<h2>5. Maximum A Posteriori (MAP) Estimation and Regularization<a class="headerlink" href="#maximum-a-posteriori-map-estimation-and-regularization" title="Link to this heading">#</a></h2>
<section id="map-estimation">
<h3>MAP Estimation<a class="headerlink" href="#map-estimation" title="Link to this heading">#</a></h3>
<p>MLE maximizes the likelihood <span class="math notranslate nohighlight">\(P(\vec{y}|\vec{X}, \vec{\theta})\)</span>. MAP incorporates a prior distribution <span class="math notranslate nohighlight">\(P(\vec{\theta})\)</span> and maximizes the posterior:</p>
<div class="math notranslate nohighlight">
\[P(\vec{\theta}|\vec{X}, \vec{y}) \propto P(\vec{y}|\vec{X}, \vec{\theta}) \cdot P(\vec{\theta})\]</div>
<p>Taking the logarithm:</p>
<div class="math notranslate nohighlight">
\[\vec{\theta}_{\text{MAP}} = \arg\max_{\vec{\theta}} \left[ \log P(\vec{y}|\vec{X}, \vec{\theta}) + \log P(\vec{\theta}) \right]\]</div>
</section>
<section id="gaussian-prior-and-l2-regularization">
<h3>Gaussian Prior and L2 Regularization<a class="headerlink" href="#gaussian-prior-and-l2-regularization" title="Link to this heading">#</a></h3>
<p>Assume a Gaussian prior on <span class="math notranslate nohighlight">\(\vec{\theta}\)</span>: <span class="math notranslate nohighlight">\(P(\vec{\theta}) \sim \mathcal{N}(0, \sigma^2 \mathbf{I})\)</span>. The log-prior is:</p>
<div class="math notranslate nohighlight">
\[\log P(\vec{\theta}) \propto -\frac{1}{2\sigma^2} \|\vec{\theta}\|_2^2\]</div>
<p>The MAP objective becomes:</p>
<div class="math notranslate nohighlight">
\[\vec{\theta}_{\text{MAP}} = \arg\max_{\vec{\theta}} \left[ \ell(\vec{\theta}) - \frac{\lambda}{2} \|\vec{\theta}\|_2^2 \right], \quad \lambda = \frac{1}{\sigma^2}\]</div>
<p>Equivalently, minimize:</p>
<div class="math notranslate nohighlight">
\[J(\vec{\theta}) = -\frac{1}{m} \ell(\vec{\theta}) + \frac{\lambda}{2} \|\vec{\theta}\|_2^2\]</div>
<p>This is the cross-entropy loss plus an <strong>L2 regularization</strong> term, which penalizes large weights to prevent overfitting, similar to ridge regression.</p>
</section>
<hr class="docutils" />
<section id="map-estimation-incorporating-prior-knowledge">
<h3>1. <strong>MAP Estimation: Incorporating Prior Knowledge</strong><a class="headerlink" href="#map-estimation-incorporating-prior-knowledge" title="Link to this heading">#</a></h3>
<p>Maximum Likelihood Estimation (MLE) focuses solely on maximizing the likelihood of the data given the model parameters, i.e., <span class="math notranslate nohighlight">\(P(\vec{y}|\vec{X}, \vec{\theta})\)</span>. However, MLE doesn’t account for prior beliefs about the parameters <span class="math notranslate nohighlight">\(\vec{\theta}\)</span>. MAP estimation improves on this by incorporating a <strong>prior distribution</strong> <span class="math notranslate nohighlight">\(P(\vec{\theta})\)</span>, which reflects what we believe about the parameters before seeing the data.</p>
<p>The <strong>posterior probability</strong> of the parameters given the data is given by Bayes’ theorem:</p>
<div class="math notranslate nohighlight">
\[ P(\vec{\theta}|\vec{X}, \vec{y}) = \frac{P(\vec{y}|\vec{X}, \vec{\theta}) \cdot P(\vec{\theta})}{P(\vec{y}|\vec{X})} \]</div>
<p>Since <span class="math notranslate nohighlight">\(P(\vec{y}|\vec{X})\)</span> is a constant (it doesn’t depend on <span class="math notranslate nohighlight">\(\vec{\theta}\)</span>), we can maximize the unnormalized posterior:</p>
<div class="math notranslate nohighlight">
\[ P(\vec{\theta}|\vec{X}, \vec{y}) \propto P(\vec{y}|\vec{X}, \vec{\theta}) \cdot P(\vec{\theta}) \]</div>
<p>To make optimization easier, we take the <strong>logarithm</strong> (since the log is a monotonic function, it preserves the maximum):</p>
<div class="math notranslate nohighlight">
\[ \vec{\theta}_{\text{MAP}} = \arg\max_{\vec{\theta}} \left[ \log P(\vec{y}|\vec{X}, \vec{\theta}) + \log P(\vec{\theta}) \right] \]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\log P(\vec{y}|\vec{X}, \vec{\theta})\)</span> is the <strong>log-likelihood</strong>, same as in MLE.</p></li>
<li><p><span class="math notranslate nohighlight">\(\log P(\vec{\theta})\)</span> is the <strong>log-prior</strong>, which encodes our prior beliefs about <span class="math notranslate nohighlight">\(\vec{\theta}\)</span>.</p></li>
</ul>
<p>The prior acts as a regularizer, guiding the solution toward parameter values that are more likely based on prior knowledge.</p>
</section>
<section id="id4">
<h3>2. <strong>Gaussian Prior and L2 Regularization</strong><a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>A common choice for the prior is a <strong>Gaussian distribution</strong> centered at zero, <span class="math notranslate nohighlight">\(P(\vec{\theta}) \sim \mathcal{N}(0, \sigma^2 \mathbf{I})\)</span>. This assumes that the parameters <span class="math notranslate nohighlight">\(\vec{\theta}\)</span> are likely to be small (close to zero), which encourages simpler models and helps prevent overfitting.</p>
<p>The probability density of a Gaussian prior is:</p>
<div class="math notranslate nohighlight">
\[ P(\vec{\theta}) \propto e\left( -\frac{1}{2\sigma^2} \|\vec{\theta}\|_2^2 \right) \]</div>
<p>Taking the logarithm:</p>
<div class="math notranslate nohighlight">
\[ \log P(\vec{\theta}) \propto -\frac{1}{2\sigma^2} \|\vec{\theta}\|_2^2 \]</div>
<p>Now, substitute this into the MAP objective:</p>
<div class="math notranslate nohighlight">
\[ \vec{\theta}_{\text{MAP}} = \arg\max_{\vec{\theta}} \left[ \log P(\vec{y}|\vec{X}, \vec{\theta}) - \frac{1}{2\sigma^2} \|\vec{\theta}\|_2^2 \right] \]</div>
<p>Let’s define <span class="math notranslate nohighlight">\(\lambda = \frac{1}{\sigma^2}\)</span>, where <span class="math notranslate nohighlight">\(\lambda\)</span> controls the strength of the prior (smaller <span class="math notranslate nohighlight">\(\sigma^2\)</span> means a stronger belief that <span class="math notranslate nohighlight">\(\vec{\theta}\)</span> should be close to zero). The objective becomes:</p>
<div class="math notranslate nohighlight">
\[ \vec{\theta}_{\text{MAP}} = \arg\max_{\vec{\theta}} \left[ \ell(\vec{\theta}) - \frac{\lambda}{2} \|\vec{\theta}\|_2^2 \right] \]</div>
<p>where <span class="math notranslate nohighlight">\(\ell(\vec{\theta}) = \log P(\vec{y}|\vec{X}, \vec{\theta})\)</span> is the log-likelihood.</p>
<p>To turn this into a <strong>minimization problem</strong> (common in optimization), we negate the objective and scale the log-likelihood by <span class="math notranslate nohighlight">\(\frac{1}{m}\)</span> (where <span class="math notranslate nohighlight">\(m\)</span> is the number of data points) for numerical stability:</p>
<div class="math notranslate nohighlight">
\[ J(\vec{\theta}) = -\frac{1}{m} \ell(\vec{\theta}) + \frac{\lambda}{2} \|\vec{\theta}\|_2^2 \]</div>
<p>This is the <strong>loss function</strong> (e.g., cross-entropy loss for classification) plus an <strong>L2 regularization term</strong> <span class="math notranslate nohighlight">\(\frac{\lambda}{2} \|\vec{\theta}\|_2^2\)</span>. The L2 term penalizes large values of <span class="math notranslate nohighlight">\(\vec{\theta}\)</span>, encouraging smaller weights, which leads to simpler models and reduces overfitting. This is mathematically equivalent to <strong>ridge regression</strong> in linear regression.</p>
</section>
<section id="why-l2-regularization-works">
<h3>3. <strong>Why L2 Regularization Works</strong><a class="headerlink" href="#why-l2-regularization-works" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Preventing Overfitting</strong>: Large weights can lead to overly complex models that fit noise in the training data. The L2 penalty discourages large weights, favoring smoother, more generalizable models.</p></li>
<li><p><strong>Connection to Gaussian Prior</strong>: The L2 term arises naturally from the Gaussian prior, which assumes parameters are likely to be small. The strength of regularization (<span class="math notranslate nohighlight">\(\lambda\)</span>) is inversely proportional to the variance of the prior (<span class="math notranslate nohighlight">\(\sigma^2\)</span>).</p></li>
<li><p><strong>Geometric Interpretation</strong>: The L2 term constrains the solution to lie closer to the origin in parameter space, balancing the trade-off between fitting the data (likelihood) and keeping parameters small (prior).</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="small-example-logistic-regression-with-map-and-l2-regularization">
<h2>Small Example: Logistic Regression with MAP and L2 Regularization<a class="headerlink" href="#small-example-logistic-regression-with-map-and-l2-regularization" title="Link to this heading">#</a></h2>
<p>Let’s apply MAP estimation with a Gaussian prior to a <strong>logistic regression</strong> problem.</p>
<section id="problem-setup">
<h3>Problem Setup<a class="headerlink" href="#problem-setup" title="Link to this heading">#</a></h3>
<p>Suppose we have a binary classification dataset with <span class="math notranslate nohighlight">\(m=2\)</span> data points:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\vec{X} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span>, <span class="math notranslate nohighlight">\(\vec{y} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}\)</span>.</p></li>
<li><p>Model parameters: <span class="math notranslate nohighlight">\(\vec{\theta} = \begin{bmatrix} \theta_1 \\ \theta_2 \end{bmatrix}\)</span>.</p></li>
<li><p>We assume a Gaussian prior: <span class="math notranslate nohighlight">\(P(\vec{\theta}) \sim \mathcal{N}(0, \sigma^2 \mathbf{I})\)</span>, with <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span>, so <span class="math notranslate nohighlight">\(\lambda = \frac{1}{\sigma^2} = 1\)</span>.</p></li>
</ul>
</section>
<section id="step-1-log-likelihood-for-logistic-regression">
<h3>Step 1: Log-Likelihood for Logistic Regression<a class="headerlink" href="#step-1-log-likelihood-for-logistic-regression" title="Link to this heading">#</a></h3>
<p>For logistic regression, the likelihood is:</p>
<div class="math notranslate nohighlight">
\[ P(\vec{y}|\vec{X}, \vec{\theta}) = \prod_{i=1}^m \sigma(\vec{x}_i^T \vec{\theta})^{y_i} (1 - \sigma(\vec{x}_i^T \vec{\theta}))^{1 - y_i} \]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span> is the sigmoid function.</p>
<p>The log-likelihood is:</p>
<div class="math notranslate nohighlight">
\[ \ell(\vec{\theta}) = \sum_{i=1}^m \left[ y_i \log \sigma(\vec{x}_i^T \vec{\theta}) + (1 - y_i) \log (1 - \sigma(\vec{x}_i^T \vec{\theta})) \right] \]</div>
<p>For our data:</p>
<ul class="simple">
<li><p>Point 1: <span class="math notranslate nohighlight">\(\vec{x}_1 = [1, 2]\)</span>, <span class="math notranslate nohighlight">\(y_1 = 0\)</span>, so <span class="math notranslate nohighlight">\(\vec{x}_1^T \vec{\theta} = \theta_1 + 2\theta_2\)</span>.</p></li>
<li><p>Point 2: <span class="math notranslate nohighlight">\(\vec{x}_2 = [3, 4]\)</span>, <span class="math notranslate nohighlight">\(y_2 = 1\)</span>, so <span class="math notranslate nohighlight">\(\vec{x}_2^T \vec{\theta} = 3\theta_1 + 4\theta_2\)</span>.</p></li>
</ul>
<p>The log-likelihood is:</p>
<div class="math notranslate nohighlight">
\[ \ell(\vec{\theta}) = \log (1 - \sigma(\theta_1 + 2\theta_2)) + \log \sigma(3\theta_1 + 4\theta_2) \]</div>
</section>
<section id="step-2-gaussian-prior">
<h3>Step 2: Gaussian Prior<a class="headerlink" href="#step-2-gaussian-prior" title="Link to this heading">#</a></h3>
<p>The log-prior with <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span> is:</p>
<div class="math notranslate nohighlight">
\[ \log P(\vec{\theta}) \propto -\frac{1}{2} \|\vec{\theta}\|_2^2 = -\frac{1}{2} (\theta_1^2 + \theta_2^2) \]</div>
</section>
<section id="step-3-map-objective">
<h3>Step 3: MAP Objective<a class="headerlink" href="#step-3-map-objective" title="Link to this heading">#</a></h3>
<p>The MAP objective is:</p>
<div class="math notranslate nohighlight">
\[ \vec{\theta}_{\text{MAP}} = \arg\max_{\vec{\theta}} \left[ \ell(\vec{\theta}) - \frac{1}{2} (\theta_1^2 + \theta_2^2) \right] \]</div>
<p>Equivalently, minimize the loss function (with <span class="math notranslate nohighlight">\(m=2\)</span>):</p>
<div class="math notranslate nohighlight">
\[ J(\vec{\theta}) = -\frac{1}{2} \ell(\vec{\theta}) + \frac{1}{2} (\theta_1^2 + \theta_2^2) \]</div>
</section>
<section id="step-4-interpretation">
<h3>Step 4: Interpretation<a class="headerlink" href="#step-4-interpretation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The term <span class="math notranslate nohighlight">\(-\frac{1}{2} \ell(\vec{\theta})\)</span> is the average negative log-likelihood (cross-entropy loss).</p></li>
<li><p>The term <span class="math notranslate nohighlight">\(\frac{1}{2} (\theta_1^2 + \theta_2^2)\)</span> is the L2 regularization term with <span class="math notranslate nohighlight">\(\lambda = 1\)</span>.</p></li>
<li><p>To find <span class="math notranslate nohighlight">\(\vec{\theta}_{\text{MAP}}\)</span>, we’d typically use gradient descent or another optimization method to minimize <span class="math notranslate nohighlight">\(J(\vec{\theta})\)</span>.</p></li>
</ul>
</section>
<section id="step-5-numerical-intuition">
<h3>Step 5: Numerical Intuition<a class="headerlink" href="#step-5-numerical-intuition" title="Link to this heading">#</a></h3>
<p>Suppose we test <span class="math notranslate nohighlight">\(\vec{\theta} = [0.5, 0.5]\)</span>:</p>
<ul class="simple">
<li><p>Compute <span class="math notranslate nohighlight">\(\vec{x}_1^T \vec{\theta} = 1 \cdot 0.5 + 2 \cdot 0.5 = 1.5\)</span>, so <span class="math notranslate nohighlight">\(\sigma(1.5) \approx 0.817\)</span>.</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(\vec{x}_2^T \vec{\theta} = 3 \cdot 0.5 + 4 \cdot 0.5 = 3.5\)</span>, so <span class="math notranslate nohighlight">\(\sigma(3.5) \approx 0.970\)</span>.</p></li>
<li><p>Log-likelihood: <span class="math notranslate nohighlight">\(\ell(\vec{\theta}) \approx \log(1 - 0.817) + \log(0.970) \approx \log(0.183) + \log(0.970) \approx -1.70\)</span>.</p></li>
<li><p>L2 term: <span class="math notranslate nohighlight">\(\frac{1}{2} (0.5^2 + 0.5^2) = \frac{1}{2} \cdot 0.5 = 0.25\)</span>.</p></li>
<li><p>Loss: <span class="math notranslate nohighlight">\(J(\vec{\theta}) \approx -\frac{1}{2} \cdot (-1.70) + 0.25 = 0.85 + 0.25 = 1.10\)</span>.</p></li>
</ul>
<p>The optimizer would adjust <span class="math notranslate nohighlight">\(\vec{\theta}\)</span> to minimize <span class="math notranslate nohighlight">\(J(\vec{\theta})\)</span>, balancing the fit to the data (log-likelihood) and the penalty on large weights (L2 term).</p>
</section>
</section>
<section id="mle-and-map-in-context">
<h2>6. MLE and MAP in Context<a class="headerlink" href="#mle-and-map-in-context" title="Link to this heading">#</a></h2>
<section id="maximum-likelihood-estimation-mle">
<h3>Maximum Likelihood Estimation (MLE)<a class="headerlink" href="#maximum-likelihood-estimation-mle" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Goal</strong>: Find <span class="math notranslate nohighlight">\(\vec{\theta}\)</span> that maximizes the likelihood <span class="math notranslate nohighlight">\(P(\vec{y}|\vec{X}, \vec{\theta})\)</span>.</p></li>
<li><p><strong>Process</strong>: Define the likelihood, take the log, and optimize.</p></li>
<li><p><strong>Example</strong>: For a coin with <span class="math notranslate nohighlight">\(k\)</span> heads in <span class="math notranslate nohighlight">\(n\)</span> flips, the likelihood is <span class="math notranslate nohighlight">\(L(p) = p^k (1-p)^{n-k}\)</span>. The MLE is <span class="math notranslate nohighlight">\(\hat{p} = \frac{k}{n}\)</span>.</p></li>
</ul>
</section>
<section id="maximum-a-posteriori-map">
<h3>Maximum A Posteriori (MAP)<a class="headerlink" href="#maximum-a-posteriori-map" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Goal</strong>: Incorporate prior beliefs <span class="math notranslate nohighlight">\(P(\vec{\theta})\)</span> to maximize the posterior.</p></li>
<li><p><strong>Process</strong>: Combine log-likelihood and log-prior, then optimize.</p></li>
<li><p><strong>Example</strong>: If you believe a coin is fair (prior <span class="math notranslate nohighlight">\(P(p) \sim \text{Beta}\)</span>), MAP adjusts the estimate based on both data and prior.</p></li>
</ul>
</section>
<section id="mle-vs-map">
<h3>MLE vs. MAP<a class="headerlink" href="#mle-vs-map" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>MLE</strong>: Data-driven, no assumptions about <span class="math notranslate nohighlight">\(\vec{\theta}\)</span>.</p></li>
<li><p><strong>MAP</strong>: Incorporates prior knowledge, leading to regularization in practice.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="example-biased-coin">
<h2>7. Example: Biased Coin<a class="headerlink" href="#example-biased-coin" title="Link to this heading">#</a></h2>
<p>Consider a dataset of coin flips: <span class="math notranslate nohighlight">\(\mathcal{D} = \{H, H, T, H, T\}\)</span> (3 heads, 2 tails).</p>
<section id="model">
<h3>Model<a class="headerlink" href="#model" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P_\theta(H) = \theta\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P_\theta(T) = 1 - \theta\)</span></p></li>
</ul>
</section>
<section id="likelihood">
<h3>Likelihood<a class="headerlink" href="#likelihood" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[L(\theta) = \theta^3 (1 - \theta)^2\]</div>
</section>
<section id="id5">
<h3>Log-Likelihood<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\ell(\theta) = 3 \log \theta + 2 \log (1 - \theta)\]</div>
</section>
<section id="mle-solution">
<h3>MLE Solution<a class="headerlink" href="#mle-solution" title="Link to this heading">#</a></h3>
<p>Differentiate and set to zero:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \ell}{\partial \theta} = \frac{3}{\theta} - \frac{2}{1 - \theta} = 0\]</div>
<div class="math notranslate nohighlight">
\[\frac{3}{\theta} = \frac{2}{1 - \theta} \implies 3 (1 - \theta) = 2 \theta \implies 3 - 3\theta = 2\theta \implies 3 = 5\theta \implies \theta = \frac{3}{5} = 0.6\]</div>
<p>The MLE estimate is <span class="math notranslate nohighlight">\(\theta = 0.6\)</span>, matching the proportion of heads.</p>
</section>
</section>
<hr class="docutils" />
<section id="conditional-maximum-likelihood">
<h2>8. Conditional Maximum Likelihood<a class="headerlink" href="#conditional-maximum-likelihood" title="Link to this heading">#</a></h2>
<p>Logistic regression models the conditional probability <span class="math notranslate nohighlight">\(P(y|\vec{x}, \vec{\theta})\)</span>. The log-likelihood is:</p>
<div class="math notranslate nohighlight">
\[\ell(\vec{\theta}) = \sum_{i=1}^m \log P(y^{(i)}|\vec{x}^{(i)}, \vec{\theta})\]</div>
<p>For logistic regression:</p>
<div class="math notranslate nohighlight">
\[P(y=1|\vec{x}, \vec{\theta}) = \sigma(\vec{\theta}^\intercal \vec{x}), \quad P(y=0|\vec{x}, \vec{\theta}) = 1 - \sigma(\vec{\theta}^\intercal \vec{x})\]</div>
<p>Maximizing <span class="math notranslate nohighlight">\(\ell(\vec{\theta})\)</span> pushes predictions toward 1 for <span class="math notranslate nohighlight">\(y=1\)</span> and 0 for <span class="math notranslate nohighlight">\(y=0\)</span>.</p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># our dataset is {H, H, T, H, T}; if theta = P(x=H), we get:</span>
<span class="n">coin_likelihood</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">theta</span><span class="o">*</span><span class="n">theta</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">theta</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta</span><span class="p">)</span>

<span class="n">theta_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Likelihood of the Data&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Parameter $\theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="mf">0.6</span><span class="p">],</span> <span class="p">[</span><span class="n">coin_likelihood</span><span class="p">(</span><span class="mf">0.6</span><span class="p">)])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span> <span class="n">coin_likelihood</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">));</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">log_likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The cost function, J(theta0, theta1) describing the goodness of fit.</span>
<span class="sd">    </span>
<span class="sd">    We added the 1e-6 term in order to avoid overflow (inf and -inf).</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    theta (np.array): d-dimensional vector of parameters</span>
<span class="sd">    X (np.array): (n,d)-dimensional design matrix</span>
<span class="sd">    y (np.array): n-dimensional vector of targets</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">yHat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">yHat</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">yHat</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;true label = 1&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;true label = 0&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Predicted probability $\hat</span><span class="si">{y}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Log loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="logistic-regression-from-scratch">
<h2>Logistic Regression from Scratch<a class="headerlink" href="#logistic-regression-from-scratch" title="Link to this heading">#</a></h2>
<p>Below is a Python implementation of logistic regression using NumPy, including gradient descent optimization and L2 regularization.</p>
<section id="explanation-of-code">
<h3>Explanation of Code:<a class="headerlink" href="#explanation-of-code" title="Link to this heading">#</a></h3>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>•	Sigmoid Function: Implements (\sigma(z) = \frac{1}{1 + e^{-z}}).
•	Loss Function: Computes binary cross-entropy plus L2 regularization.
•	Gradient Descent: Updates weights and bias using the gradients of the loss.
•	Prediction: Outputs probabilities (via predict_proba) or class labels (via predict).
•	Example: Generates synthetic 2D data, trains the model, and plots the loss curve.
</pre></div>
</div>
</section>
<section id="id6">
<h3>Notes:<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>•	The implementation includes L2 regularization (controlled by lambda_reg), which corresponds to the Gaussian prior in MAP estimation.
•	A small constant (1e-15) is added to logarithms to avoid numerical issues with (\log(0)).
•	The synthetic dataset is linearly separable for simplicity, but the model can handle non-separable data.
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">division</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">minimize</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;ex2data1.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;exam1&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;exam2&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s1">&#39;y&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>exam1</th>
      <th>exam2</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>34.623660</td>
      <td>78.024693</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>30.286711</td>
      <td>43.894998</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>35.847409</td>
      <td>72.902198</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>60.182599</td>
      <td>86.308552</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>79.032736</td>
      <td>75.344376</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">][</span><span class="s1">&#39;exam1&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">][</span><span class="s1">&#39;exam2&#39;</span><span class="p">],</span>
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Not admitted&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;yellow&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][</span><span class="s1">&#39;exam1&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][</span><span class="s1">&#39;exam2&#39;</span><span class="p">],</span>
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Admitted&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Exam 1 score&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Exam 2 score&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Scores indicating admission&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/29e32365982629420a23f15b32ac1ad2bd0b2f9a87bfa0df9863af20296e0660.png" src="_images/29e32365982629420a23f15b32ac1ad2bd0b2f9a87bfa0df9863af20296e0660.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LogisticRegression</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iterations</span> <span class="o">=</span> <span class="n">n_iterations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_reg</span> <span class="o">=</span> <span class="n">lambda_reg</span>  <span class="c1"># Regularization strength</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="c1"># Binary cross-entropy loss + L2 regularization</span>
        <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">cross_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">+</span> <span class="mf">1e-15</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred</span> <span class="o">+</span> <span class="mf">1e-15</span><span class="p">))</span>
        <span class="n">l2_penalty</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambda_reg</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cross_entropy</span> <span class="o">+</span> <span class="n">l2_penalty</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># Initialize weights and bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Gradient descent</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iterations</span><span class="p">):</span>
            <span class="c1"># Forward pass</span>
            <span class="n">linear_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">linear_model</span><span class="p">)</span>

            <span class="c1"># Compute loss</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1"># Compute gradients</span>
            <span class="n">dw</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambda_reg</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
            <span class="n">db</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>

            <span class="c1"># Update parameters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dw</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">db</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">linear_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">linear_model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># Example usage</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Generate synthetic data</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="c1"># Train model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Predict</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Plot loss curve</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Loss Curve&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.9900
</pre></div>
</div>
<img alt="_images/984ea76f8811baf5cc9ad6503278a364cfbea2858b7c4570ff1dea707881c839.png" src="_images/984ea76f8811baf5cc9ad6503278a364cfbea2858b7c4570ff1dea707881c839.png" />
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="softmax-regression-explained">
<h2>Softmax Regression Explained<a class="headerlink" href="#softmax-regression-explained" title="Link to this heading">#</a></h2>
<p>Softmax regression (also known as multinomial logistic regression) is a <strong>multi-class classification algorithm</strong> that generalizes logistic regression to handle more than two classes. It predicts the probability that an input belongs to each of <span class="math notranslate nohighlight">\(K\)</span> possible classes, making it suitable for problems like classifying images into multiple categories (e.g., digits 0–9 in MNIST).</p>
<section id="id7">
<h3>Key Components<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Score Computation</strong>: Computes a score for each class based on a linear combination of input features and class-specific parameters.</p></li>
<li><p><strong>Softmax Function</strong>: Converts scores into probabilities that sum to 1, representing the likelihood of each class.</p></li>
<li><p><strong>Optimization</strong>: Trains the model by maximizing the likelihood of the observed data using a loss function (negative log-likelihood).</p></li>
</ol>
<p>Despite the name “regression,” softmax regression is used for <strong>classification</strong>, as it outputs class probabilities.</p>
</section>
</section>
<hr class="docutils" />
<section id="id8">
<h2>1. Model Formulation<a class="headerlink" href="#id8" title="Link to this heading">#</a></h2>
<p>Softmax regression maps an input <span class="math notranslate nohighlight">\(\vec{x} \in \mathbb{R}^d\)</span> to a probability distribution over <span class="math notranslate nohighlight">\(K\)</span> classes, producing a vector of probabilities:</p>
<div class="math notranslate nohighlight">
\[f_\theta(\vec{x}) \in [0, 1]^K, \quad \sum_{k=1}^K f_\theta(\vec{x})_k = 1\]</div>
<section id="parameters">
<h3>Parameters<a class="headerlink" href="#parameters" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Parameters</strong>: <span class="math notranslate nohighlight">\(\theta = (\theta_1, \theta_2, \ldots, \theta_K)\)</span>, where each <span class="math notranslate nohighlight">\(\theta_k \in \mathbb{R}^d\)</span> is the parameter vector for class <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p><strong>Parameter Space</strong>: <span class="math notranslate nohighlight">\(\Theta = \mathbb{R}^{K \times d}\)</span>, a matrix where each row <span class="math notranslate nohighlight">\(\theta_k^\top\)</span> corresponds to a class.</p></li>
<li><p><strong>Input</strong>: <span class="math notranslate nohighlight">\(\vec{x} \in \mathbb{R}^d\)</span>, the feature vector (often includes a bias term by appending a 1).</p></li>
</ul>
</section>
<section id="step-1-score-computation">
<h3>Step 1: Score Computation<a class="headerlink" href="#step-1-score-computation" title="Link to this heading">#</a></h3>
<p>For each class <span class="math notranslate nohighlight">\(k = 1, \ldots, K\)</span>, compute a score (or logit):</p>
<div class="math notranslate nohighlight">
\[z_k = \theta_k^\top \vec{x} = \theta_{k1} x_1 + \theta_{k2} x_2 + \cdots + \theta_{kd} x_d\]</div>
<p>The vector of scores is:</p>
<div class="math notranslate nohighlight">
\[\vec{z} = (z_1, z_2, \ldots, z_K), \quad z_k = \theta_k^\top \vec{x}\]</div>
</section>
<section id="step-2-softmax-function">
<h3>Step 2: Softmax Function<a class="headerlink" href="#step-2-softmax-function" title="Link to this heading">#</a></h3>
<p>The scores are converted to probabilities using the <strong>softmax function</strong>, which ensures the outputs are positive and sum to 1:</p>
<div class="math notranslate nohighlight">
\[\vec{\sigma}(\vec{z})_k = \frac{e(z_k)}{\sum_{l=1}^K e(z_l)}\]</div>
<p>For a given input <span class="math notranslate nohighlight">\(\vec{x}\)</span>, the probability of class <span class="math notranslate nohighlight">\(k\)</span> is:</p>
<div class="math notranslate nohighlight">
\[P(y = k | \vec{x}, \theta) = f_\theta(\vec{x})_k = \frac{e(\theta_k^\top \vec{x})}{\sum_{l=1}^K e(\theta_l^\top \vec{x})}\]</div>
</section>
<section id="id9">
<h3>Intuition<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Exponentiation</strong>: <span class="math notranslate nohighlight">\(e(z_k)\)</span> ensures all scores are positive, emphasizing larger scores.</p></li>
<li><p><strong>Normalization</strong>: Dividing by the sum <span class="math notranslate nohighlight">\(\sum_{l=1}^K e(z_l)\)</span> makes the probabilities sum to 1.</p></li>
<li><p><strong>Competition</strong>: Classes with higher scores <span class="math notranslate nohighlight">\(z_k\)</span> get higher probabilities, but the softmax ensures a smooth distribution across all classes.</p></li>
</ul>
</section>
<section id="identifiability-note">
<h3>Identifiability Note<a class="headerlink" href="#identifiability-note" title="Link to this heading">#</a></h3>
<p>The model is <strong>over-parametrized</strong>. Adding a constant vector <span class="math notranslate nohighlight">\(\vec{c}\)</span> to all <span class="math notranslate nohighlight">\(\theta_k\)</span> (i.e., <span class="math notranslate nohighlight">\(\theta_k \to \theta_k + \vec{c}\)</span>) doesn’t change the probabilities because:</p>
<div class="math notranslate nohighlight">
\[e((\theta_k + \vec{c})^\top \vec{x}) = e(\theta_k^\top \vec{x}) \cdot e(\vec{c}^\top \vec{x})\]</div>
<p>The <span class="math notranslate nohighlight">\(e(\vec{c}^\top \vec{x})\)</span> terms cancel out in the softmax denominator. To address this, we can fix one parameter vector (e.g., <span class="math notranslate nohighlight">\(\theta_1 = \vec{0}\)</span>) without loss of generality, reducing the parameter space.</p>
</section>
</section>
<hr class="docutils" />
<section id="deeper-explanation-of-the-softmax-function">
<h2>Deeper Explanation of the Softmax Function<a class="headerlink" href="#deeper-explanation-of-the-softmax-function" title="Link to this heading">#</a></h2>
<section id="what-is-the-softmax-function">
<h3>1. <strong>What is the Softmax Function?</strong><a class="headerlink" href="#what-is-the-softmax-function" title="Link to this heading">#</a></h3>
<p>The softmax function is a mathematical operation used to transform a vector of real-valued scores (often called <strong>logits</strong>) into a probability distribution. It is widely used in multi-class classification problems, where the goal is to assign an input to one of <span class="math notranslate nohighlight">\(K\)</span> possible classes.</p>
<p>Given a vector of scores <span class="math notranslate nohighlight">\(\vec{z} = [z_1, z_2, \dots, z_K] \in \mathbb{R}^K\)</span>, the softmax function computes probabilities for each class <span class="math notranslate nohighlight">\(k\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[ \vec{\sigma}(\vec{z})_k = \frac{e(z_k)}{\sum_{l=1}^K e(z_l)} \]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(e(z_k)\)</span>: The exponential function ensures that the output for each class is positive, as <span class="math notranslate nohighlight">\(e(z_k) &gt; 0\)</span> for any real <span class="math notranslate nohighlight">\(z_k\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{l=1}^K e(z_l)\)</span>: The denominator normalizes the outputs by summing the exponentials of all scores, ensuring that the resulting probabilities sum to 1:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \sum_{k=1}^K \vec{\sigma}(\vec{z})_k = 1 \]</div>
<p>The output <span class="math notranslate nohighlight">\(\vec{\sigma}(\vec{z})_k\)</span> represents the probability of class <span class="math notranslate nohighlight">\(k\)</span>, and the vector <span class="math notranslate nohighlight">\(\vec{\sigma}(\vec{z}) = [\vec{\sigma}(\vec{z})_1, \vec{\sigma}(\vec{z})_2, \dots, \vec{\sigma}(\vec{z})_K]\)</span> is a valid probability distribution.</p>
</section>
<section id="why-use-the-softmax-function">
<h3>2. <strong>Why Use the Softmax Function?</strong><a class="headerlink" href="#why-use-the-softmax-function" title="Link to this heading">#</a></h3>
<p>The softmax function is ideal for multi-class classification because:</p>
<ul class="simple">
<li><p><strong>Positivity</strong>: The exponential ensures all outputs are positive, which is necessary for probabilities.</p></li>
<li><p><strong>Normalization</strong>: The denominator ensures the outputs sum to 1, making them interpretable as probabilities.</p></li>
<li><p><strong>Amplifies Differences</strong>: The exponential function exaggerates differences between scores. A larger <span class="math notranslate nohighlight">\(z_k\)</span> leads to a much larger <span class="math notranslate nohighlight">\(e(z_k)\)</span>, assigning higher probability to the corresponding class.</p></li>
<li><p><strong>Differentiability</strong>: The softmax function is smooth and differentiable, which is crucial for optimization in machine learning (e.g., gradient-based methods like gradient descent).</p></li>
</ul>
</section>
<section id="softmax-in-multi-class-classification">
<h3>3. <strong>Softmax in Multi-Class Classification</strong><a class="headerlink" href="#softmax-in-multi-class-classification" title="Link to this heading">#</a></h3>
<p>In a multi-class classification model (e.g., logistic regression for multiple classes or a neural network), the model computes a score for each class based on the input <span class="math notranslate nohighlight">\(\vec{x}\)</span> and model parameters <span class="math notranslate nohighlight">\(\theta\)</span>. For class <span class="math notranslate nohighlight">\(k\)</span>, the score (or logit) is often computed as a linear combination:</p>
<div class="math notranslate nohighlight">
\[ z_k = \theta_k^\top \vec{x} \]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\vec{x} \in \mathbb{R}^d\)</span> is the input feature vector.</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_k \in \mathbb{R}^d\)</span> is the parameter vector (e.g., weights) for class <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta = [\theta_1, \theta_2, \dots, \theta_K]\)</span> is the collection of parameters for all <span class="math notranslate nohighlight">\(K\)</span> classes.</p></li>
</ul>
<p>The scores <span class="math notranslate nohighlight">\(\vec{z} = [z_1, z_2, \dots, z_K]\)</span> are then passed through the softmax function to obtain the probability of each class:</p>
<div class="math notranslate nohighlight">
\[ P(y = k | \vec{x}, \theta) = f_\theta(\vec{x})_k = \frac{e(\theta_k^\top \vec{x})}{\sum_{l=1}^K e(\theta_l^\top \vec{x})} \]</div>
<p>This probability represents the model’s confidence that the input <span class="math notranslate nohighlight">\(\vec{x}\)</span> belongs to class <span class="math notranslate nohighlight">\(k\)</span>. The predicted class is typically the one with the highest probability:</p>
<div class="math notranslate nohighlight">
\[ \hat{y} = \arg\max_k P(y = k | \vec{x}, \theta) \]</div>
</section>
<section id="properties-of-softmax">
<h3>4. <strong>Properties of Softmax</strong><a class="headerlink" href="#properties-of-softmax" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Invariance to Constant Shifts</strong>: Adding a constant <span class="math notranslate nohighlight">\(c\)</span> to all scores <span class="math notranslate nohighlight">\(z_k\)</span> doesn’t change the output probabilities, because:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \frac{e(z_k + c)}{\sum_{l=1}^K e(z_l + c)} = \frac{e(z_k) \cdot e(c)}{\sum_{l=1}^K e(z_l) \cdot e(c)} = \frac{e(z_k)}{\sum_{l=1}^K e(z_l)} \]</div>
<p>This property is useful numerically, as subtracting the maximum score (i.e., <span class="math notranslate nohighlight">\(z_{\text{max}}\)</span>) from all scores can prevent overflow in computations:</p>
<div class="math notranslate nohighlight">
\[ \vec{\sigma}(\vec{z})_k = \frac{e(z_k - z_{\text{max}})}{\sum_{l=1}^K e(z_l - z_{\text{max}})} \]</div>
<ul class="simple">
<li><p><strong>Sensitivity to Score Differences</strong>: The exponential amplifies differences between scores. If <span class="math notranslate nohighlight">\(z_k\)</span> is much larger than the others, <span class="math notranslate nohighlight">\(\vec{\sigma}(\vec{z})_k\)</span> will be close to 1, and the others will be close to 0.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="small-example-applying-the-softmax-function">
<h2>Small Example: Applying the Softmax Function<a class="headerlink" href="#small-example-applying-the-softmax-function" title="Link to this heading">#</a></h2>
<p>Let’s walk through a concrete example to illustrate how the softmax function works in a multi-class classification setting.</p>
<section id="id10">
<h3>Problem Setup<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p>Suppose we have a classification problem with <span class="math notranslate nohighlight">\(K = 3\)</span> classes (e.g., classifying an image as a cat, dog, or bird). For a given input <span class="math notranslate nohighlight">\(\vec{x}\)</span>, a model computes scores for each class based on parameters <span class="math notranslate nohighlight">\(\theta\)</span>. Let’s assume:</p>
<ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\(\vec{x} = [1, 2]\)</span> (e.g., features extracted from an image).</p></li>
<li><p>Parameters for each class:</p>
<ul>
<li><p>Class 1 (cat): <span class="math notranslate nohighlight">\(\theta_1 = [0.5, 1.0]\)</span></p></li>
<li><p>Class 2 (dog): <span class="math notranslate nohighlight">\(\theta_2 = [1.0, 0.5]\)</span></p></li>
<li><p>Class 3 (bird): <span class="math notranslate nohighlight">\(\theta_3 = [-0.5, 0.5]\)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="step-1-compute-scores-logits">
<h3>Step 1: Compute Scores (Logits)<a class="headerlink" href="#step-1-compute-scores-logits" title="Link to this heading">#</a></h3>
<p>Calculate the score for each class using <span class="math notranslate nohighlight">\(z_k = \theta_k^\top \vec{x}\)</span>:</p>
<ul class="simple">
<li><p>Class 1: <span class="math notranslate nohighlight">\(z_1 = \theta_1^\top \vec{x} = 0.5 \cdot 1 + 1.0 \cdot 2 = 0.5 + 2.0 = 2.5\)</span></p></li>
<li><p>Class 2: <span class="math notranslate nohighlight">\(z_2 = \theta_2^\top \vec{x} = 1.0 \cdot 1 + 0.5 \cdot 2 = 1.0 + 1.0 = 2.0\)</span></p></li>
<li><p>Class 3: <span class="math notranslate nohighlight">\(z_3 = \theta_3^\top \vec{x} = -0.5 \cdot 1 + 0.5 \cdot 2 = -0.5 + 1.0 = 0.5\)</span></p></li>
</ul>
<p>So, the score vector is:</p>
<div class="math notranslate nohighlight">
\[ \vec{z} = [2.5, 2.0, 0.5] \]</div>
</section>
<section id="step-2-apply-the-softmax-function">
<h3>Step 2: Apply the Softmax Function<a class="headerlink" href="#step-2-apply-the-softmax-function" title="Link to this heading">#</a></h3>
<p>Compute the softmax probabilities:</p>
<div class="math notranslate nohighlight">
\[ \vec{\sigma}(\vec{z})_k = \frac{e(z_k)}{\sum_{l=1}^3 e(z_l)} \]</div>
<p>First, calculate the exponentials:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(e(z_1) = e(2.5) \approx 12.182\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(e(z_2) = e(2.0) \approx 7.389\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(e(z_3) = e(0.5) \approx 1.649\)</span></p></li>
</ul>
<p>Sum the exponentials:</p>
<div class="math notranslate nohighlight">
\[ \sum_{l=1}^3 e(z_l) \approx 12.182 + 7.389 + 1.649 = 21.220 \]</div>
<p>Now compute the probabilities:</p>
<ul class="simple">
<li><p>Class 1: <span class="math notranslate nohighlight">\(\vec{\sigma}(\vec{z})_1 = \frac{e(2.5)}{21.220} \approx \frac{12.182}{21.220} \approx 0.574\)</span></p></li>
<li><p>Class 2: <span class="math notranslate nohighlight">\(\vec{\sigma}(\vec{z})_2 = \frac{e(2.0)}{21.220} \approx \frac{7.389}{21.220} \approx 0.348\)</span></p></li>
<li><p>Class 3: <span class="math notranslate nohighlight">\(\vec{\sigma}(\vec{z})_3 = \frac{e(0.5)}{21.220} \approx \frac{1.649}{21.220} \approx 0.078\)</span></p></li>
</ul>
<p>The probability distribution is:</p>
<div class="math notranslate nohighlight">
\[ P(y = \text{cat} | \vec{x}, \theta) \approx 0.574, \quad P(y = \text{dog} | \vec{x}, \theta) \approx 0.348, \quad P(y = \text{bird} | \vec{x}, \theta) \approx 0.078 \]</div>
</section>
<section id="step-3-interpretation">
<h3>Step 3: Interpretation<a class="headerlink" href="#step-3-interpretation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The model assigns the highest probability (57.4%) to the “cat” class, so the predicted class is “cat”:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \hat{y} = \arg\max_k P(y = k | \vec{x}, \theta) = \text{cat} \]</div>
<ul class="simple">
<li><p>The probabilities sum to 1: <span class="math notranslate nohighlight">\(0.574 + 0.348 + 0.078 \approx 1.000\)</span>.</p></li>
<li><p>The exponential in the softmax amplifies the effect of the highest score (<span class="math notranslate nohighlight">\(z_1 = 2.5\)</span>), giving the “cat” class a significantly higher probability than the others.</p></li>
</ul>
</section>
<section id="step-4-numerical-stability-optional">
<h3>Step 4: Numerical Stability (Optional)<a class="headerlink" href="#step-4-numerical-stability-optional" title="Link to this heading">#</a></h3>
<p>To avoid numerical overflow (e.g., if scores are very large), we can subtract the maximum score (<span class="math notranslate nohighlight">\(z_{\text{max}} = 2.5\)</span>) from all scores:</p>
<ul class="simple">
<li><p>Adjusted scores: <span class="math notranslate nohighlight">\([2.5 - 2.5, 2.0 - 2.5, 0.5 - 2.5] = [0, -0.5, -2.0]\)</span></p></li>
<li><p>Exponentials: <span class="math notranslate nohighlight">\(e(0) = 1\)</span>, <span class="math notranslate nohighlight">\(e(-0.5) \approx 0.607\)</span>, <span class="math notranslate nohighlight">\(e(-2.0) \approx 0.135\)</span></p></li>
<li><p>Sum: <span class="math notranslate nohighlight">\(1 + 0.607 + 0.135 \approx 1.742\)</span></p></li>
<li><p>Probabilities:</p>
<ul>
<li><p>Class 1: <span class="math notranslate nohighlight">\(\frac{1}{1.742} \approx 0.574\)</span></p></li>
<li><p>Class 2: <span class="math notranslate nohighlight">\(\frac{0.607}{1.742} \approx 0.348\)</span></p></li>
<li><p>Class 3: <span class="math notranslate nohighlight">\(\frac{0.135}{1.742} \approx 0.078\)</span></p></li>
</ul>
</li>
</ul>
<p>The results are identical, but the computation is more stable.</p>
</section>
</section>
<section id="probabilistic-interpretation">
<h2>2. Probabilistic Interpretation<a class="headerlink" href="#probabilistic-interpretation" title="Link to this heading">#</a></h2>
<p>Softmax regression models the conditional probability of the class label <span class="math notranslate nohighlight">\(y \in \{1, 2, \ldots, K\}\)</span> given the input <span class="math notranslate nohighlight">\(\vec{x}\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(y = k | \vec{x}, \theta) = \frac{e(\theta_k^\top \vec{x})}{\sum_{l=1}^K e(\theta_l^\top \vec{x})}\]</div>
<p>This defines a <strong>categorical distribution</strong> over the <span class="math notranslate nohighlight">\(K\)</span> classes. The model assigns higher probabilities to classes whose parameter vectors <span class="math notranslate nohighlight">\(\theta_k\)</span> are more aligned with the input <span class="math notranslate nohighlight">\(\vec{x}\)</span> (i.e., larger <span class="math notranslate nohighlight">\(\theta_k^\top \vec{x}\)</span>).</p>
<section id="decision-rule">
<h3>Decision Rule<a class="headerlink" href="#decision-rule" title="Link to this heading">#</a></h3>
<p>To classify an input, choose the class with the highest probability:</p>
<div class="math notranslate nohighlight">
\[\hat{y} = \arg\max_{k} P(y = k | \vec{x}, \theta) = \arg\max_{k} \theta_k^\top \vec{x}\]</div>
<p>Since the softmax is monotonic, this is equivalent to picking the class with the highest score <span class="math notranslate nohighlight">\(z_k\)</span>.</p>
</section>
</section>
<hr class="docutils" />
<section id="learning-objective-maximum-likelihood-estimation-mle">
<h2>3. Learning Objective: Maximum Likelihood Estimation (MLE)<a class="headerlink" href="#learning-objective-maximum-likelihood-estimation-mle" title="Link to this heading">#</a></h2>
<p>The goal is to find the parameters <span class="math notranslate nohighlight">\(\theta\)</span> that maximize the likelihood of the observed data.</p>
<section id="dataset">
<h3>Dataset<a class="headerlink" href="#dataset" title="Link to this heading">#</a></h3>
<p>Consider a dataset:</p>
<div class="math notranslate nohighlight">
\[\mathcal{D} = \{(\vec{x}^{(i)}, y^{(i)})\}_{i=1}^n\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\vec{x}^{(i)} \in \mathbb{R}^d\)</span>: Feature vector for the <span class="math notranslate nohighlight">\(i\)</span>-th instance.</p></li>
<li><p><span class="math notranslate nohighlight">\(y^{(i)} \in \{1, 2, \ldots, K\}\)</span>: True class label.</p></li>
</ul>
</section>
<section id="id11">
<h3>Likelihood Function<a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<p>The probability of observing label <span class="math notranslate nohighlight">\(y^{(i)}\)</span> given <span class="math notranslate nohighlight">\(\vec{x}^{(i)}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[P(y^{(i)} | \vec{x}^{(i)}, \theta) = \frac{e(\theta_{y^{(i)}}^\top \vec{x}^{(i)})}{\sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)})}\]</div>
<p>Assuming the instances are independent, the likelihood is:</p>
<div class="math notranslate nohighlight">
\[L(\theta) = \prod_{i=1}^n P(y^{(i)} | \vec{x}^{(i)}, \theta) = \prod_{i=1}^n \frac{e(\theta_{y^{(i)}}^\top \vec{x}^{(i)})}{\sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)})}\]</div>
</section>
<section id="id12">
<h3>Log-Likelihood<a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
<p>To simplify optimization, take the logarithm:</p>
<div class="math notranslate nohighlight">
\[\ell(\theta) = \log L(\theta) = \sum_{i=1}^n \log P(y^{(i)} | \vec{x}^{(i)}, \theta)\]</div>
<p>Substitute the probability:</p>
<div class="math notranslate nohighlight">
\[\ell(\theta) = \sum_{i=1}^n \left[ \log \left( e(\theta_{y^{(i)}}^\top \vec{x}^{(i)}) \right) - \log \left( \sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)}) \right) \right]\]</div>
<div class="math notranslate nohighlight">
\[= \sum_{i=1}^n \left[ \theta_{y^{(i)}}^\top \vec{x}^{(i)} - \log \sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)}) \right]\]</div>
</section>
<section id="id13">
<h3>Cost Function<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>In practice, we minimize the <strong>negative log-likelihood</strong> (cross-entropy loss), averaged over the dataset:</p>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \ell(\theta) = -\frac{1}{n} \sum_{i=1}^n \left[ \theta_{y^{(i)}}^\top \vec{x}^{(i)} - \log \sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)}) \right]\]</div>
<p>This is the <strong>multinomial cross-entropy loss</strong>. Minimizing <span class="math notranslate nohighlight">\(J(\theta)\)</span> is equivalent to maximizing the likelihood.</p>
</section>
</section>
<hr class="docutils" />
<section id="id14">
<h2>4. Gradient of the Cost Function<a class="headerlink" href="#id14" title="Link to this heading">#</a></h2>
<p>To optimize <span class="math notranslate nohighlight">\(J(\theta)\)</span> using gradient descent, we need the gradient with respect to <span class="math notranslate nohighlight">\(\theta_k\)</span> for each class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<section id="gradient-derivation">
<h3>Gradient Derivation<a class="headerlink" href="#gradient-derivation" title="Link to this heading">#</a></h3>
<p>The cost function is:</p>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum_{i=1}^n \left[ \theta_{y^{(i)}}^\top \vec{x}^{(i)} - \log \sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)}) \right]\]</div>
<p>Define the loss for a single instance <span class="math notranslate nohighlight">\(i\)</span>:</p>
<div class="math notranslate nohighlight">
\[L^{(i)} = \theta_{y^{(i)}}^\top \vec{x}^{(i)} - \log \sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)})\]</div>
<p>We compute the partial derivative with respect to <span class="math notranslate nohighlight">\(\theta_m\)</span> (parameters for class <span class="math notranslate nohighlight">\(m\)</span>):</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\theta)}{\partial \theta_m} = -\frac{1}{n} \sum_{i=1}^n \frac{\partial L^{(i)}}{\partial \theta_m}\]</div>
<p>For a single instance, differentiate:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L^{(i)}}{\partial \theta_m} = \frac{\partial}{\partial \theta_m} \left[ \theta_{y^{(i)}}^\top \vec{x}^{(i)} - \log \sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)}) \right]\]</div>
<ul class="simple">
<li><p><strong>First term</strong>: <span class="math notranslate nohighlight">\(\theta_{y^{(i)}}^\top \vec{x}^{(i)}\)</span></p></li>
</ul>
<p>This is non-zero only if <span class="math notranslate nohighlight">\(m = y^{(i)}\)</span> (since only <span class="math notranslate nohighlight">\(\theta_{y^{(i)}}\)</span> involves class <span class="math notranslate nohighlight">\(y^{(i)}\)</span>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial}{\partial \theta_m} (\theta_{y^{(i)}}^\top \vec{x}^{(i)}) = \begin{cases} 
\vec{x}^{(i)}, &amp; \text{if } m = y^{(i)} \\
0, &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
<p>Using an indicator function, this is:</p>
<div class="math notranslate nohighlight">
\[\mathbb{1}\{m = y^{(i)}\} \cdot \vec{x}^{(i)}\]</div>
<ul class="simple">
<li><p><strong>Second term</strong>: <span class="math notranslate nohighlight">\(-\log \sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)})\)</span></p></li>
</ul>
<p>Let <span class="math notranslate nohighlight">\(Z = \sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)})\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_m} \log Z = \frac{1}{Z} \cdot \frac{\partial Z}{\partial \theta_m}\]</div>
<p>Compute:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial Z}{\partial \theta_m} = \frac{\partial}{\partial \theta_m} \sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)}) = e(\theta_m^\top \vec{x}^{(i)}) \cdot \vec{x}^{(i)}\]</div>
<p>So:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_m} \log Z = \frac{e(\theta_m^\top \vec{x}^{(i)}) \cdot \vec{x}^{(i)}}{Z} = P(y = m | \vec{x}^{(i)}, \theta) \cdot \vec{x}^{(i)}\]</div>
<p>Since <span class="math notranslate nohighlight">\(P(y = m | \vec{x}^{(i)}, \theta) = \frac{e(\theta_m^\top \vec{x}^{(i)})}{Z}\)</span>, the derivative of the second term is:</p>
<div class="math notranslate nohighlight">
\[-\frac{\partial}{\partial \theta_m} \log Z = -P(y = m | \vec{x}^{(i)}, \theta) \cdot \vec{x}^{(i)}\]</div>
<p>Combine:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L^{(i)}}{\partial \theta_m} = \mathbb{1}\{m = y^{(i)}\} \cdot \vec{x}^{(i)} - P(y = m | \vec{x}^{(i)}, \theta) \cdot \vec{x}^{(i)}\]</div>
<p>The full gradient is:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\theta)}{\partial \theta_m} = -\frac{1}{n} \sum_{i=1}^n \left[ \mathbb{1}\{m = y^{(i)}\} - P(y = m | \vec{x}^{(i)}, \theta) \right] \vec{x}^{(i)}\]</div>
</section>
<section id="id15">
<h3>Intuition<a class="headerlink" href="#id15" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(m = y^{(i)}\)</span>, the term <span class="math notranslate nohighlight">\(\mathbb{1}\{m = y^{(i)}\} = 1\)</span>, and the gradient pushes <span class="math notranslate nohighlight">\(P(y = m | \vec{x}^{(i)}, \theta)\)</span> toward 1.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(m \neq y^{(i)}\)</span>, the gradient pushes <span class="math notranslate nohighlight">\(P(y = m | \vec{x}^{(i)}, \theta)\)</span> toward 0.</p></li>
<li><p>The difference <span class="math notranslate nohighlight">\(\mathbb{1}\{m = y^{(i)}\} - P(y = m | \vec{x}^{(i)}, \theta)\)</span> measures the error in the predicted probability.</p></li>
</ul>
</section>
<section id="gradient-descent-update">
<h3>Gradient Descent Update<a class="headerlink" href="#gradient-descent-update" title="Link to this heading">#</a></h3>
<p>Update each <span class="math notranslate nohighlight">\(\theta_m\)</span> using:</p>
<div class="math notranslate nohighlight">
\[\theta_m := \theta_m - \alpha \cdot \frac{\partial J(\theta)}{\partial \theta_m} = \theta_m + \alpha \cdot \frac{1}{n} \sum_{i=1}^n \left[ \mathbb{1}\{m = y^{(i)}\} - P(y = m | \vec{x}^{(i)}, \theta) \right] \vec{x}^{(i)}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate.</p>
</section>
</section>
<hr class="docutils" />
<section id="connection-to-logistic-regression">
<h2>5. Connection to Logistic Regression<a class="headerlink" href="#connection-to-logistic-regression" title="Link to this heading">#</a></h2>
<p>Softmax regression generalizes <strong>logistic regression</strong>:</p>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(K = 2\)</span>, softmax regression reduces to logistic regression.</p></li>
<li><p>Let class 1 have probability <span class="math notranslate nohighlight">\(p = \frac{\exp(\theta_1^\top \vec{x})}{\exp(\theta_1^\top \vec{x}) + \exp(\theta_2^\top \vec{x})}\)</span>. Set <span class="math notranslate nohighlight">\(\theta = \theta_1 - \theta_2\)</span>, then:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[p = \frac{\exp(\theta_1^\top \vec{x})}{\exp(\theta_1^\top \vec{x}) + \exp(\theta_2^\top \vec{x})} = \frac{1}{1 + \exp(-(\theta_1^\top \vec{x} - \theta_2^\top \vec{x}))} = \sigma(\theta^\top \vec{x})\]</div>
<p>This matches the logistic regression sigmoid function.</p>
<hr class="docutils" />
<p><span class="math notranslate nohighlight">\(e^\theta\)</span> is same as <span class="math notranslate nohighlight">\(exp(\theta)\)</span></p>
<p>starting with:
$<span class="math notranslate nohighlight">\(p = \frac{e(\theta_1^\top \vec{x})}{e(\theta_1^\top \vec{x}) + e(\theta_2^\top \vec{x})}\)</span>$</p>
<p>Then you performed the following algebraic manipulations:</p>
<ol class="arabic simple">
<li><p><strong>Divide numerator and denominator by <span class="math notranslate nohighlight">\(e(\theta_1^\top \vec{x})\)</span>:</strong>
$<span class="math notranslate nohighlight">\(p = \frac{\frac{e(\theta_1^\top \vec{x})}{e(\theta_1^\top \vec{x})}}{\frac{e(\theta_1^\top \vec{x})}{e(\theta_1^\top \vec{x})} + \frac{e(\theta_2^\top \vec{x})}{e(\theta_1^\top \vec{x})}} = \frac{1}{1 + e(\theta_2^\top \vec{x} - \theta_1^\top \vec{x})}\)</span>$</p></li>
<li><p><strong>Introduce <span class="math notranslate nohighlight">\(\theta = \theta_1 - \theta_2\)</span>:</strong>
Notice that <span class="math notranslate nohighlight">\(\theta_2^\top \vec{x} - \theta_1^\top \vec{x} = -(\theta_1^\top \vec{x} - \theta_2^\top \vec{x}) = -(\theta^\top \vec{x})\)</span>. Substituting this, you get:
$<span class="math notranslate nohighlight">\(p = \frac{1}{1 + e(-(\theta^\top \vec{x}))}\)</span>$</p></li>
<li><p><strong>Recognize the sigmoid function:</strong>
You correctly identified this as the sigmoid function <span class="math notranslate nohighlight">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span>, where in this case, <span class="math notranslate nohighlight">\(z = \theta^\top \vec{x}\)</span>.</p></li>
</ol>
<p><strong>So, the <span class="math notranslate nohighlight">\(e^z\)</span> format in this context is:</strong></p>
<p>With <span class="math notranslate nohighlight">\(z = \theta^\top \vec{x}\)</span>, the probability <span class="math notranslate nohighlight">\(p\)</span> can be expressed as:</p>
<div class="math notranslate nohighlight">
\[p = \frac{1}{1 + e^{-z}}\]</div>
<p>Alternatively, if you wanted the numerator to explicitly have <span class="math notranslate nohighlight">\(e^z\)</span>, you could multiply the numerator and denominator of the original expression by <span class="math notranslate nohighlight">\(e(-\theta_2^\top \vec{x})\)</span>:</p>
<div class="math notranslate nohighlight">
\[p = \frac{e(\theta_1^\top \vec{x}) \cdot e(-\theta_2^\top \vec{x})}{(e(\theta_1^\top \vec{x}) + e(\theta_2^\top \vec{x})) \cdot e(-\theta_2^\top \vec{x})}$$$$p = \frac{e(\theta_1^\top \vec{x} - \theta_2^\top \vec{x})}{e(\theta_1^\top \vec{x} - \theta_2^\top \vec{x}) + e(\theta_2^\top \vec{x} - \theta_2^\top \vec{x})}$$$$p = \frac{e((\theta_1 - \theta_2)^\top \vec{x})}{e((\theta_1 - \theta_2)^\top \vec{x}) + e(0)}\]</div>
<div class="math notranslate nohighlight">
\[p = \frac{e(\theta^\top \vec{x})}{e(\theta^\top \vec{x}) + 1}\]</div>
<p>If we still define <span class="math notranslate nohighlight">\(z = \theta^\top \vec{x}\)</span>, then this form is:</p>
<div class="math notranslate nohighlight">
\[p = \frac{e^z}{e^z + 1}\]</div>
<p>Both <span class="math notranslate nohighlight">\(\frac{1}{1 + e^{-z}}\)</span> and <span class="math notranslate nohighlight">\(\frac{e^z}{e^z + 1}\)</span> are equivalent forms of the sigmoid function and represent the probability <span class="math notranslate nohighlight">\(p\)</span> in terms of <span class="math notranslate nohighlight">\(e^z\)</span>. Your derivation correctly arrived at the first form. The second form is just another way to express the same relationship.</p>
</section>
<hr class="docutils" />
<section id="implementation">
<h2>6. Implementation<a class="headerlink" href="#implementation" title="Link to this heading">#</a></h2>
<p>Softmax regression can be implemented using scikit-learn’s <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> with the <code class="docutils literal notranslate"><span class="pre">multi_class='multinomial'</span></code> option:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>solver=’lbfgs’</strong>: Uses a second-order optimization method for efficiency.</p></li>
<li><p>The model automatically handles the softmax function and optimizes the cross-entropy loss.</p></li>
</ul>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># 1. Generate synthetic business-style 3-class data (e.g., low/medium/high value customers)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">class_sep</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># 2. Scale features (for stability and visualization)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># 3. Train multinomial logistic regression (softmax)</span>
<span class="n">logreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1e5</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>
<span class="n">logreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="c1"># 4. Meshgrid for decision boundary plot</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">),</span>
    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">Z</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># 5. Plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">,</span> <span class="n">shading</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Customer Feature 1 (scaled)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Customer Feature 2 (scaled)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Multinomial Logistic Regression (3-Class Business Example)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/chandraveshchaudhari/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: &#39;multi_class&#39; was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use &#39;multinomial&#39;. Leave it to its default value to avoid this warning.
  warnings.warn(
</pre></div>
</div>
<img alt="_images/2b7bdecc1f9e0a47248accdaec6c442a5eab9d468c25108e70cf2fc10d093851.png" src="_images/2b7bdecc1f9e0a47248accdaec6c442a5eab9d468c25108e70cf2fc10d093851.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">softmax</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelBinarizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SoftmaxRegression</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Separate bias term</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="c1"># Initialize weights and bias separately</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">_one_hot_encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="c1"># Gradient of cross-entropy loss</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_true</span>
        <span class="n">grad_weights</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">error</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">grad_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">error</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_weights</span><span class="p">,</span> <span class="n">grad_bias</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># Convert labels to one-hot encoding</span>
        <span class="n">y_one_hot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_one_hot_encode</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer</span><span class="o">.</span><span class="n">classes_</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">n_classes</span> <span class="o">=</span> <span class="n">y_one_hot</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># Initialize weights and bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
        
        <span class="n">prev_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
        
        <span class="c1"># Gradient descent</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="c1"># Forward pass: compute predictions</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>  <span class="c1"># Add bias here</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># Compute loss (cross-entropy)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_one_hot</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">+</span> <span class="mf">1e-15</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
            
            <span class="c1"># Check for convergence</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">prev_loss</span> <span class="o">-</span> <span class="n">loss</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Converged at iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">break</span>
            <span class="n">prev_loss</span> <span class="o">=</span> <span class="n">loss</span>
            
            <span class="c1"># Backward pass: compute gradients and update parameters</span>
            <span class="n">grad_weights</span><span class="p">,</span> <span class="n">grad_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_one_hot</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_weights</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_bias</span>
            
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">proba</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>

<span class="c1"># Example usage</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Load and prepare data</span>
    <span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
    
    <span class="c1"># Split data</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="c1"># Standardize features</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_test</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
    
    <span class="c1"># Train model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SoftmaxRegression</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
    <span class="c1"># Evaluate</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Test Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Show predictions</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Sample predictions:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True:&quot;</span><span class="p">,</span> <span class="n">y_test</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pred:&quot;</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration 0, Loss: 1.1053
Iteration 100, Loss: 0.3389
Iteration 200, Loss: 0.2699
Iteration 300, Loss: 0.2291
Iteration 400, Loss: 0.2013
Iteration 500, Loss: 0.1810
Iteration 600, Loss: 0.1657
Iteration 700, Loss: 0.1536
Converged at iteration 737

Test Accuracy: 1.0000

Sample predictions:
True: [1 0 2 1 1]
Pred: [1 0 2 1 1]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.colors</span><span class="w"> </span><span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mf">0.02</span><span class="p">):</span>
    <span class="c1"># Setup marker generator and color map</span>
    <span class="n">markers</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">)</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;lightgreen&#39;</span><span class="p">,</span> <span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="s1">&#39;cyan&#39;</span><span class="p">)</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">colors</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))])</span>
    
    <span class="c1"># Plot the decision surface</span>
    <span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">x2_min</span><span class="p">,</span> <span class="n">x2_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span><span class="p">,</span> <span class="n">resolution</span><span class="p">),</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x2_min</span><span class="p">,</span> <span class="n">x2_max</span><span class="p">,</span> <span class="n">resolution</span><span class="p">))</span>
    
    <span class="c1"># Predict for each point in meshgrid</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xx1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">xx2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx1</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx1</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">xx2</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx2</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    
    <span class="c1"># Plot class samples</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">cl</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">cl</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
                    <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">cl</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> 
                    <span class="n">c</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idx</span><span class="p">),</span>
                    <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span>
                    <span class="n">marker</span><span class="o">=</span><span class="n">markers</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> 
                    <span class="n">label</span><span class="o">=</span><span class="n">cl</span><span class="p">)</span>

<span class="c1"># Select two features for visualization (sepal length and width)</span>
<span class="n">X_vis</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># Using first two features for visualization</span>

<span class="c1"># Retrain model on just these two features</span>
<span class="n">model_vis</span> <span class="o">=</span> <span class="n">SoftmaxRegression</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">model_vis</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_vis</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Create plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_vis</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">model_vis</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal length (standardized)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sepal width (standardized)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Softmax Regression Decision Boundaries on Iris Dataset&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Also plot the actual test results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">model_vis</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal length (standardized)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sepal width (standardized)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Test Set Classification Results&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration 0, Loss: 1.0934
Iteration 100, Loss: 0.8577
Iteration 200, Loss: 0.7380
Iteration 300, Loss: 0.6699
Iteration 400, Loss: 0.6268
Iteration 500, Loss: 0.5973
Iteration 600, Loss: 0.5759
Iteration 700, Loss: 0.5595
Iteration 800, Loss: 0.5466
Converged at iteration 876
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/93/7lt42x5j7m39kz7wxbcghvrm0000gn/T/ipykernel_9647/1777411610.py:27: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.
  plt.scatter(x=X[y == cl, 0],
/var/folders/93/7lt42x5j7m39kz7wxbcghvrm0000gn/T/ipykernel_9647/1777411610.py:27: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
  plt.scatter(x=X[y == cl, 0],
</pre></div>
</div>
<img alt="_images/046466b1c59cfa939fb4b920fa62c8087579732a41d39d93c272e994a78fadc3.png" src="_images/046466b1c59cfa939fb4b920fa62c8087579732a41d39d93c272e994a78fadc3.png" />
<img alt="_images/b9845c304e060ac174cc327ed994d9bd171fc1d155ce32b687b9ee012a8c4bff.png" src="_images/b9845c304e060ac174cc327ed994d9bd171fc1d155ce32b687b9ee012a8c4bff.png" />
</div>
</div>
<section id="information-theoretic-view-entropy-kl-divergence-in-classification">
<h3>Information-Theoretic View: Entropy &amp; KL Divergence in Classification<a class="headerlink" href="#information-theoretic-view-entropy-kl-divergence-in-classification" title="Link to this heading">#</a></h3>
<p>In the context of classification, an information-theoretic perspective provides a framework to understand and quantify uncertainty, information content, and divergence between probability distributions. Two key concepts in this view are <strong>entropy</strong> and <strong>Kullback-Leibler (KL) divergence</strong>. Below, we explore these concepts and their relevance to classification tasks.</p>
<hr class="docutils" />
<section id="entropy-in-classification">
<h4><strong>1. Entropy in Classification</strong><a class="headerlink" href="#entropy-in-classification" title="Link to this heading">#</a></h4>
<p><strong>Entropy</strong> is a measure of uncertainty or randomness in a probability distribution. In classification, it quantifies the uncertainty associated with predicting the class label of a data point.</p>
<ul class="simple">
<li><p><strong>Definition</strong>: For a discrete random variable <span class="math notranslate nohighlight">\( Y \)</span> representing class labels with probability distribution <span class="math notranslate nohighlight">\( P(Y) \)</span>, the entropy <span class="math notranslate nohighlight">\( H(Y) \)</span> is defined as:
$<span class="math notranslate nohighlight">\(
H(Y) = - \sum_{i=1}^C P(y_i) \log P(y_i)
\)</span>$
where:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\( C \)</span> is the number of classes.</p></li>
<li><p><span class="math notranslate nohighlight">\( P(y_i) \)</span> is the probability of class <span class="math notranslate nohighlight">\( y_i \)</span>.</p></li>
<li><p>The logarithm is typically base 2 (for bits) or base <span class="math notranslate nohighlight">\( e \)</span> (for nats).</p></li>
</ul>
</li>
<li><p><strong>Interpretation</strong>:</p>
<ul>
<li><p><strong>High entropy</strong>: The class distribution is uniform (e.g., <span class="math notranslate nohighlight">\( P(y_i) = \frac{1}{C} \)</span>), indicating maximum uncertainty. For example, in a binary classification problem with <span class="math notranslate nohighlight">\( P(y_1) = 0.5 \)</span> and <span class="math notranslate nohighlight">\( P(y_2) = 0.5 \)</span>, the entropy is <span class="math notranslate nohighlight">\( H(Y) = 1 \)</span> bit.</p></li>
<li><p><strong>Low entropy</strong>: The distribution is skewed toward one class (e.g., <span class="math notranslate nohighlight">\( P(y_1) = 0.99, P(y_2) = 0.01 \)</span>), indicating low uncertainty.</p></li>
<li><p><strong>Zero entropy</strong>: The outcome is certain (e.g., <span class="math notranslate nohighlight">\( P(y_1) = 1, P(y_2) = 0 \)</span>).</p></li>
</ul>
</li>
<li><p><strong>Role in Classification</strong>:</p>
<ul>
<li><p><strong>Decision Trees</strong>: Entropy is used in algorithms like ID3 or C4.5 to measure the impurity of a node. The goal is to split the data to minimize entropy (i.e., reduce uncertainty about class labels).</p></li>
<li><p><strong>Model Evaluation</strong>: Entropy can help assess the uncertainty in a model’s predictions. For example, a model outputting high-entropy (uniform) probability distributions for test samples may indicate poor confidence.</p></li>
<li><p><strong>Cross-Entropy Loss</strong>: In training classifiers (e.g., neural networks), the cross-entropy loss measures the difference between the true label distribution and the predicted distribution, effectively penalizing high uncertainty in incorrect predictions.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="kl-divergence-in-classification">
<h4><strong>2. KL Divergence in Classification</strong><a class="headerlink" href="#kl-divergence-in-classification" title="Link to this heading">#</a></h4>
<p><strong>Kullback-Leibler (KL) divergence</strong> measures how much one probability distribution differs from another. In classification, it is often used to compare the predicted probability distribution to the true distribution.</p>
<ul class="simple">
<li><p><strong>Definition</strong>: For two probability distributions <span class="math notranslate nohighlight">\( P \)</span> (true distribution) and <span class="math notranslate nohighlight">\( Q \)</span> (approximated/predicted distribution) over the same set of class labels, the KL divergence is:
$<span class="math notranslate nohighlight">\(
D_{KL}(P || Q) = \sum_{i=1}^C P(y_i) \log \frac{P(y_i)}{Q(y_i)}
\)</span>$</p>
<ul>
<li><p><span class="math notranslate nohighlight">\( D_{KL}(P || Q) \geq 0 \)</span>, with equality if and only if <span class="math notranslate nohighlight">\( P = Q \)</span>.</p></li>
<li><p>It is not symmetric: <span class="math notranslate nohighlight">\( D_{KL}(P || Q) \neq D_{KL}(Q || P) \)</span>.</p></li>
</ul>
</li>
<li><p><strong>Interpretation</strong>:</p>
<ul>
<li><p>KL divergence quantifies the “extra information” (in bits or nats) needed to encode samples from <span class="math notranslate nohighlight">\( P \)</span> using a code optimized for <span class="math notranslate nohighlight">\( Q \)</span>.</p></li>
<li><p>A small KL divergence indicates that <span class="math notranslate nohighlight">\( Q \)</span> is a good approximation of <span class="math notranslate nohighlight">\( P \)</span>.</p></li>
<li><p>A large KL divergence suggests that the predicted distribution <span class="math notranslate nohighlight">\( Q \)</span> deviates significantly from the true distribution <span class="math notranslate nohighlight">\( P \)</span>.</p></li>
</ul>
</li>
<li><p><strong>Role in Classification</strong>:</p>
<ul>
<li><p><strong>Cross-Entropy Loss and KL Divergence</strong>: The cross-entropy loss used in classification can be decomposed as:
$<span class="math notranslate nohighlight">\(
H(P, Q) = H(P) + D_{KL}(P || Q)
\)</span>$
where:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\( H(P, Q) = - \sum_{i=1}^C P(y_i) \log Q(y_i) \)</span> is the cross-entropy.</p></li>
<li><p><span class="math notranslate nohighlight">\( H(P) \)</span> is the entropy of the true distribution.</p></li>
<li><p><span class="math notranslate nohighlight">\( D_{KL}(P || Q) \)</span> is the KL divergence.
Since <span class="math notranslate nohighlight">\( H(P) \)</span> is constant for a given true distribution, minimizing the cross-entropy loss is equivalent to minimizing the KL divergence between the true and predicted distributions.</p></li>
</ul>
</li>
<li><p><strong>Model Calibration</strong>: KL divergence can be used to evaluate how well a model’s predicted probabilities align with the true class distribution. A high KL divergence may indicate miscalibration.</p></li>
<li><p><strong>Regularization</strong>: In some models, KL divergence is used as a regularization term. For example, in <strong>variational inference</strong> or <strong>Bayesian neural networks</strong>, KL divergence measures the difference between the learned parameter distribution and a prior distribution.</p></li>
<li><p><strong>Domain Adaptation</strong>: KL divergence can quantify the difference between source and target domain distributions, helping to align feature distributions in transfer learning or domain adaptation tasks.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="practical-implications-in-classification">
<h4><strong>3. Practical Implications in Classification</strong><a class="headerlink" href="#practical-implications-in-classification" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Entropy and Decision Making</strong>:</p>
<ul>
<li><p>Entropy guides feature selection in decision trees by identifying splits that reduce class uncertainty.</p></li>
<li><p>In ensemble methods (e.g., random forests), entropy can help assess the diversity of predictions across trees.</p></li>
<li><p>In active learning, entropy is used to select uncertain samples for labeling, maximizing information gain.</p></li>
</ul>
</li>
<li><p><strong>KL Divergence and Model Optimization</strong>:</p>
<ul>
<li><p>During training, minimizing the cross-entropy loss implicitly minimizes the KL divergence, aligning the predicted probabilities with the true labels.</p></li>
<li><p>In generative models (e.g., GANs or VAEs) used for classification, KL divergence may appear in the objective function to ensure the generated distribution matches the true data distribution.</p></li>
<li><p>In knowledge distillation, KL divergence is used to transfer knowledge from a teacher model (with a “soft” probability distribution) to a student model.</p></li>
</ul>
</li>
<li><p><strong>Evaluation Metrics</strong>:</p>
<ul>
<li><p>While entropy and KL divergence are not typically used directly as evaluation metrics, they underpin metrics like log-loss (cross-entropy) and can inform diagnostic tools for model performance.</p></li>
<li><p>For example, a model with high cross-entropy loss may have a large KL divergence, indicating poor alignment with the true distribution.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="example-binary-classification">
<h4><strong>4. Example: Binary Classification</strong><a class="headerlink" href="#example-binary-classification" title="Link to this heading">#</a></h4>
<p>Consider a binary classification problem with true labels <span class="math notranslate nohighlight">\( P(y=1) = 0.7, P(y=0) = 0.3 \)</span>, and a model’s predicted probabilities <span class="math notranslate nohighlight">\( Q(y=1) = 0.6, Q(y=0) = 0.4 \)</span>.</p>
<ul class="simple">
<li><p><strong>Entropy of True Distribution</strong>:
$<span class="math notranslate nohighlight">\(
H(P) = - [0.7 \log 0.7 + 0.3 \log 0.3] \approx 0.881 \text{ bits}
\)</span>$</p></li>
<li><p><strong>Cross-Entropy</strong>:
$<span class="math notranslate nohighlight">\(
H(P, Q) = - [0.7 \log 0.6 + 0.3 \log 0.4] \approx 0.918 \text{ bits}
\)</span>$</p></li>
<li><p><strong>KL Divergence</strong>:
$<span class="math notranslate nohighlight">\(
D_{KL}(P || Q) = H(P, Q) - H(P) \approx 0.918 - 0.881 = 0.037 \text{ bits}
\)</span><span class="math notranslate nohighlight">\(
The KL divergence is small, indicating that the predicted distribution \)</span> Q <span class="math notranslate nohighlight">\( is close to the true distribution \)</span> P $.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="limitations-and-considerations">
<h4><strong>5. Limitations and Considerations</strong><a class="headerlink" href="#limitations-and-considerations" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Entropy</strong>:</p>
<ul>
<li><p>Entropy assumes a discrete distribution. For continuous variables, differential entropy is used, but it has different properties (e.g., it can be negative).</p></li>
<li><p>Entropy is sensitive to the number of classes; more classes generally increase entropy unless the distribution is highly skewed.</p></li>
</ul>
</li>
<li><p><strong>KL Divergence</strong>:</p>
<ul>
<li><p>KL divergence is asymmetric and not a true distance metric.</p></li>
<li><p>It can be undefined if <span class="math notranslate nohighlight">\( Q(y_i) = 0 \)</span> for any <span class="math notranslate nohighlight">\( y_i \)</span> where <span class="math notranslate nohighlight">\( P(y_i) &gt; 0 \)</span>. Smoothing techniques (e.g., adding a small <span class="math notranslate nohighlight">\( \epsilon \)</span> to probabilities) are often used to avoid this.</p></li>
<li><p>KL divergence is sensitive to small differences in low-probability events, which may or may not be desirable depending on the application.</p></li>
</ul>
</li>
<li><p><strong>Computational Considerations</strong>:</p>
<ul>
<li><p>Estimating entropy and KL divergence requires reliable probability estimates, which can be challenging with limited data or poorly calibrated models.</p></li>
<li><p>In high-dimensional settings, approximating these quantities may require techniques like Monte Carlo sampling or kernel density estimation.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="broader-context">
<h4><strong>6. Broader Context</strong><a class="headerlink" href="#broader-context" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Mutual Information</strong>: Entropy is closely related to mutual information, which measures the reduction in uncertainty about one variable (e.g., class labels) given another (e.g., features). Mutual information can guide feature selection in classification.</p></li>
<li><p><strong>Information Bottleneck</strong>: This framework uses entropy and KL divergence to balance the trade-off between compressing input features and preserving information about the class labels.</p></li>
<li><p><strong>Robustness and Uncertainty</strong>: Information-theoretic measures can help quantify model robustness to adversarial attacks or distributional shifts by analyzing changes in entropy or KL divergence under perturbations.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Entropy</strong> quantifies the uncertainty in a class distribution, guiding decision-making in algorithms like decision trees and informing loss functions like cross-entropy.</p></li>
<li><p><strong>KL Divergence</strong> measures the difference between true and predicted distributions, playing a central role in optimizing classifiers and evaluating model calibration.</p></li>
<li><p>Together, these concepts provide a principled way to understand, train, and evaluate classification models, ensuring that predictions align with the true underlying distribution while minimizing uncertainty.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="multinomial-cross-entropy-loss-multi-class">
<h2>1. <strong>Multinomial Cross-Entropy Loss (Multi-Class)</strong><a class="headerlink" href="#multinomial-cross-entropy-loss-multi-class" title="Link to this heading">#</a></h2>
<p>The provided cost function is the <strong>multinomial cross-entropy loss</strong>, used for multi-class classification with <span class="math notranslate nohighlight">\(K\)</span> classes. It is derived from the negative log-likelihood of the data under a multinomial logistic regression model (softmax classifier).</p>
<section id="id16">
<h3>Formula<a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
<p>For a dataset with <span class="math notranslate nohighlight">\(n\)</span> examples, the cost function is:</p>
<div class="math notranslate nohighlight">
\[ J(\theta) = -\frac{1}{n} \ell(\theta) = -\frac{1}{n} \sum_{i=1}^n \left[ \theta_{y^{(i)}}^\top \vec{x}^{(i)} - \log \sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)}) \right] \]</div>
</section>
<section id="explanation">
<h3>Explanation<a class="headerlink" href="#explanation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Input</strong>: For the <span class="math notranslate nohighlight">\(i\)</span>-th example, <span class="math notranslate nohighlight">\(\vec{x}^{(i)}\)</span> is the feature vector, <span class="math notranslate nohighlight">\(y^{(i)} \in \{1, 2, \dots, K\}\)</span> is the true class label, and <span class="math notranslate nohighlight">\(\theta_l\)</span> is the parameter vector for class <span class="math notranslate nohighlight">\(l\)</span>.</p></li>
<li><p><strong>Probability</strong>: The model predicts the probability of class <span class="math notranslate nohighlight">\(k\)</span> using the softmax function:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ P(y = k | \vec{x}^{(i)}, \theta) = \frac{e(\theta_k^\top \vec{x}^{(i)})}{\sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)})} \]</div>
<ul class="simple">
<li><p><strong>Log-Likelihood</strong>: The log-likelihood for the <span class="math notranslate nohighlight">\(i\)</span>-th example is the log of the probability of the true class <span class="math notranslate nohighlight">\(y^{(i)}\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \log P(y = y^{(i)} | \vec{x}^{(i)}, \theta) = \theta_{y^{(i)}}^\top \vec{x}^{(i)} - \log \sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)}) \]</div>
<ul class="simple">
<li><p><strong>Cost Function</strong>: The negative average log-likelihood over <span class="math notranslate nohighlight">\(n\)</span> examples gives <span class="math notranslate nohighlight">\(J(\theta)\)</span>, which we minimize to find the optimal parameters <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ul>
</section>
<section id="key-features">
<h3>Key Features<a class="headerlink" href="#key-features" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Applies to <strong><span class="math notranslate nohighlight">\(K \geq 2\)</span> classes</strong>.</p></li>
<li><p>Uses the <strong>softmax function</strong> to compute probabilities across all classes.</p></li>
<li><p>The loss penalizes the model when the predicted probability for the true class is low.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="binary-cross-entropy-loss-two-classes">
<h2>2. <strong>Binary Cross-Entropy Loss (Two Classes)</strong><a class="headerlink" href="#binary-cross-entropy-loss-two-classes" title="Link to this heading">#</a></h2>
<p>For binary classification (2 classes, typically labeled <span class="math notranslate nohighlight">\(y \in \{0, 1\}\)</span>), the <strong>binary cross-entropy loss</strong> (also called log loss) is used. It’s a special case of the multinomial cross-entropy loss when <span class="math notranslate nohighlight">\(K = 2\)</span>, but it’s often written in a simpler form using the sigmoid function.</p>
<section id="id17">
<h3>Formula<a class="headerlink" href="#id17" title="Link to this heading">#</a></h3>
<p>For a dataset with <span class="math notranslate nohighlight">\(n\)</span> examples, the binary cross-entropy loss is:</p>
<div class="math notranslate nohighlight">
\[ J(\theta) = -\frac{1}{n} \sum_{i=1}^n \left[ y^{(i)} \log \hat{p}^{(i)} + (1 - y^{(i)}) \log (1 - \hat{p}^{(i)}) \right] \]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{p}^{(i)} = \sigma(\theta^\top \vec{x}^{(i)})\)</span> is the predicted probability of class 1, and <span class="math notranslate nohighlight">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span> is the sigmoid function.</p></li>
<li><p><span class="math notranslate nohighlight">\(y^{(i)} \in \{0, 1\}\)</span> is the true label for the <span class="math notranslate nohighlight">\(i\)</span>-th example.</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> is the parameter vector (single set of weights, unlike the multi-class case).</p></li>
</ul>
</section>
<section id="id18">
<h3>Explanation<a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Probability</strong>: The model predicts the probability of class 1 as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ P(y = 1 | \vec{x}^{(i)}, \theta) = \sigma(\theta^\top \vec{x}^{(i)}) = \hat{p}^{(i)} \]</div>
<ul class="simple">
<li><p>The probability of class 0 is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ P(y = 0 | \vec{x}^{(i)}, \theta) = 1 - \hat{p}^{(i)} \]</div>
<ul class="simple">
<li><p><strong>Log-Likelihood</strong>: The log-likelihood for the <span class="math notranslate nohighlight">\(i\)</span>-th example is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \log P(y = y^{(i)} | \vec{x}^{(i)}, \theta) = y^{(i)} \log \hat{p}^{(i)} + (1 - y^{(i)}) \log (1 - \hat{p}^{(i)}) \]</div>
<ul class="simple">
<li><p><strong>Cost Function</strong>: The negative average log-likelihood gives <span class="math notranslate nohighlight">\(J(\theta)\)</span>, which is minimized to optimize <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ul>
</section>
<section id="id19">
<h3>Key Features<a class="headerlink" href="#id19" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Applies to <strong>exactly 2 classes</strong> (<span class="math notranslate nohighlight">\(y = 0\)</span> or <span class="math notranslate nohighlight">\(y = 1\)</span>).</p></li>
<li><p>Uses the <strong>sigmoid function</strong> to compute the probability of class 1.</p></li>
<li><p>The loss penalizes incorrect predictions by assigning higher loss when the predicted probability <span class="math notranslate nohighlight">\(\hat{p}^{(i)}\)</span> is far from the true label <span class="math notranslate nohighlight">\(y^{(i)}\)</span>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="comparison">
<h2>3. <strong>Comparison</strong><a class="headerlink" href="#comparison" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Aspect</strong></p></th>
<th class="head"><p><strong>Multinomial Cross-Entropy (Multi-Class)</strong></p></th>
<th class="head"><p><strong>Binary Cross-Entropy (Two Classes)</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Number of Classes</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(K \geq 2\)</span> (general case, including 2 or more classes)</p></td>
<td><p>Exactly 2 classes (<span class="math notranslate nohighlight">\(y \in \{0, 1\}\)</span>)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Probability Model</strong></p></td>
<td><p>Softmax: $P(y = k</p></td>
<td><p>\vec{x}, \theta) = \frac{e(\theta_k^\top \vec{x})}{\sum_{l=1}^K e(\theta_l^\top \vec{x})}$</p></td>
</tr>
<tr class="row-even"><td><p><strong>Parameters</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(K\)</span> parameter vectors: <span class="math notranslate nohighlight">\(\theta_1, \theta_2, \dots, \theta_K\)</span> (one per class)</p></td>
<td><p>Single parameter vector: <span class="math notranslate nohighlight">\(\theta\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Loss Formula</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(J(\theta) = -\frac{1}{n} \sum_{i=1}^n \left[ \theta_{y^{(i)}}^\top \vec{x}^{(i)} - \log \sum_{l=1}^K e(\theta_l^\top \vec{x}^{(i)}) \right]\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(J(\theta) = -\frac{1}{n} \sum_{i=1}^n \left[ y^{(i)} \log \hat{p}^{(i)} + (1 - y^{(i)}) \log (1 - \hat{p}^{(i)}) \right]\)</span></p></td>
</tr>
<tr class="row-even"><td><p><strong>Special Case</strong></p></td>
<td><p>Reduces to binary cross-entropy when <span class="math notranslate nohighlight">\(K = 2\)</span> (see below)</p></td>
<td><p>Is a special case of multinomial cross-entropy when <span class="math notranslate nohighlight">\(K = 2\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Interpretation</strong></p></td>
<td><p>Measures how well the predicted probability distribution matches the true class across <span class="math notranslate nohighlight">\(K\)</span> classes</p></td>
<td><p>Measures how well the predicted probability for class 1 (or 0) matches the true binary label</p></td>
</tr>
</tbody>
</table>
</div>
<section id="key-insight-binary-case-as-a-special-case">
<h3>Key Insight: Binary Case as a Special Case<a class="headerlink" href="#key-insight-binary-case-as-a-special-case" title="Link to this heading">#</a></h3>
<p>When <span class="math notranslate nohighlight">\(K = 2\)</span> in the multinomial case, the softmax model can be shown to be equivalent to the sigmoid-based binary model. Let’s derive this briefly:</p>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(K = 2\)</span>, classes are <span class="math notranslate nohighlight">\(y \in \{1, 2\}\)</span> (or relabeled as <span class="math notranslate nohighlight">\(\{0, 1\}\)</span>). The softmax probabilities are:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ P(y = 1 | \vec{x}, \theta) = \frac{e(\theta_1^\top \vec{x})}{e(\theta_1^\top \vec{x}) + e(\theta_2^\top \vec{x})} \]</div>
<div class="math notranslate nohighlight">
\[ P(y = 2 | \vec{x}, \theta) = \frac{e(\theta_2^\top \vec{x})}{e(\theta_1^\top \vec{x}) + e(\theta_2^\top \vec{x})} \]</div>
<ul class="simple">
<li><p>Divide numerator and denominator by <span class="math notranslate nohighlight">\(e(\theta_2^\top \vec{x})\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ P(y = 1 | \vec{x}, \theta) = \frac{e(\theta_1^\top \vec{x} - \theta_2^\top \vec{x})}{1 + e(\theta_1^\top \vec{x} - \theta_2^\top \vec{x})} = \sigma((\theta_1 - \theta_2)^\top \vec{x}) \]</div>
<ul class="simple">
<li><p>Let <span class="math notranslate nohighlight">\(\theta = \theta_1 - \theta_2\)</span>. This matches the sigmoid form: <span class="math notranslate nohighlight">\(P(y = 1) = \sigma(\theta^\top \vec{x})\)</span>.</p></li>
<li><p>The multinomial loss for <span class="math notranslate nohighlight">\(K = 2\)</span> simplifies to the binary cross-entropy loss, confirming they are equivalent up to a reparameterization.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="small-examples">
<h2>4. <strong>Small Examples</strong><a class="headerlink" href="#small-examples" title="Link to this heading">#</a></h2>
<section id="example-1-binary-cross-entropy-2-classes">
<h3>Example 1: Binary Cross-Entropy (2 Classes)<a class="headerlink" href="#example-1-binary-cross-entropy-2-classes" title="Link to this heading">#</a></h3>
<p><strong>Setup</strong>:</p>
<ul class="simple">
<li><p>Dataset: <span class="math notranslate nohighlight">\(n = 2\)</span> examples.</p></li>
<li><p>Example 1: <span class="math notranslate nohighlight">\(\vec{x}^{(1)} = [1, 2]\)</span>, <span class="math notranslate nohighlight">\(y^{(1)} = 1\)</span> (class 1).</p></li>
<li><p>Example 2: <span class="math notranslate nohighlight">\(\vec{x}^{(2)} = [3, 4]\)</span>, <span class="math notranslate nohighlight">\(y^{(2)} = 0\)</span> (class 0).</p></li>
<li><p>Parameters: <span class="math notranslate nohighlight">\(\theta = [0.5, 0.5]\)</span>.</p></li>
</ul>
<p><strong>Calculation</strong>:</p>
<ul class="simple">
<li><p>For example 1: <span class="math notranslate nohighlight">\(\theta^\top \vec{x}^{(1)} = 0.5 \cdot 1 + 0.5 \cdot 2 = 1.5\)</span>, <span class="math notranslate nohighlight">\(\hat{p}^{(1)} = \sigma(1.5) \approx \frac{1}{1 + e^{-1.5}} \approx 0.818\)</span>.</p></li>
<li><p>Loss term: <span class="math notranslate nohighlight">\(y^{(1)} \log \hat{p}^{(1)} + (1 - y^{(1)}) \log (1 - \hat{p}^{(1)}) = 1 \cdot \log(0.818) + 0 \cdot \log(1 - 0.818) \approx \log(0.818) \approx -0.201\)</span>.</p></li>
<li><p>For example 2: <span class="math notranslate nohighlight">\(\theta^\top \vec{x}^{(2)} = 0.5 \cdot 3 + 0.5 \cdot 4 = 3.5\)</span>, <span class="math notranslate nohighlight">\(\hat{p}^{(2)} = \sigma(3.5) \approx 0.970\)</span>.</p></li>
<li><p>Loss term: <span class="math notranslate nohighlight">\(0 \cdot \log(0.970) + 1 \cdot \log(1 - 0.970) \approx \log(0.030) \approx -3.507\)</span>.</p></li>
<li><p>Total loss: <span class="math notranslate nohighlight">\(J(\theta) = -\frac{1}{2} [(-0.201) + (-3.507)] \approx \frac{3.708}{2} \approx 1.854\)</span>.</p></li>
</ul>
</section>
<section id="example-2-multinomial-cross-entropy-3-classes">
<h3>Example 2: Multinomial Cross-Entropy (3 Classes)<a class="headerlink" href="#example-2-multinomial-cross-entropy-3-classes" title="Link to this heading">#</a></h3>
<p><strong>Setup</strong>:</p>
<ul class="simple">
<li><p>Dataset: <span class="math notranslate nohighlight">\(n = 2\)</span> examples, <span class="math notranslate nohighlight">\(K = 3\)</span> classes.</p></li>
<li><p>Example 1: <span class="math notranslate nohighlight">\(\vec{x}^{(1)} = [1, 2]\)</span>, <span class="math notranslate nohighlight">\(y^{(1)} = 2\)</span> (class 2).</p></li>
<li><p>Example 2: <span class="math notranslate nohighlight">\(\vec{x}^{(2)} = [3, 4]\)</span>, <span class="math notranslate nohighlight">\(y^{(2)} = 1\)</span> (class 1).</p></li>
<li><p>Parameters: <span class="math notranslate nohighlight">\(\theta_1 = [0.5, 1.0]\)</span>, <span class="math notranslate nohighlight">\(\theta_2 = [1.0, 0.5]\)</span>, <span class="math notranslate nohighlight">\(\theta_3 = [-0.5, 0.5]\)</span>.</p></li>
</ul>
<p><strong>Calculation</strong>:</p>
<ul class="simple">
<li><p>For example 1: Scores are <span class="math notranslate nohighlight">\(\theta_1^\top \vec{x}^{(1)} = 0.5 \cdot 1 + 1.0 \cdot 2 = 2.5\)</span>, <span class="math notranslate nohighlight">\(\theta_2^\top \vec{x}^{(1)} = 1.0 \cdot 1 + 0.5 \cdot 2 = 2.0\)</span>, <span class="math notranslate nohighlight">\(\theta_3^\top \vec{x}^{(1)} = -0.5 \cdot 1 + 0.5 \cdot 2 = 0.5\)</span>.</p></li>
<li><p>Sum of exponentials: <span class="math notranslate nohighlight">\(e(2.5) \approx 12.182\)</span>, <span class="math notranslate nohighlight">\(e(2.0) \approx 7.389\)</span>, <span class="math notranslate nohighlight">\(e(0.5) \approx 1.649\)</span>, total <span class="math notranslate nohighlight">\(\approx 21.220\)</span>.</p></li>
<li><p>Loss term: <span class="math notranslate nohighlight">\(\theta_2^\top \vec{x}^{(1)} - \log \sum_{l=1}^3 e(\theta_l^\top \vec{x}^{(1)}) = 2.0 - \log(21.220) \approx 2.0 - 3.055 \approx -1.055\)</span>.</p></li>
<li><p>For example 2: Scores are <span class="math notranslate nohighlight">\(\theta_1^\top \vec{x}^{(2)} = 3.5\)</span>, <span class="math notranslate nohighlight">\(\theta_2^\top \vec{x}^{(2)} = 2.0\)</span>, <span class="math notranslate nohighlight">\(\theta_3^\top \vec{x}^{(2)} = 1.0\)</span>.</p></li>
<li><p>Sum of exponentials: <span class="math notranslate nohighlight">\(e(3.5) \approx 33.115\)</span>, <span class="math notranslate nohighlight">\(e(2.0) \approx 7.389\)</span>, <span class="math notranslate nohighlight">\(e(1.0) \approx 2.718\)</span>, total <span class="math notranslate nohighlight">\(\approx 43.222\)</span>.</p></li>
<li><p>Loss term: <span class="math notranslate nohighlight">\(\theta_1^\top \vec{x}^{(2)} - \log \sum_{l=1}^3 e(\theta_l^\top \vec{x}^{(2)}) = 3.5 - \log(43.222) \approx 3.5 - 3.767 \approx -0.267\)</span>.</p></li>
<li><p>Total loss: <span class="math notranslate nohighlight">\(J(\theta) = -\frac{1}{2} [(-1.055) + (-0.267)] \approx \frac{1.322}{2} \approx 0.661\)</span>.</p></li>
</ul>
<hr class="docutils" />
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="3_Naive_Bayes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Naive Bayes</p>
      </div>
    </a>
    <a class="right-next"
       href="5_Dimensionality_Reduction.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Dimensionality Reduction: PCA</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts"><strong>1. Key Concepts</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sigmoid-function"><strong>2. The Sigmoid Function</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-representation"><strong>3. Hypothesis Representation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-function-log-loss"><strong>4. Cost Function (Log Loss)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-optimization"><strong>5. Gradient Descent Optimization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary"><strong>6. Decision Boundary</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiclass-logistic-regression-softmax"><strong>7. Multiclass Logistic Regression (Softmax)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-limitations"><strong>8. Assumptions &amp; Limitations</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-in-python-scikit-learn"><strong>Example in Python (Scikit-learn)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-coin-flipping-with-maximum-likelihood-estimation-mle-and-maximum-a-posteriori-map">Example: Coin Flipping with Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-trust-only-the-data"><strong>MLE: Trust Only the Data</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#map-combine-data-with-a-prior-belief"><strong>MAP: Combine Data with a Prior Belief</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-difference">Key Difference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-recap"><strong>Linear Regression Recap</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-for-classification"><strong>Motivation for Classification</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-components">Key Components</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-function">1. Sigmoid Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties">Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-e-x-in-the-sigmoid">Why Use <span class="math notranslate nohighlight">\(e^x\)</span> in the Sigmoid?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2. Decision Boundary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-formulation">3. Model Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-with-maximum-likelihood-estimation-mle">4. Training with Maximum Likelihood Estimation (MLE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data">Training Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-function">Likelihood Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-logarithm-rules">Basic Logarithm Rules</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rules-involving-e-x-and-ln-x-natural-logarithm">Rules Involving <span class="math notranslate nohighlight">\(e^x\)</span> and <span class="math notranslate nohighlight">\(\ln(x)\)</span> (Natural Logarithm)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-calculating-e-z-or-e-z">Examples of Calculating <span class="math notranslate nohighlight">\(e(z)\)</span> or <span class="math notranslate nohighlight">\(e^z\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-e-0">Example 1: <span class="math notranslate nohighlight">\(e(0)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-e-1">Example 2: <span class="math notranslate nohighlight">\(e(1)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-e-2">Example 3: <span class="math notranslate nohighlight">\(e(2)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-4-e-1">Example 4: <span class="math notranslate nohighlight">\(e(-1)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-5-e-0-5">Example 5: <span class="math notranslate nohighlight">\(e(0.5)\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notes">Notes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood">Log-Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivation-of-the-sigmoid-function">5. Derivation of the Sigmoid Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-derivation">Step-by-Step Derivation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-the-sigmoid-function">6. Gradient of the Sigmoid Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivation">Derivation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-function-and-optimization-objective-for-logistic-regression">Cost Function and Optimization Objective for Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-a-specific-cost-function">Why a Specific Cost Function?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-function">1. Cost Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compact-form">Compact Form</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#full-cost-function">Full Cost Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-measure-of-uncertainty">1. <strong>Entropy: Measure of Uncertainty</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#log-loss-binary-cross-entropy">2. <strong>Log-Loss / Binary Cross-Entropy</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#formula">Formula:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-maximum-likelihood-estimation-mle">2. Connection to Maximum Likelihood Estimation (MLE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Likelihood Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Log-Likelihood</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-and-parameter-updates">3. Gradient Descent and Parameter Updates</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-the-sigmoid">Gradient of the Sigmoid</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-the-cost-function">Gradient of the Cost Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-update-rule">Gradient Descent Update Rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-to-linear-regression">Similarity to Linear Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-weights">4. Interpretation of Weights</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-a-posteriori-map-estimation-and-regularization">5. Maximum A Posteriori (MAP) Estimation and Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#map-estimation">MAP Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-prior-and-l2-regularization">Gaussian Prior and L2 Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#map-estimation-incorporating-prior-knowledge">1. <strong>MAP Estimation: Incorporating Prior Knowledge</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2. <strong>Gaussian Prior and L2 Regularization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-l2-regularization-works">3. <strong>Why L2 Regularization Works</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#small-example-logistic-regression-with-map-and-l2-regularization">Small Example: Logistic Regression with MAP and L2 Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-setup">Problem Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-log-likelihood-for-logistic-regression">Step 1: Log-Likelihood for Logistic Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-gaussian-prior">Step 2: Gaussian Prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-map-objective">Step 3: MAP Objective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-interpretation">Step 4: Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-numerical-intuition">Step 5: Numerical Intuition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-and-map-in-context">6. MLE and MAP in Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-a-posteriori-map">Maximum A Posteriori (MAP)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-vs-map">MLE vs. MAP</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-biased-coin">7. Example: Biased Coin</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood">Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Log-Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-solution">MLE Solution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-maximum-likelihood">8. Conditional Maximum Likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-from-scratch">Logistic Regression from Scratch</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-code">Explanation of Code:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Notes:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-regression-explained">Softmax Regression Explained</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Key Components</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">1. Model Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-score-computation">Step 1: Score Computation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-softmax-function">Step 2: Softmax Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#identifiability-note">Identifiability Note</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deeper-explanation-of-the-softmax-function">Deeper Explanation of the Softmax Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-softmax-function">1. <strong>What is the Softmax Function?</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-the-softmax-function">2. <strong>Why Use the Softmax Function?</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-in-multi-class-classification">3. <strong>Softmax in Multi-Class Classification</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-softmax">4. <strong>Properties of Softmax</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#small-example-applying-the-softmax-function">Small Example: Applying the Softmax Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Problem Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-compute-scores-logits">Step 1: Compute Scores (Logits)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-apply-the-softmax-function">Step 2: Apply the Softmax Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-interpretation">Step 3: Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-numerical-stability-optional">Step 4: Numerical Stability (Optional)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-interpretation">2. Probabilistic Interpretation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-rule">Decision Rule</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objective-maximum-likelihood-estimation-mle">3. Learning Objective: Maximum Likelihood Estimation (MLE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Likelihood Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Log-Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Cost Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">4. Gradient of the Cost Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-derivation">Gradient Derivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-update">Gradient Descent Update</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-logistic-regression">5. Connection to Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">6. Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#information-theoretic-view-entropy-kl-divergence-in-classification">Information-Theoretic View: Entropy &amp; KL Divergence in Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-in-classification"><strong>1. Entropy in Classification</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kl-divergence-in-classification"><strong>2. KL Divergence in Classification</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-implications-in-classification"><strong>3. Practical Implications in Classification</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-binary-classification"><strong>4. Example: Binary Classification</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-and-considerations"><strong>5. Limitations and Considerations</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#broader-context"><strong>6. Broader Context</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-cross-entropy-loss-multi-class">1. <strong>Multinomial Cross-Entropy Loss (Multi-Class)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Formula</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation">Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-features">Key Features</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-cross-entropy-loss-two-classes">2. <strong>Binary Cross-Entropy Loss (Two Classes)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Formula</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Key Features</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison">3. <strong>Comparison</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insight-binary-case-as-a-special-case">Key Insight: Binary Case as a Special Case</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#small-examples">4. <strong>Small Examples</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-binary-cross-entropy-2-classes">Example 1: Binary Cross-Entropy (2 Classes)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-multinomial-cross-entropy-3-classes">Example 2: Multinomial Cross-Entropy (3 Classes)</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chandravesh Chaudhari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>