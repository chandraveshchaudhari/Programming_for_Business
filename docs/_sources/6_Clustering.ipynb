{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# K-Means and Clustering\n",
   "id": "23d23833a4484c60"
  },
  {
   "cell_type": "markdown",
   "id": "f8480bd3-c2ce-4437-b89c-364258b955de",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border-width:0; color:gray; background-color:gray\">\n",
    "\n",
    "<table width=\"100%\">\n",
    "  <tr>\n",
    "    <td align=\"left\" width=\"60%\">\n",
    "      <h2 style=\"color: #2c3e50; font-family: Arial, sans-serif;\">\n",
    "        Machine Learning for Business\n",
    "      </h2>\n",
    "      <p>\n",
    "        <strong>Chandravesh Chaudhari</strong><br>\n",
    "        Assistant Professor<br>\n",
    "        School of Business and Management<br>\n",
    "        <a href=\"mailto:chandraveshchaudhari@gmail.com\" style=\"color: #2980b9; text-decoration: none;\">\n",
    "          chandraveshchaudhari@gmail.com\n",
    "        </a>\n",
    "      </p>\n",
    "    </td>\n",
    "    <td align=\"right\" width=\"40%\">\n",
    "      <img src=\"logo.jpg\" alt=\"Christ University\" width=\"250\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<hr style=\"height:2px; border-width:0; color:gray; background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6a9dd-f07f-44b5-807e-e02824485ca9",
   "metadata": {},
   "source": [
    "K-Means and Clustering\n",
    "\n",
    "\t• Objective Function Minimization\n",
    "\t• Lloyd’s Algorithm\n",
    "\t• Distance Metrics (Euclidean, Manhattan)\n",
    "Python: K-Means with Random Initialization"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "K-Means and Clustering\n",
    "K-Means is an unsupervised clustering algorithm that partitions a dataset into ( k ) clusters by minimizing the variance within each cluster. It assigns each data point to the nearest cluster centroid and iteratively updates the centroids to optimize the objective function.\n",
    "\n",
    "1. Objective Function Minimization\n",
    "Objective: Minimize the within-cluster sum of squares (WCSS), also known as the inertia, which measures the total squared distance between each point and its assigned cluster centroid.\n",
    "\t•\tMathematical Formulation: Given a dataset ( X = {x_1, x_2, \\dots, x_n} ) with ( n ) points in ( \\mathbb{R}^p ), and ( k ) clusters with centroids ( {\\mu_1, \\mu_2, \\dots, \\mu_k} ), the objective function is: [ J = \\sum_{i=1}^n \\sum_{j=1}^k r_{ij} | x_i - \\mu_j |^2 ] where:\n",
    "\t◦\t( r_{ij} = 1 ) if point ( x_i ) is assigned to cluster ( j ), and ( 0 ) otherwise.\n",
    "\t◦\t( | x_i - \\mu_j |^2 ) is the squared distance between point ( x_i ) and centroid ( \\mu_j ).\n",
    "\t•\tGoal: Find the optimal assignments ( r_{ij} ) and centroids ( \\mu_j ) that minimize ( J ).\n",
    "\n",
    "2. Lloyd’s Algorithm\n",
    "Lloyd’s algorithm is the standard iterative method for solving the K-Means optimization problem. It alternates between two steps until convergence:\n",
    "\t1\tAssignment Step:\n",
    "\t◦\tAssign each data point to the nearest centroid based on a distance metric: [ r_{ij} = \\begin{cases} 1 & \\text{if } j = \\arg\\min_{j’} | x_i - \\mu_{j’} |^2 \\ 0 & \\text{otherwise} \\end{cases} ]\n",
    "\t2\tUpdate Step:\n",
    "\t◦\tRecalculate the centroid of each cluster as the mean of all points assigned to it: [ \\mu_j = \\frac{\\sum_{i=1}^n r_{ij} x_i}{\\sum_{i=1}^n r_{ij}} ]\n",
    "\t•\tConvergence:\n",
    "\t◦\tThe algorithm iterates until the centroids stabilize (i.e., no significant change in ( J ) or assignments) or a maximum number of iterations is reached.\n",
    "\t◦\tNote: K-Means is guaranteed to converge to a local minimum, but the solution may not be globally optimal.\n",
    "\t•\tChallenges:\n",
    "\t◦\tSensitive to initial centroid placement, which can lead to suboptimal solutions.\n",
    "\t◦\tRequires specifying ( k ) in advance.\n",
    "\t◦\tAssumes clusters are spherical and of similar size due to the use of distance-based assignments.\n",
    "\n",
    "3. Distance Metrics\n",
    "The choice of distance metric affects how points are assigned to clusters. Common metrics include:\n",
    "\t•\tEuclidean Distance:\n",
    "\t◦\tThe standard distance metric used in K-Means: [ d(x_i, \\mu_j) = \\sqrt{\\sum_{l=1}^p (x_{il} - \\mu_{jl})^2} ]\n",
    "\t◦\tAssumes clusters are spherical and works well for continuous, isotropic data.\n",
    "\t◦\tSensitive to feature scaling.\n",
    "\t•\tManhattan Distance:\n",
    "\t◦\tAlso known as L1 distance or city-block distance: [ d(x_i, \\mu_j) = \\sum_{l=1}^p |x_{il} - \\mu_{jl}| ]\n",
    "\t◦\tBetter suited for data with non-spherical clusters or when robustness to outliers is desired.\n",
    "\t◦\tLess sensitive to large differences in individual dimensions compared to Euclidean distance.\n",
    "\t•\tOther Metrics:\n",
    "\t◦\tCosine similarity, Mahalanobis distance, or custom metrics can be used for specific applications, but they require modifications to the standard K-Means algorithm.\n",
    "\t•\tChoosing a Metric:\n",
    "\t◦\tEuclidean is default for K-Means due to its compatibility with the mean-based centroid update.\n",
    "\t◦\tStandardize features (zero mean, unit variance) to ensure fair contributions from all dimensions.\n",
    "\n",
    "4. Python: K-Means with Random Initialization\n",
    "Below is a Python implementation of K-Means using random initialization, followed by a scikit-learn example for comparison.\n",
    "Implementation from Scratch\n",
    "import numpy as np\n",
    "\n",
    "def kmeans(X, k, max_iters=100, distance_metric='euclidean'):\n",
    "    # Randomly initialize centroids\n",
    "    n_samples, n_features = X.shape\n",
    "    idx = np.random.choice(n_samples, k, replace=False)\n",
    "    centroids = X[idx]\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        # Assignment step\n",
    "        distances = np.zeros((n_samples, k))\n",
    "        for j in range(k):\n",
    "            if distance_metric == 'euclidean':\n",
    "                distances[:, j] = np.sum((X - centroids[j])**2, axis=1)\n",
    "            elif distance_metric == 'manhattan':\n",
    "                distances[:, j] = np.sum(np.abs(X - centroids[j]), axis=1)\n",
    "        \n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Update step\n",
    "        new_centroids = np.zeros_like(centroids)\n",
    "        for j in range(k):\n",
    "            if np.sum(labels == j) > 0:  # Avoid division by zero\n",
    "                new_centroids[j] = np.mean(X[labels == j], axis=0)\n",
    "            else:\n",
    "                new_centroids[j] = centroids[j]  # Keep old centroid if no points assigned\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.all(centroids == new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    # Compute WCSS (inertia)\n",
    "    inertia = 0\n",
    "    for j in range(k):\n",
    "        if distance_metric == 'euclidean':\n",
    "            inertia += np.sum((X[labels == j] - centroids[j])**2)\n",
    "        elif distance_metric == 'manhattan':\n",
    "            inertia += np.sum(np.abs(X[labels == j] - centroids[j]))\n",
    "    \n",
    "    return labels, centroids, inertia\n",
    "\n",
    "# Example usage\n",
    "np.random.seed(42)\n",
    "X = np.concatenate([np.random.normal(0, 1, (50, 2)),\n",
    "                    np.random.normal(5, 1, (50, 2)),\n",
    "                    np.random.normal(10, 1, (50, 2))])  # 3 clusters\n",
    "k = 3\n",
    "labels, centroids, inertia = kmeans(X, k, distance_metric='euclidean')\n",
    "\n",
    "print(\"Cluster Labels:\", labels)\n",
    "print(\"Centroids:\", centroids)\n",
    "print(\"Inertia (WCSS):\", inertia)\n",
    "Using scikit-learn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply K-Means\n",
    "kmeans_sklearn = KMeans(n_clusters=k, init='random', max_iter=100, random_state=42)\n",
    "labels_sklearn = kmeans_sklearn.fit_predict(X_scaled)\n",
    "centroids_sklearn = scaler.inverse_transform(kmeans_sklearn.cluster_centers_)  # Transform back\n",
    "inertia_sklearn = kmeans_sklearn.inertia_\n",
    "\n",
    "print(\"scikit-learn Cluster Labels:\", labels_sklearn)\n",
    "print(\"scikit-learn Centroids:\", centroids_sklearn)\n",
    "print(\"scikit-learn Inertia (WCSS):\", inertia_sklearn)\n",
    "Output Explanation\n",
    "\t•\tCluster Labels: The cluster assignment for each data point (0 to ( k-1 )).\n",
    "\t•\tCentroids: The coordinates of the cluster centers.\n",
    "\t•\tInertia (WCSS): The objective function value, measuring the total within-cluster variance.\n",
    "Notes\n",
    "\t•\tRandom Initialization: The scratch implementation uses random selection of initial centroids, which can lead to different results across runs. For better results, consider K-Means++ initialization (available in scikit-learn).\n",
    "\t•\tStandardization: Always standardize the data before clustering to ensure all features contribute equally.\n",
    "\t•\tChoosing ( k ): Use the elbow method (plotting inertia vs. ( k )) or silhouette score to select an appropriate number of clusters.\n",
    "\t•\tscikit-learn: More robust, supports K-Means++ initialization, and handles edge cases efficiently.\n",
    "\n",
    "Summary\n",
    "\t•\tObjective Function: K-Means minimizes the within-cluster sum of squares (WCSS).\n",
    "\t•\tLloyd’s Algorithm: Iteratively assigns points to the nearest centroid and updates centroids.\n",
    "\t•\tDistance Metrics: Euclidean is standard, but Manhattan or others can be used for specific cases.\n",
    "\t•\tPython Implementation: Random initialization works but is sensitive to starting points; scikit-learn’s K-Means is recommended for production.\n",
    "Let me know if you need further details, visualization code, or help with evaluating clustering performance!\n"
   ],
   "id": "6b8872148270daab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ce5dc-175c-41ba-87f6-44345172dd89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "⸻\n",
    "\n",
    "K-means\n",
    "\n",
    "The KMeans algorithm clusters data by trying to separate samples into $n$ groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified. It scales well to large numbers of samples and has been used across a large range of application areas in many different fields.\n",
    "\n",
    "The k-means algorithm divides a set of $N$ samples $X$ into $K$ disjoint clusters $C$, each described by the mean $\\mu_j$ of the samples in the cluster. The means are commonly called the cluster “centroids”; note that they are not, in general, points from $X$, although they live in the same space.\n",
    "\n",
    "The K-means algorithm aims to choose centroids that minimize the inertia, or within-cluster sum-of-squares criterion:\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{n} \\min_{\\mu_j \\in C} \\left( |x_i - \\mu_j|^2 \\right)\n",
    "$$\n",
    "\n",
    "Inertia can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:\n",
    "\t•\tInertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters or manifolds with irregular shapes.\n",
    "\t•\tInertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”).\n",
    "\t•\tRunning a dimensionality reduction algorithm such as PCA prior to k-means clustering can alleviate this problem and speed up the computations.\n",
    "\n",
    "⸻\n"
   ],
   "id": "dc514ce197080bb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094c54a9-c888-408f-aa09-3a1e719b18e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe1db84-e451-4f90-b345-d524ec0e3080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b5c83-1450-4d95-936e-ac7d5422cfb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300c1b7-8314-42f2-9682-b6cb3d4abd76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96094fb4-8fb4-438c-8692-fa7f1451f56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b1a06-c923-45a2-8a72-8ad366c4ae60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb9563-a571-44e8-8131-e947a706fe01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b48390-8159-43ff-952a-d2438b318360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8eb419-02a1-4118-87ec-8897bcbdeae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7593e826-4c08-436c-b20f-468bbf19519d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1ebf74b-af09-4334-818e-a259f24bd88e",
   "metadata": {},
   "source": [
    "# Step-by-Step: KMeans Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ed7eb-670d-46da-8493-6de27a46b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Step 1: Import KMeans and Reduce Data\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Use 2D PCA coordinates of stocks (each stock is a point)\n",
    "X_cluster = W.T  # shape (2, num_stocks)\n",
    "X_cluster = X_cluster.T  # shape (num_stocks, 2)\n",
    "print(\"Clustering Data Shape:\", X_cluster.shape)  # (10, 2)\n",
    "\n",
    "# KMeans clustering (e.g., 3 clusters)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Visualize Clusters in 2D PCA Space\n",
    "\n",
    "# Plot stocks with cluster colors\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "colors = ['red', 'green', 'blue', 'purple', 'orange']\n",
    "for i, ticker in enumerate(tickers):\n",
    "    plt.scatter(X_cluster[i, 0], X_cluster[i, 1], color=colors[labels[i] % len(colors)], s=100)\n",
    "    plt.text(X_cluster[i, 0]+0.01, X_cluster[i, 1]+0.01, ticker, fontsize=12)\n",
    "\n",
    "plt.title('KMeans Clustering of Stocks in 2D PCA Space')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 3 (Optional): Cluster Membership Table\n",
    "\n",
    "# Show which stocks belong to which cluster\n",
    "df_clusters = pd.DataFrame({'Stock': tickers, 'Cluster': labels})\n",
    "df_clusters = df_clusters.sort_values('Cluster')\n",
    "print(df_clusters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Elbow Method vs Silhouette Score — Which is More Popular?\n",
    "\n",
    "1. Elbow Method\n",
    "\t•\tIdea: Plot total within-cluster variance (inertia) vs number of clusters k.\n",
    "\t•\tAs k increases, inertia decreases. But after a point, the rate of improvement “bends” like an elbow — suggesting the optimal number of clusters.\n",
    "\t•\tPopular because it’s simple and visual.\n",
    "\t•\tWorks well when you expect compact, spherical clusters.\n",
    "\t•\tUsed more often in practice for KMeans, especially in business or EDA contexts.\n",
    "\n",
    "2. Silhouette Score\n",
    "\t•\tIdea: Measures how similar a point is to its own cluster vs others.\n",
    "\t•\tScore ranges from -1 to 1 (higher is better).\n",
    "\t•\tTells both how well-separated and how dense clusters are.\n",
    "\t•\tBetter for algorithmic tuning, especially for non-spherical clusters or comparing across clustering algorithms.\n",
    "\n",
    "⸻\n",
    "\n",
    "Which One Should You Use?\n",
    "\n",
    "Use Case\tRecommended Method\n",
    "Quick visual guess (KMeans)\tElbow Method\n",
    "Want a more objective, numerical score\tSilhouette Score\n",
    "Comparing non-KMeans algorithms\tSilhouette Score\n",
    "Exploratory, business reporting\tElbow (visual appeal)\n",
    "Small datasets or when clusters are hard to spot\tSilhouette is more reliable\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n"
   ],
   "id": "968ea0c2a0b7c34d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c72d27-2b7a-41c4-b86e-7ddd5a599db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "K_range = range(1, 10)\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(X_cluster)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "# Plot Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, inertias, 'o-', color='darkblue')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia (Within-Cluster Sum of Squares)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b3c12-8950-46f6-a35b-a49b78b69d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ae1b4-d785-481b-bfb8-6a08937604a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, K=5, max_iters=100, plot_steps=False):\n",
    "        self.K = K\n",
    "        self.max_iters = max_iters\n",
    "        self.plot_steps = plot_steps\n",
    "\n",
    "        # list of sample indices for each cluster\n",
    "        self.clusters = [[] for _ in range(self.K)]\n",
    "        # the centers (mean feature vector) for each cluster\n",
    "        self.centroids = []\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X = X\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "\n",
    "        # initialize\n",
    "        random_sample_idxs = np.random.choice(self.n_samples, self.K, replace=False)\n",
    "        self.centroids = [self.X[idx] for idx in random_sample_idxs]\n",
    "\n",
    "        # Optimize clusters\n",
    "        for _ in range(self.max_iters):\n",
    "            # Assign samples to closest centroids (create clusters)\n",
    "            self.clusters = self._create_clusters(self.centroids)\n",
    "\n",
    "            if self.plot_steps:\n",
    "                self.plot()\n",
    "\n",
    "            # Calculate new centroids from the clusters\n",
    "            centroids_old = self.centroids\n",
    "            self.centroids = self._get_centroids(self.clusters)\n",
    "\n",
    "            # check if clusters have changed\n",
    "            if self._is_converged(centroids_old, self.centroids):\n",
    "                break\n",
    "\n",
    "            if self.plot_steps:\n",
    "                self.plot()\n",
    "\n",
    "        # Classify samples as the index of their clusters\n",
    "        return self._get_cluster_labels(self.clusters)\n",
    "\n",
    "    def _get_cluster_labels(self, clusters):\n",
    "        # each sample will get the label of the cluster it was assigned to\n",
    "        labels = np.empty(self.n_samples)\n",
    "\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            for sample_index in cluster:\n",
    "                labels[sample_index] = cluster_idx\n",
    "        return labels\n",
    "\n",
    "    def _create_clusters(self, centroids):\n",
    "        # Assign the samples to the closest centroids to create clusters\n",
    "        clusters = [[] for _ in range(self.K)]\n",
    "        for idx, sample in enumerate(self.X):\n",
    "            centroid_idx = self._closest_centroid(sample, centroids)\n",
    "            clusters[centroid_idx].append(idx)\n",
    "        return clusters\n",
    "\n",
    "    def _closest_centroid(self, sample, centroids):\n",
    "        # distance of the current sample to each centroid\n",
    "        distances = [euclidean_distance(sample, point) for point in centroids]\n",
    "        closest_index = np.argmin(distances)\n",
    "        return closest_index\n",
    "\n",
    "    def _get_centroids(self, clusters):\n",
    "        # assign mean value of clusters to centroids\n",
    "        centroids = np.zeros((self.K, self.n_features))\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            cluster_mean = np.mean(self.X[cluster], axis=0)\n",
    "            centroids[cluster_idx] = cluster_mean\n",
    "        return centroids\n",
    "\n",
    "    def _is_converged(self, centroids_old, centroids):\n",
    "        # distances between each old and new centroids, fol all centroids\n",
    "        distances = [\n",
    "            euclidean_distance(centroids_old[i], centroids[i]) for i in range(self.K)\n",
    "        ]\n",
    "        return sum(distances) == 0\n",
    "\n",
    "    def plot(self):\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        for i, index in enumerate(self.clusters):\n",
    "            point = self.X[index].T\n",
    "            ax.scatter(*point)\n",
    "\n",
    "        for point in self.centroids:\n",
    "            ax.scatter(*point, marker=\"x\", color=\"black\", linewidth=2)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Testing\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.datasets import make_blobs\n",
    "\n",
    "    X, y = make_blobs(\n",
    "        centers=3, n_samples=500, n_features=2, shuffle=True, random_state=40\n",
    "    )\n",
    "    print(X.shape)\n",
    "\n",
    "    clusters = len(np.unique(y))\n",
    "    print(clusters)\n",
    "\n",
    "    k = KMeans(K=clusters, max_iters=150, plot_steps=True)\n",
    "    y_pred = k.predict(X)\n",
    "\n",
    "    k.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c151bef-a77a-46c0-8854-87f623a39616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a591d21-7e83-493f-848c-3353d8d76b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524bd5d-0757-4de4-b912-21b763b6f640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c3a5c-e9e7-45e8-a41f-99d216111658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7a3fe7-2751-4a8b-b2c8-ed35176263e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23513c0c-8452-4ed7-aa3a-1e5e7cc647b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b5f02-2de9-4f72-a34b-2aabb85a739d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149eaec-6274-4060-abd3-a86ee89a8db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a328110-45a1-4559-b12e-7effa20fa56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c579fd-c5ae-4171-ae3e-118be55a0848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f5149-f930-40ac-888b-4f568385d3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd02cdb-e048-4f33-ba72-6c4d7b9d906d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33bb7d6-bd02-4c45-9360-1316572988b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161211d5-859d-4abc-88de-8e0b223da742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba45efc-ac7a-4461-802c-9d242d2edf75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede37c4-03fd-4022-95af-e211ecf9e5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701f4a4-ab48-4ea3-913a-7d77fb61e6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588808e8-94c0-418e-a8e8-a696fa2334af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fed77-b59f-4ccf-9698-dbe71c01794f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
