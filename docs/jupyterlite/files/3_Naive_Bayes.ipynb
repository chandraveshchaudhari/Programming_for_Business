{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "582b8688-2137-42a1-abf9-b1ddaadbe6ae",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "\t• Conditional Independence Assumption\n",
    "\t• Bag-of-Words Representation\n",
    "\t• Multinomial vs Bernoulli Naive Bayes\n",
    "\t• Python: Text Classification with Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265cd9d7-e13f-4950-ae2b-6d5441861db6",
   "metadata": {},
   "source": [
    "\n",
    "# Probability Math for Understanding Naive Bayes\n",
    "\n",
    "To understand Naive Bayes, we first need to cover the foundational probability concepts that underpin it. This section progresses from basic probability to the specifics of Naive Bayes.\n",
    "\n",
    "## 1. Simple Probability\n",
    "Probability measures the likelihood of an event occurring, expressed as a number between 0 and 1.\n",
    "\n",
    "- **Definition**: The probability of an event $A$, denoted $P(A)$, is:\n",
    "\n",
    "$$\n",
    "P(A) = \\frac{\\text{Number of favorable outcomes for } A}{\\text{Total number of possible outcomes}}\n",
    "$$\n",
    "\n",
    "- **Example**: If you roll a fair six-sided die, the probability of rolling a 3 is:\n",
    "\n",
    "$$\n",
    "P(3) = \\frac{1}{6} \\approx 0.1667\n",
    "$$\n",
    "\n",
    "- **Properties**:\n",
    "  - $0 \\leq P(A) \\leq 1$\n",
    "  - The probability of the entire sample space $S$ is $P(S) = 1$.\n",
    "  - For mutually exclusive events $A$ and $B$, $P(A \\text{ or } B) = P(A) + P(B)$.\n",
    "\n",
    "## 2. Joint Probability\n",
    "Joint probability is the probability of two or more events occurring together.\n",
    "\n",
    "- **Definition**: For events $A$ and $B$, the joint probability is $P(A \\cap B)$, the probability that both $A$ and $B$ occur.\n",
    "\n",
    "- **Independent Events**: If $A$ and $B$ are independent (the occurrence of one does not affect the other), then:\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A) \\cdot P(B)\n",
    "$$\n",
    "\n",
    "- **Example**: If you flip two fair coins, the probability of getting heads on both is:\n",
    "\n",
    "$$\n",
    "P(\\text{Heads}_1 \\cap \\text{Heads}_2) = P(\\text{Heads}_1) \\cdot P(\\text{Heads}_2) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\n",
    "$$\n",
    "\n",
    "- **Dependent Events**: If $A$ and $B$ are not independent, we use conditional probability (see below).\n",
    "\n",
    "## 3. Conditional Probability\n",
    "Conditional probability measures the probability of an event given that another event has occurred.\n",
    "\n",
    "- **Definition**: The probability of event $A$ given event $B$ has occurred is:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}, \\quad \\text{where } P(B) > 0\n",
    "$$\n",
    "\n",
    "- **Example**: In a deck of 52 cards, what is the probability of drawing a heart given that the card is red? There are 26 red cards, 13 of which are hearts:\n",
    "\n",
    "$$\n",
    "P(\\text{Heart}|\\text{Red}) = \\frac{P(\\text{Heart} \\cap \\text{Red})}{P(\\text{Red})} = \\frac{\\frac{13}{52}}{\\frac{26}{52}} = \\frac{13}{26} = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "- **Relation to Joint Probability**:\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A|B) \\cdot P(B) = P(B|A) \\cdot P(A)\n",
    "$$\n",
    "\n",
    "## 4. Law of Total Probability\n",
    "The law of total probability helps compute the probability of an event by considering all possible scenarios.\n",
    "\n",
    "- **Definition**: If events $B_1, B_2, \\dots, B_n$ are mutually exclusive and exhaustive (they cover the entire sample space), then for any event $A$:\n",
    "\n",
    "$$\n",
    "P(A) = \\sum_{i=1}^n P(A|B_i) \\cdot P(B_i)\n",
    "$$\n",
    "\n",
    "- **Example**: Suppose 60% of emails are spam ($P(\\text{Spam}) = 0.6$), and 40% are not ($P(\\text{Non-Spam}) = 0.4$). The probability an email contains the word \"free\" is 0.8 for spam and 0.1 for non-spam. The total probability of \"free\" is:\n",
    "\n",
    "$$\n",
    "P(\\text{Free}) = P(\\text{Free}|\\text{Spam}) \\cdot P(\\text{Spam}) + P(\\text{Free}|\\text{Non-Spam}) \\cdot P(\\text{Non-Spam})\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.8 \\cdot 0.6) + (0.1 \\cdot 0.4) = 0.48 + 0.04 = 0.52\n",
    "$$\n",
    "\n",
    "## 5. Bayes' Theorem\n",
    "Bayes' Theorem relates conditional probabilities and is the foundation of Naive Bayes.\n",
    "\n",
    "- **Definition**:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "- **Interpretation**:\n",
    "  - $P(A|B)$: **Posterior**, the probability of $A$ given $B$.\n",
    "  - $P(B|A)$: **Likelihood**, the probability of $B$ given $A$.\n",
    "  - $P(A)$: **Prior**, the probability of $A$ before observing $B$.\n",
    "  - $P(B)$: **Evidence**, the total probability of $B$, often computed using the law of total probability.\n",
    "\n",
    "- **Example**: Using the email example, what is the probability an email is spam given it contains \"free\"?\n",
    "\n",
    "$$\n",
    "P(\\text{Spam}|\\text{Free}) = \\frac{P(\\text{Free}|\\text{Spam}) \\cdot P(\\text{Spam})}{P(\\text{Free})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{0.8 \\cdot 0.6}{0.52} = \\frac{0.48}{0.52} \\approx 0.923\n",
    "$$\n",
    "\n",
    "## 6. Naive Bayes Classifier\n",
    "Naive Bayes applies Bayes' Theorem to classification, assuming features are conditionally independent given the class.\n",
    "\n",
    "### 6.1. Setup\n",
    "Given a data point with features $X = \\{x_1, x_2, \\dots, x_n\\}$ and class labels $C \\in \\{C_1, C_2, \\dots, C_k\\}$, we want to find the class that maximizes the posterior probability:\n",
    "\n",
    "$$\n",
    "P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\n",
    "$$\n",
    "\n",
    "### 6.2. Naive Assumption\n",
    "The \"naive\" assumption is that features $x_1, x_2, \\dots, x_n$ are **conditionally independent** given the class $C$. Thus, the joint likelihood is:\n",
    "\n",
    "$$\n",
    "P(X|C) = P(x_1, x_2, \\dots, x_n|C) = \\prod_{i=1}^n P(x_i|C)\n",
    "$$\n",
    "\n",
    "So, the posterior becomes:\n",
    "\n",
    "$$\n",
    "P(C|X) = \\frac{P(C) \\cdot \\prod_{i=1}^n P(x_i|C)}{P(X)}\n",
    "$$\n",
    "\n",
    "Since $P(X)$ is constant across classes, we maximize:\n",
    "\n",
    "$$\n",
    "P(C|X) \\propto P(C) \\cdot \\prod_{i=1}^n P(x_i|C)\n",
    "$$\n",
    "\n",
    "### 6.3. Classification\n",
    "Choose the class with the highest posterior:\n",
    "\n",
    "$$\n",
    "\\hat{C} = \\arg\\max_C \\left[ P(C) \\cdot \\prod_{i=1}^n P(x_i|C) \\right]\n",
    "$$\n",
    "\n",
    "To avoid numerical underflow, use log-probabilities:\n",
    "\n",
    "$$\n",
    "\\hat{C} = \\arg\\max_C \\left[ \\log P(C) + \\sum_{i=1}^n \\log P(x_i|C) \\right]\n",
    "$$\n",
    "\n",
    "### 6.4. Estimating Probabilities\n",
    "- **Prior**: Estimated from training data:\n",
    "\n",
    "$$\n",
    "P(C) = \\frac{\\text{Number of instances of class } C}{\\text{Total number of instances}}\n",
    "$$\n",
    "\n",
    "- **Likelihood**:\n",
    "  - **Categorical Features**:\n",
    "\n",
    "  $$\n",
    "  P(x_i|C) = \\frac{\\text{Count of } x_i \\text{ in class } C}{\\text{Total instances in class } C}\n",
    "  $$\n",
    "\n",
    "  Use **Laplace smoothing** to avoid zero probabilities:\n",
    "\n",
    "  $$\n",
    "  P(x_i|C) = \\frac{\\text{Count of } x_i \\text{ in class } C + 1}{\\text{Total instances in class } C + K}\n",
    "  $$\n",
    "\n",
    "  where $K$ is the number of possible values for $x_i$.\n",
    "\n",
    "  - **Continuous Features** (Gaussian Naive Bayes):\n",
    "\n",
    "  $$\n",
    "  P(x_i|C) = \\frac{1}{\\sqrt{2\\pi\\sigma_C^2}} \\exp\\left(-\\frac{(x_i - \\mu_C)^2}{2\\sigma_C^2}\\right)\n",
    "  $$\n",
    "\n",
    "  where $\\mu_C$ and $\\sigma_C^2$ are the mean and variance of feature $x_i$ in class $C$.\n",
    "\n",
    "### 6.5. Example\n",
    "Classify an email as Spam ($C_1$) or Non-Spam ($C_2$) based on two features: $x_1 = \\text{\"free\" (yes/no)}$, $x_2 = \\text{\"urgent\" (yes/no)}$. Given:\n",
    "- $P(C_1) = 0.6$, $P(C_2) = 0.4$\n",
    "- $P(\\text{free}|C_1) = 0.8$, $P(\\text{free}|C_2) = 0.1$\n",
    "- $P(\\text{urgent}|C_1) = 0.5$, $P(\\text{urgent}|C_2) = 0.2$\n",
    "\n",
    "For an email with $\\text{free=yes}$, $\\text{urgent=yes}$:\n",
    "\n",
    "- Spam:\n",
    "\n",
    "$$\n",
    "P(C_1|X) \\propto 0.6 \\cdot 0.8 \\cdot 0.5 = 0.24\n",
    "$$\n",
    "\n",
    "- Non-Spam:\n",
    "\n",
    "$$\n",
    "P(C_2|X) \\propto 0.4 \\cdot 0.1 \\cdot 0.2 = 0.008\n",
    "$$\n",
    "\n",
    "Since $0.24 > 0.008$, classify as Spam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0cfa1b-a695-4b7a-83c1-8434a219aad7",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "**Naive Bayes Implementation**:\n",
    "   - **Training**: Estimates priors $P(C)$ (e.g., $P(\\text{spam})$) and likelihoods $P(x_i|C)$ (e.g., $P(\\text{lottery}=1|\\text{spam})$) using frequency counts with Laplace smoothing to avoid zero probabilities:\n",
    "\n",
    "     $$\n",
    "     P(x_i|C) = \\frac{\\text{Count of } x_i \\text{ in class } C + 1}{\\text{Total instances in class } C + 2}\n",
    "     $$\n",
    "\n",
    "   - **Prediction**: Computes the log posterior for each class:\n",
    "\n",
    "     $$\n",
    "     \\hat{C} = \\arg\\max_C \\left[ \\log P(C) + \\sum_i \\log P(x_i|C) \\right]\n",
    "     $$\n",
    "\n",
    "     and selects the class with the highest value.\n",
    "\n",
    "3. **Test Emails**:\n",
    "   - Five test cases cover common spam (lottery, prince) and non-spam (LinkedIn, professional) scenarios.\n",
    "   - The output includes a human-readable description of each email’s features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18026209-826d-4b15-8d66-4e3a07e90165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Filter Detailed Calculation for Test Email:\n",
      "Email features: mentions lottery and mentions urgent and mentions congratulations\n",
      "\n",
      "Step-by-Step Calculation:\n",
      "\n",
      "Calculating for class 'spam':\n",
      "  log(P(spam)) = log(0.4667) = -0.7621\n",
      "  log(P(urgent=1|spam)) = log(0.5556) = -0.5878\n",
      "  log(P(lottery=1|spam)) = log(0.5556) = -0.5878\n",
      "  log(P(congratulations=1|spam)) = log(0.5556) = -0.5878\n",
      "  log(P(prince=0|spam)) = log(0.5556) = -0.5878\n",
      "  log(P(linkedin=0|spam)) = log(0.8889) = -0.1178\n",
      "  Total log posterior for spam: -3.2311\n",
      "\n",
      "Calculating for class 'no_spam':\n",
      "  log(P(no_spam)) = log(0.5333) = -0.6286\n",
      "  log(P(urgent=1|no_spam)) = log(0.2000) = -1.6094\n",
      "  log(P(lottery=1|no_spam)) = log(0.1000) = -2.3026\n",
      "  log(P(congratulations=1|no_spam)) = log(0.3000) = -1.2040\n",
      "  log(P(prince=0|no_spam)) = log(0.9000) = -0.1054\n",
      "  log(P(linkedin=0|no_spam)) = log(0.4000) = -0.9163\n",
      "  Total log posterior for no_spam: -6.7663\n",
      "\n",
      "Predicted class: 'spam' (highest log posterior: -3.2311)\n",
      "\n",
      "Final Classification: spam\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Sample dataset: List of (email_features, label) pairs\n",
    "# Features are dictionaries with key phrases (e.g., \"lottery\", \"prince\") and presence (1 for present, 0 for absent)\n",
    "# Labels: \"spam\" or \"no_spam\"\n",
    "data = [\n",
    "    ({\"lottery\": 1, \"prince\": 0, \"urgent\": 1, \"linkedin\": 0, \"congratulations\": 1}, \"spam\"),  # \"Earn 55 lakh lottery!\"\n",
    "    ({\"lottery\": 1, \"prince\": 0, \"urgent\": 0, \"linkedin\": 0, \"congratulations\": 1}, \"spam\"),  # \"You won a lottery!\"\n",
    "    ({\"lottery\": 0, \"prince\": 1, \"urgent\": 1, \"linkedin\": 0, \"congratulations\": 0}, \"spam\"),  # \"Nigerian prince needs help\"\n",
    "    ({\"lottery\": 0, \"prince\": 1, \"urgent\": 1, \"linkedin\": 0, \"congratulations\": 0}, \"spam\"),  # \"Prince urgent transfer\"\n",
    "    ({\"lottery\": 1, \"prince\": 0, \"urgent\": 1, \"linkedin\": 0, \"congratulations\": 1}, \"spam\"),  # \"Lottery urgent claim\"\n",
    "    ({\"lottery\": 0, \"prince\": 0, \"urgent\": 0, \"linkedin\": 1, \"congratulations\": 0}, \"no_spam\"),  # \"LinkedIn connection request\"\n",
    "    ({\"lottery\": 0, \"prince\": 0, \"urgent\": 0, \"linkedin\": 1, \"congratulations\": 0}, \"no_spam\"),  # \"LinkedIn message\"\n",
    "    ({\"lottery\": 0, \"prince\": 0, \"urgent\": 0, \"linkedin\": 1, \"congratulations\": 1}, \"no_spam\"),  # \"Congratulations on LinkedIn milestone\"\n",
    "    ({\"lottery\": 0, \"prince\": 0, \"urgent\": 0, \"linkedin\": 0, \"congratulations\": 0}, \"no_spam\"),  # \"Meeting reminder\"\n",
    "    ({\"lottery\": 0, \"prince\": 0, \"urgent\": 0, \"linkedin\": 1, \"congratulations\": 0}, \"no_spam\"),  # \"LinkedIn profile view\"\n",
    "    ({\"lottery\": 0, \"prince\": 0, \"urgent\": 1, \"linkedin\": 0, \"congratulations\": 0}, \"no_spam\"),  # \"Urgent team meeting\"\n",
    "    ({\"lottery\": 0, \"prince\": 0, \"urgent\": 0, \"linkedin\": 0, \"congratulations\": 1}, \"no_spam\"),  # \"Congratulations on promotion\"\n",
    "    ({\"lottery\": 1, \"prince\": 0, \"urgent\": 0, \"linkedin\": 0, \"congratulations\": 1}, \"spam\"),  # \"Lottery win, congratulations!\"\n",
    "    ({\"lottery\": 0, \"prince\": 1, \"urgent\": 0, \"linkedin\": 0, \"congratulations\": 0}, \"spam\"),  # \"Help from Nigerian prince\"\n",
    "    ({\"lottery\": 0, \"prince\": 0, \"urgent\": 0, \"linkedin\": 1, \"congratulations\": 0}, \"no_spam\")  # \"LinkedIn job alert\"\n",
    "]\n",
    "\n",
    "# Class to implement Naive Bayes for spam filtering\n",
    "class NaiveBayesSpamFilter:\n",
    "    def __init__(self):\n",
    "        self.priors = defaultdict(float)  # P(C)\n",
    "        self.likelihoods = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))  # P(x_i|C)\n",
    "        self.classes = set()\n",
    "        self.features = set()\n",
    "\n",
    "    def train(self, data):\n",
    "        # Count instances per class for priors\n",
    "        total_instances = len(data)\n",
    "        class_counts = defaultdict(int)\n",
    "        for features, label in data:\n",
    "            class_counts[label] += 1\n",
    "            self.classes.add(label)\n",
    "            self.features.update(features.keys())\n",
    "\n",
    "        # Calculate priors: P(C) = count(C) / total_instances\n",
    "        for label in class_counts:\n",
    "            self.priors[label] = class_counts[label] / total_instances\n",
    "\n",
    "        # Count feature occurrences per class for likelihoods\n",
    "        feature_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "        for features, label in data:\n",
    "            for feature, value in features.items():\n",
    "                feature_counts[label][feature][value] += 1\n",
    "\n",
    "        # Calculate likelihoods with Laplace smoothing: P(x_i|C) = (count(x_i, C) + 1) / (count(C) + K)\n",
    "        # K = number of possible values for feature (here, 2: 0 or 1)\n",
    "        for label in self.classes:\n",
    "            for feature in self.features:\n",
    "                for value in [0, 1]:  # Binary features\n",
    "                    self.likelihoods[label][feature][value] = (\n",
    "                        (feature_counts[label][feature][value] + 1) / \n",
    "                        (class_counts[label] + 2)\n",
    "                    )\n",
    "\n",
    "    def predict_with_details(self, features):\n",
    "        # Store calculation details\n",
    "        details = []\n",
    "        log_posteriors = {}\n",
    "\n",
    "        # Calculate log posterior for each class: log(P(C|X)) ∝ log(P(C)) + ∑ log(P(x_i|C))\n",
    "        for label in self.classes:\n",
    "            details.append(f\"\\nCalculating for class '{label}':\")\n",
    "            log_posterior = math.log(self.priors[label])\n",
    "            details.append(f\"  log(P({label})) = log({self.priors[label]:.4f}) = {log_posterior:.4f}\")\n",
    "\n",
    "            # Sum log likelihoods for each feature\n",
    "            for feature in self.features:\n",
    "                value = features.get(feature, 0)  # Assume 0 if feature missing\n",
    "                likelihood = self.likelihoods[label][feature][value]\n",
    "                log_likelihood = math.log(likelihood)\n",
    "                details.append(\n",
    "                    f\"  log(P({feature}={value}|{label})) = log({likelihood:.4f}) = {log_likelihood:.4f}\"\n",
    "                )\n",
    "                log_posterior += log_likelihood\n",
    "\n",
    "            log_posteriors[label] = log_posterior\n",
    "            details.append(f\"  Total log posterior for {label}: {log_posterior:.4f}\")\n",
    "\n",
    "        # Determine predicted class\n",
    "        predicted_class = max(log_posteriors, key=log_posteriors.get)\n",
    "        details.append(f\"\\nPredicted class: '{predicted_class}' (highest log posterior: {log_posteriors[predicted_class]:.4f})\")\n",
    "\n",
    "        return predicted_class, details\n",
    "\n",
    "# Train the model\n",
    "spam_filter = NaiveBayesSpamFilter()\n",
    "spam_filter.train(data)\n",
    "\n",
    "# Test email: \"Earn 55 lakh lottery, urgent!\"\n",
    "test_email = {\"lottery\": 1, \"prince\": 0, \"urgent\": 1, \"linkedin\": 0, \"congratulations\": 1}\n",
    "\n",
    "# Predict with detailed calculations\n",
    "print(\"Spam Filter Detailed Calculation for Test Email:\")\n",
    "# Create description of email\n",
    "description = []\n",
    "for feature, value in test_email.items():\n",
    "    if value == 1:\n",
    "        description.append(f\"mentions {feature}\")\n",
    "description = \" and \".join(description) if description else \"no key phrases\"\n",
    "print(f\"Email features: {description}\")\n",
    "\n",
    "# Get prediction and details\n",
    "prediction, calculation_details = spam_filter.predict_with_details(test_email)\n",
    "\n",
    "# Print full calculation\n",
    "print(\"\\nStep-by-Step Calculation:\")\n",
    "for line in calculation_details:\n",
    "    print(line)\n",
    "print(f\"\\nFinal Classification: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92673404-6330-4b74-adc2-0e25ead59afb",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Frequentist vs. Bayesian Probability\n",
    "\n",
    "Probability can be interpreted in two primary ways: **Frequentist** and **Bayesian**. These approaches differ in how they define probability and handle uncertainty, which impacts their application in machine learning, including Naive Bayes.\n",
    "\n",
    "### 1.1. Frequentist Probability\n",
    "The Frequentist approach views probability as the long-run frequency of an event occurring in repeated trials.\n",
    "\n",
    "- **Definition**: Probability of an event $A$, denoted $P(A)$, is the limit of the relative frequency of $A$ as the number of trials $n$ approaches infinity:\n",
    "\n",
    "$$\n",
    "P(A) = \\lim_{n \\to \\infty} \\frac{\\text{Number of times } A \\text{ occurs}}{n}\n",
    "$$\n",
    "\n",
    "- **Key Characteristics**:\n",
    "  - Parameters (e.g., mean, probability) are fixed but unknown constants.\n",
    "  - Inference relies on sampling and point estimates (e.g., maximum likelihood estimation).\n",
    "  - Confidence intervals describe the range where the true parameter lies with a certain probability (e.g., 95% confidence).\n",
    "  - No incorporation of prior knowledge beyond the data.\n",
    "\n",
    "- **Example**: If you flip a coin 1000 times and get 510 heads, the Frequentist estimate of the probability of heads is:\n",
    "\n",
    "$$\n",
    "P(\\text{Heads}) \\approx \\frac{510}{1000} = 0.51\n",
    "$$\n",
    "\n",
    "- **In Machine Learning**: Frequentist methods estimate model parameters (e.g., weights in logistic regression) using maximum likelihood, assuming the data is a random sample from a fixed distribution.\n",
    "\n",
    "- **Limitations**:\n",
    "  - Requires large sample sizes for reliable estimates.\n",
    "  - Does not naturally incorporate prior knowledge or uncertainty about parameters.\n",
    "\n",
    "### 1.2. Bayesian Probability\n",
    "The Bayesian approach treats probability as a measure of belief or uncertainty about an event, updated with new evidence.\n",
    "\n",
    "- **Definition**: Probability $P(A)$ represents the degree of belief in event $A$, quantified using Bayes' Theorem:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "- **Key Characteristics**:\n",
    "  - Parameters are treated as random variables with probability distributions.\n",
    "  - Prior beliefs about parameters ($P(A)$) are updated with observed data ($P(B|A)$) to form the posterior distribution ($P(A|B)$).\n",
    "  - Inference involves computing the full posterior distribution or summarizing it (e.g., mean, mode).\n",
    "  - Naturally incorporates prior knowledge via the prior distribution.\n",
    "\n",
    "- **Example**: Suppose you believe a coin is fair (prior: $P(\\theta = 0.5) = 0.9$, where $\\theta$ is the probability of heads) but allow for bias (e.g., a Beta distribution prior). After observing 510 heads in 1000 flips, you update the prior using the likelihood to get a posterior distribution for $\\theta$, which might center around 0.51 but reflect uncertainty.\n",
    "\n",
    "- **In Machine Learning**: Bayesian methods model uncertainty in parameters (e.g., Bayesian linear regression) and are used in algorithms like Naive Bayes, which relies on Bayes' Theorem to compute posterior probabilities.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Incorporates prior knowledge, useful for small datasets.\n",
    "  - Provides full uncertainty quantification via the posterior.\n",
    "- **Limitations**:\n",
    "  - Computationally intensive (e.g., integrating over posterior distributions).\n",
    "  - Choice of prior can be subjective.\n",
    "\n",
    "## 2. Generative vs. Discriminative Models\n",
    "\n",
    "Machine learning models can be categorized as **generative** or **discriminative** based on what they model and how they approach classification. Naive Bayes is a generative model.\n",
    "\n",
    "### 2.1. Generative Models\n",
    "Generative models learn the joint probability distribution $P(X, C)$ of the features $X$ and class $C$, allowing them to generate new data similar to the training set.\n",
    "\n",
    "- **Definition**: A generative model models:\n",
    "\n",
    "$$\n",
    "P(X, C) = P(X|C) \\cdot P(C)\n",
    "$$\n",
    "\n",
    "For classification, it uses Bayes' Theorem to compute the posterior:\n",
    "\n",
    "$$\n",
    "P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\n",
    "$$\n",
    "\n",
    "- **Key Characteristics**:\n",
    "  - Models how the data is generated (i.e., the distribution of $X$ for each class $C$).\n",
    "  - Can generate synthetic data by sampling from $P(X|C)$.\n",
    "  - Requires estimating both $P(X|C)$ (likelihood) and $P(C)$ (prior).\n",
    "  - Often more robust to missing data or small datasets because it models the full joint distribution.\n",
    "\n",
    "- **Examples**:\n",
    "  - **Naive Bayes**: Assumes features are conditionally independent given the class, modeling $P(X|C) = \\prod_i P(x_i|C)$.\n",
    "  - Gaussian Mixture Models (GMMs).\n",
    "  - Hidden Markov Models (HMMs).\n",
    "\n",
    "- **In Naive Bayes**: For a data point $X = \\{x_1, x_2, \\dots, x_n\\}$, Naive Bayes models:\n",
    "\n",
    "$$\n",
    "P(X|C) = \\prod_{i=1}^n P(x_i|C)\n",
    "$$\n",
    "\n",
    "and uses the prior $P(C)$ to compute $P(C|X)$. It can generate new data by sampling feature values from $P(x_i|C)$ for a given class.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Can handle missing features by marginalizing over them.\n",
    "  - Useful for tasks beyond classification (e.g., data generation).\n",
    "  - Works well with small datasets if the generative assumptions hold.\n",
    "- **Limitations**:\n",
    "  - Requires strong assumptions (e.g., feature independence in Naive Bayes).\n",
    "  - May not focus directly on the decision boundary, potentially leading to suboptimal classification performance.\n",
    "\n",
    "### 2.2. Discriminative Models\n",
    "Discriminative models learn the conditional probability $P(C|X)$ or directly model the decision boundary between classes, focusing on classification.\n",
    "\n",
    "- **Definition**: A discriminative model directly models:\n",
    "\n",
    "$$\n",
    "P(C|X)\n",
    "$$\n",
    "\n",
    "or learns a mapping from $X$ to $C$ without modeling the data distribution.\n",
    "\n",
    "- **Key Characteristics**:\n",
    "  - Focuses on distinguishing classes rather than modeling how data is generated.\n",
    "  - Often simpler to train for classification tasks since it avoids modeling $P(X)$.\n",
    "  - Typically better at classification accuracy for large datasets.\n",
    "\n",
    "- **Examples**:\n",
    "  - Logistic Regression: Models $P(C|X)$ using a logistic function.\n",
    "  - Support Vector Machines (SVMs): Learns the decision boundary directly.\n",
    "  - Neural Networks: Often used as discriminative models for classification.\n",
    "\n",
    "- **In Context**: Unlike Naive Bayes, logistic regression directly estimates:\n",
    "\n",
    "$$\n",
    "P(C|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}}\n",
    "$$\n",
    "\n",
    "without modeling $P(X|C)$ or $P(C)$.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Often more accurate for classification, especially with large datasets.\n",
    "  - Less sensitive to incorrect assumptions about data distribution.\n",
    "- **Limitations**:\n",
    "  - Cannot generate data or handle missing features as naturally.\n",
    "  - May require more data to achieve good performance.\n",
    "\n",
    "## 3. Parametric vs. Non-Parametric Models\n",
    "\n",
    "#### **1. Parametric Models**  \n",
    "- Assume a fixed functional form with **finite parameters** $ \\theta $, independent of data size $ n $.  \n",
    "- **Model**: $ p(y|x, \\theta) $, where $ \\theta \\in \\mathbb{R}^d $ (e.g., $ d = \\text{dim}(\\theta) $).  \n",
    "\n",
    "#### **2. Non-Parametric Models**  \n",
    "- **No fixed form**; model complexity grows with $ n $.  \n",
    "- **Model**: Relies directly on data (e.g., kernel methods, distances).  \n",
    "- **Example (k-Nearest Neighbors)**:  \n",
    "  \\[\n",
    "  \\hat{y}(x) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x)} y_i, \\quad \\mathcal{N}_k(x) = \\text{k closest points to } x\n",
    "  \\]  \n",
    "- **Learning**: Stores/adapts to data (e.g., kernel density estimation: $ p(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i) $).  \n",
    "\n",
    "#### **Key Difference**  \n",
    "- **Parametric**: Fixed $ \\dim(\\theta) $ (e.g., $ O(d) $).  \n",
    "- **Non-Parametric**: Grows with $ n $ (e.g., $ O(n) $ for kNN).  \n",
    "\n",
    "**Trade-off**: Bias (parametric) vs. Variance (non-parametric)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a948b8ab-dd59-44a8-88a0-07f50dacb9c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a94a9-eb89-44e8-bedf-295694f5e447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aad5d118-be43-4ecc-bf05-ce3788fa50cf",
   "metadata": {},
   "source": [
    "# Turning Text into Numbers for Machine Learning with Naive Bayes\n",
    "\n",
    "## Turning Text into Usable Data\n",
    "\n",
    "Each piece of data, called $ x $, is a string of text (like an email or message) that can be any length. To use this text with machine learning, we need to convert it into a list of numbers, called a $ d $-dimensional feature vector $ \\phi(x) $, where $ d $ is the number of features (like characteristics we measure).\n",
    "\n",
    "### Ways to Convert Text to Numbers\n",
    "\n",
    "1. **Custom Features**:\n",
    "   - Look at the text and create features based on what you know about it.\n",
    "   - Examples:\n",
    "     - Does the text have the word \"church\"? ($ x_j = 1 $ if yes, $ 0 $ if no)\n",
    "     - Is the email sent from outside the U.S.? ($ x_j = 1 $ if yes, $ 0 $ if no)\n",
    "     - Is the sender a university? ($ x_j = 1 $ if yes, $ 0 $ if no)\n",
    "\n",
    "2. **Word Presence Features**:\n",
    "   - Make a list of words (called a vocabulary) and check if each word is in the text.\n",
    "   - For words like \"Aardvark\", \"Apple\", ..., \"Zebra\":\n",
    "     - Set $ x_j = 1 $ if the word is in the text, $ 0 $ if it’s not.\n",
    "\n",
    "3. **Deep Learning Methods**:\n",
    "   - Advanced machine learning techniques can work directly with raw text, using tools like neural networks that understand sequences of letters or words.\n",
    "\n",
    "### Bag of Words Model\n",
    "\n",
    "A common way to represent text is the **bag of words** model, which treats text like a bag of words, ignoring their order.\n",
    "\n",
    "- **Vocabulary**:\n",
    "  - Create a list of words to track, called the vocabulary $ V $:\n",
    "    $$ V = \\{\\text{church}, \\text{doctor}, \\text{fervently}, \\text{purple}, \\text{slow}, \\dots\\} $$\n",
    "\n",
    "- **Feature Vector**:\n",
    "  - Turn a text $ x $ into a list of 1s and 0s, with one number for each word in $ V $. This list is $ \\phi(x) $, and its length is the number of words in $ V $, written $ |V| $:\n",
    "    $$\n",
    "    \\phi(x) = \\begin{pmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    \\vdots \\\\\n",
    "    1 \\\\\n",
    "    \\vdots\n",
    "    \\end{pmatrix}\n",
    "    \\begin{array}{l}\n",
    "    \\text{church} \\\\\n",
    "    \\text{doctor} \\\\\n",
    "    \\text{fervently} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\text{purple} \\\\\n",
    "    \\vdots\n",
    "    \\end{array}\n",
    "    $$\n",
    "  - Each number $ \\phi(x)_j $ is $ 1 $ if the $ j $-th word in $ V $ is in the text, or $ 0 $ if it’s not. For example, if \"doctor\" is in the text, the second number is $ 1 $.\n",
    "\n",
    "## Building a Model to Classify Text\n",
    "\n",
    "For binary classification (e.g., spam vs. not spam), we create two models using a dataset with labeled examples (texts marked as spam or not):\n",
    "\\begin{align*}\n",
    "P_\\theta(x|y=0) && \\text{and} && P_\\theta(x|y=1)\n",
    "\\end{align*}\n",
    "- $ P_\\theta(x|y=0) $ tells us how likely a text $ x $ is to be not spam ($ y=0 $).\n",
    "- $ P_\\theta(x|y=1) $ tells us how likely it is to be spam ($ y=1 $).\n",
    "- The $ \\theta $ means these probabilities depend on parameters we’ll learn, and we use the bag-of-words representation for $ x $.\n",
    "\n",
    "### What is a Categorical Distribution?\n",
    "\n",
    "A **Categorical** distribution is like rolling a die with $ K $ sides, where each side has a probability:\n",
    "$$\n",
    "P_\\theta(x = j) = \\theta_j\n",
    "$$\n",
    "- $ x $ can be one of $ K $ outcomes (like 1, 2, ..., $ K $).\n",
    "- $ \\theta_j $ is the probability of outcome $ j $.\n",
    "- When $ K=2 $ (e.g., yes/no), it’s called a **Bernoulli** distribution, like flipping a coin.\n",
    "\n",
    "### Naive Bayes Simplification\n",
    "\n",
    "Text data can have many words (e.g., 10,000), making it hard to calculate probabilities for every possible combination of words. The **Naive Bayes** assumption simplifies this by assuming each word’s presence is independent of the others:\n",
    "$$\n",
    "P_\\theta(x = x' | y) = \\prod_{j=1}^d P_\\theta(x_j = x_j' | y)\n",
    "$$\n",
    "- $ x $ is the text’s feature vector, and $ x' $ is a specific vector (e.g., $ [0, 1, 0, \\dots] $).\n",
    "- $ P_\\theta(x = x' | y) $ is the probability that a text has the exact word pattern $ x' $ given its class $ y $.\n",
    "- The product $ \\prod $ means we multiply the probabilities for each word’s presence or absence.\n",
    "\n",
    "For example, if a text has:\n",
    "$$\n",
    "x = \\left( \\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{array} \\right) \\begin{array}{l}\n",
    "\\text{church} \\\\\n",
    "\\text{doctor} \\\\\n",
    "\\text{fervently} \\\\\n",
    "\\vdots \\\\\n",
    "\\text{purple}\n",
    "\\end{array}\n",
    "$$\n",
    "The probability it belongs to class $ y $ (e.g., spam) is:\n",
    "$$\n",
    "P_\\theta \\left( x = \\left( \\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{array} \\right) \\middle| y \\right) = P_\\theta(x_1=0|y) \\cdot P_\\theta(x_2=1|y) \\cdot \\dots \\cdot P_\\theta(x_d=0|y)\n",
    "$$\n",
    "- $ P_\\theta(x_1=0|y) $ is the chance \"church\" is absent given class $ y $.\n",
    "- $ P_\\theta(x_2=1|y) $ is the chance \"doctor\" is present, and so on.\n",
    "\n",
    "#### Parameters in Naive Bayes\n",
    "- Each word’s probability $ P_\\theta(x_j | y=k) $ is a Bernoulli distribution with a parameter $ \\psi_{jk} $:\n",
    "  $$\n",
    "  P_\\theta(x_j = 1 | y=k) = \\psi_{jk}, \\quad P_\\theta(x_j = 0 | y=k) = 1 - \\psi_{jk}\n",
    "  $$\n",
    "  - $ \\psi_{jk} $ is the probability that word $ j $ appears in class $ k $.\n",
    "- If we have $ K $ classes (e.g., spam and not spam, so $ K=2 $) and $ d $ words, we need $ Kd $ parameters, which is much simpler than calculating every possible text combination.\n",
    "\n",
    "#### Naive Bayes for Bag of Words\n",
    "- The chance a text $ x $ has a specific word pattern $ x' $ in class $ k $:\n",
    "  $$\n",
    "  P_\\theta(x = x' | y=k) = \\prod_{j=1}^d P_\\theta(x_j = x_j' | y=k)\n",
    "  $$\n",
    "- Each word’s presence is a Bernoulli:\n",
    "  $$\n",
    "  P_\\theta(x_j = 1 | y=k) = \\psi_{jk}, \\quad P_\\theta(x_j = 0 | y=k) = 1 - \\psi_{jk}\n",
    "  $$\n",
    "- We need $ Kd $ parameters, where $ \\psi_{jk} $ is the chance word $ j $ is in a text of class $ k $.\n",
    "\n",
    "#### Does Naive Bayes Work Well?\n",
    "- **Problem**: It assumes words don’t affect each other, but in real life, words like \"bank\" and \"account\" often appear together in spam.\n",
    "- **Effect**: This can make the model’s probabilities too extreme (over- or under-confident).\n",
    "- **Benefit**: Even with this flaw, Naive Bayes often classifies texts accurately, making it a practical choice.\n",
    "\n",
    "### Setting Up Class Probabilities\n",
    "We need a starting guess for how likely each class is, called the prior $ P_\\theta(y=k) $:\n",
    "- Use a Categorical distribution with parameters $ \\vec{\\phi} = (\\phi_1, \\dots, \\phi_K) $:\n",
    "  $$\n",
    "  P_\\theta(y=k) = \\phi_k\n",
    "  $$\n",
    "- We can learn $ \\phi_k $ from the data, like counting how many texts are spam vs. not spam.\n",
    "\n",
    "### Bernoulli Naive Bayes Model\n",
    "The **Bernoulli Naive Bayes** model works with binary data $ x \\in \\{0,1\\}^d $ (e.g., bag-of-words where each word is present or absent):\n",
    "- **Parameters**: $ \\theta = (\\phi_1, \\dots, \\phi_K, \\psi_{11}, \\dots, \\psi_{dK}) $, totaling $ K(d+1) $ parameters.\n",
    "- **Class Prior**:\n",
    "  $$\n",
    "  P_\\theta(y) = \\text{Categorical}(\\phi_1, \\phi_2, \\dots, \\phi_K)\n",
    "  $$\n",
    "- **Word Probabilities**:\n",
    "  $$\n",
    "  P_\\theta(x_j = 1 | y=k) = \\text{Bernoulli}(\\psi_{jk})\n",
    "  $$\n",
    "  $$\n",
    "  P_\\theta(x | y=k) = \\prod_{j=1}^d P_\\theta(x_j | y=k)\n",
    "  $$\n",
    "\n",
    "## Learning the Model’s Parameters\n",
    "\n",
    "### Learning Class Probabilities $ \\phi_k $\n",
    "- Suppose we have $ n $ texts, and $ n_k $ of them belong to class $ k $ (e.g., $ n_k $ spam emails).\n",
    "- The best estimate for $ \\phi_k $ is:\n",
    "  $$\n",
    "  \\phi_k = \\frac{n_k}{n}\n",
    "  $$\n",
    "- This is just the fraction of texts in class $ k $. For example, if 30 out of 100 emails are spam, $ \\phi_{\\text{spam}} = 0.3 $.\n",
    "\n",
    "### Learning Word Probabilities $ \\psi_{jk} $\n",
    "- For each class $ k $ and word $ j $, $ \\psi_{jk} $ is the chance that word $ j $ appears in texts of class $ k $:\n",
    "  $$\n",
    "  \\psi_{jk} = \\frac{n_{jk}}{n_k}\n",
    "  $$\n",
    "- $ n_{jk} $ is the number of texts in class $ k $ that contain word $ j $.\n",
    "- Example: If 20 out of 50 spam emails contain \"doctor\", then $ \\psi_{\\text{doctor,spam}} = \\frac{20}{50} = 0.4 $.\n",
    "\n",
    "### Making Predictions\n",
    "- To classify a new text, use Bayes’ rule to find the most likely class:\n",
    "  $$\n",
    "  \\arg\\max_y P_\\theta(y|x) = \\arg\\max_y P_\\theta(x|y)P_\\theta(y)\n",
    "  $$\n",
    "- Calculate $ P_\\theta(x|y=k)P_\\theta(y=k) $ for each class $ k $ (e.g., spam and not spam), and pick the class with the highest value.\n",
    "- Think of it like scoring how \"spam-like\" or \"not-spam-like\" the text is, then choosing the best match.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90584355-87ba-47bf-86a4-7d567a120be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d15555-ee36-42dd-947e-5eee911acb81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d3b3bbf-a2ff-4315-8061-8f9da98bf386",
   "metadata": {},
   "source": [
    "### Example Scenario\n",
    "Suppose we have a small dataset of emails, and we want to classify them into three categories: Spam ($ y = 1 $), Work ($ y = 2 $), or Personal ($ y = 3 $). Each email is represented as a bag-of-words feature vector based on a small vocabulary of four words: \"deal,\" \"meeting,\" \"friend,\" and \"family\" ($ d = 4 $). We’ll use the Bernoulli Naive Bayes model to:\n",
    "1. Represent the emails as feature vectors.\n",
    "2. Estimate the model parameters ($ \\phi_k $ and $ \\psi_{jk} $).\n",
    "3. Predict the class of a new email.\n",
    "\n",
    "\n",
    "\n",
    "# Example: Classifying Emails with Naive Bayes (Three Categories)\n",
    "\n",
    "This example applies the Naive Bayes model from the provided text to classify emails into three categories: Spam, Work, or Personal, using a small vocabulary. We’ll use the same math and concepts, showing how they work with $ K = 3 $ classes.\n",
    "\n",
    "## Step 1: Representing Emails as Feature Vectors\n",
    "\n",
    "Each email $ x $ is a sequence of words. We convert it into a $ d $-dimensional feature vector $ \\phi(x) $ using the **bag of words** model.\n",
    "\n",
    "### Vocabulary\n",
    "Define a vocabulary $ V $ with $ d = 4 $ words:\n",
    "$$\n",
    "V = \\{\\text{deal}, \\text{meeting}, \\text{friend}, \\text{family}\\}\n",
    "$$\n",
    "\n",
    "### Feature Vector\n",
    "For an email $ x $, we create a binary vector $ \\phi(x) \\in \\{0,1\\}^4 $:\n",
    "$$\n",
    "\\phi(x) = \\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4\n",
    "\\end{pmatrix}\n",
    "\\begin{array}{l}\n",
    "\\text{deal} \\\\\n",
    "\\text{meeting} \\\\\n",
    "\\text{friend} \\\\\n",
    "\\text{family}\n",
    "\\end{array}\n",
    "$$\n",
    "- $ x_j = 1 $ if word $ j $ (e.g., \"deal\") is in the email, else $ x_j = 0 $.\n",
    "- Example: If an email contains \"meeting\" and \"friend\" but not \"deal\" or \"family,\" its feature vector is:\n",
    "  $$\n",
    "  \\phi(x) = \\begin{pmatrix}\n",
    "  0 \\\\\n",
    "  1 \\\\\n",
    "  1 \\\\\n",
    "  0\n",
    "  \\end{pmatrix}\n",
    "  $$\n",
    "\n",
    "## Step 2: Dataset\n",
    "Suppose we have a small training dataset with $ n = 10 $ emails, labeled as Spam ($ y = 1 $), Work ($ y = 2 $), or Personal ($ y = 3 $):\n",
    "\n",
    "| Email ID | Words Present              | Class ($ y $) |\n",
    "|----------|----------------------------|-----------------|\n",
    "| 1        | deal, friend              | Spam (1)        |\n",
    "| 2        | deal                      | Spam (1)        |\n",
    "| 3        | meeting, friend           | Work (2)        |\n",
    "| 4        | meeting                   | Work (2)        |\n",
    "| 5        | meeting, family           | Work (2)        |\n",
    "| 6        | friend, family            | Personal (3)    |\n",
    "| 7        | family                    | Personal (3)    |\n",
    "| 8        | friend                    | Personal (3)    |\n",
    "| 9        | deal, meeting             | Spam (1)        |\n",
    "| 10       | friend, family            | Personal (3)    |\n",
    "\n",
    "### Feature Vectors\n",
    "Each email is converted to a 4D binary vector:\n",
    "- Email 1: $ x^{(1)} = [1, 0, 1, 0] $ (deal, friend)\n",
    "- Email 2: $ x^{(2)} = [1, 0, 0, 0] $ (deal)\n",
    "- Email 3: $ x^{(3)} = [0, 1, 1, 0] $ (meeting, friend)\n",
    "- ...\n",
    "- Email 10: $ x^{(10)} = [0, 0, 1, 1] $ (friend, family)\n",
    "\n",
    "## Step 3: Bernoulli Naive Bayes Model\n",
    "\n",
    "We use the **Bernoulli Naive Bayes** model for binary data $ x \\in \\{0,1\\}^4 $, with three classes ($ K = 3 $).\n",
    "\n",
    "### Model Components\n",
    "- **Parameters**: $ \\theta = (\\phi_1, \\phi_2, \\phi_3, \\psi_{11}, \\psi_{21}, \\dots, \\psi_{43}) $, with $ K(d+1) = 3(4+1) = 15 $ parameters.\n",
    "- **Class Prior**:\n",
    "  $$\n",
    "  P_\\theta(y) = \\text{Categorical}(\\phi_1, \\phi_2, \\phi_3)\n",
    "  $$\n",
    "  - $ \\phi_k $ is the probability of class $ k $.\n",
    "- **Feature Likelihood**:\n",
    "  $$\n",
    "  P_\\theta(x_j = 1 | y=k) = \\text{Bernoulli}(\\psi_{jk})\n",
    "  $$\n",
    "  $$\n",
    "  P_\\theta(x | y=k) = \\prod_{j=1}^4 P_\\theta(x_j | y=k)\n",
    "  $$\n",
    "  - $ \\psi_{jk} $ is the probability that word $ j $ is present in class $ k $.\n",
    "  - $ P_\\theta(x_j = 0 | y=k) = 1 - \\psi_{jk} $.\n",
    "\n",
    "### Naive Bayes Assumption\n",
    "We assume words are independent given the class:\n",
    "$$\n",
    "P_\\theta(x = x' | y=k) = \\prod_{j=1}^4 P_\\theta(x_j = x_j' | y=k)\n",
    "$$\n",
    "For a vector $ x' = [0, 1, 1, 0] $:\n",
    "$$\n",
    "P_\\theta \\left( x = \\begin{pmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{pmatrix} \\middle| y=k \\right) = P_\\theta(x_1=0|y=k) \\cdot P_\\theta(x_2=1|y=k) \\cdot P_\\theta(x_3=1|y=k) \\cdot P_\\theta(x_4=0|y=k)\n",
    "$$\n",
    "\n",
    "## Step 4: Learning Parameters\n",
    "\n",
    "### Learning Class Priors $ \\phi_k $\n",
    "Count the number of emails in each class:\n",
    "- Spam ($ y = 1 $): $ n_1 = 3 $ (Emails 1, 2, 9)\n",
    "- Work ($ y = 2 $): $ n_2 = 3 $ (Emails 3, 4, 5)\n",
    "- Personal ($ y = 3 $): $ n_3 = 4 $ (Emails 6, 7, 8, 10)\n",
    "- Total emails: $ n = 10 $\n",
    "\n",
    "The prior probabilities are:\n",
    "$$\n",
    "\\phi_k = \\frac{n_k}{n}\n",
    "$$\n",
    "- $ \\phi_1 = \\frac{3}{10} = 0.3 $ (Spam)\n",
    "- $ \\phi_2 = \\frac{3}{10} = 0.3 $ (Work)\n",
    "- $ \\phi_3 = \\frac{4}{10} = 0.4 $ (Personal)\n",
    "\n",
    "### Learning Feature Parameters $ \\psi_{jk} $\n",
    "For each class $ k $ and word $ j $, compute:\n",
    "$$\n",
    "\\psi_{jk} = \\frac{n_{jk}}{n_k}\n",
    "$$\n",
    "- $ n_{jk} $ is the number of emails in class $ k $ where word $ j $ is present.\n",
    "- $ n_k $ is the number of emails in class $ k $.\n",
    "\n",
    "#### Spam ($ k = 1 $, $ n_1 = 3 $)\n",
    "- Word 1 (deal): Present in Emails 1, 2, 9 ($ n_{11} = 3 $)\n",
    "  $$\n",
    "  \\psi_{11} = \\frac{3}{3} = 1.0\n",
    "  $$\n",
    "- Word 2 (meeting): Present in Email 9 ($ n_{21} = 1 $)\n",
    "  $$\n",
    "  \\psi_{21} = \\frac{1}{3} \\approx 0.333\n",
    "  $$\n",
    "- Word 3 (friend): Present in Email 1 ($ n_{31} = 1 $)\n",
    "  $$\n",
    "  \\psi_{31} = \\frac{1}{3} \\approx 0.333\n",
    "  $$\n",
    "- Word 4 (family): Absent ($ n_{41} = 0 $)\n",
    "  $$\n",
    "  \\psi_{41} = \\frac{0}{3} = 0.0\n",
    "  $$\n",
    "\n",
    "#### Work ($ k = 2 $, $ n_2 = 3 $)\n",
    "- Word 1 (deal): Absent ($ n_{12} = 0 $)\n",
    "  $$\n",
    "  \\psi_{12} = \\frac{0}{3} = 0.0\n",
    "  $$\n",
    "- Word 2 (meeting): Present in Emails 3, 4, 5 ($ n_{22} = 3 $)\n",
    "  $$\n",
    "  \\psi_{22} = \\frac{3}{3} = 1.0\n",
    "  $$\n",
    "- Word 3 (friend): Present in Email 3 ($ n_{32} = 1 $)\n",
    "  $$\n",
    "  \\psi_{32} = \\frac{1}{3} \\approx 0.333\n",
    "  $$\n",
    "- Word 4 (family): Present in Email 5 ($ n_{42} = 1 $)\n",
    "  $$\n",
    "  \\psi_{42} = \\frac{1}{3} \\approx 0.333\n",
    "  $$\n",
    "\n",
    "#### Personal ($ k = 3 $, $ n_3 = 4 $)\n",
    "- Word 1 (deal): Absent ($ n_{13} = 0 $)\n",
    "  $$\n",
    "  \\psi_{13} = \\frac{0}{4} = 0.0\n",
    "  $$\n",
    "- Word 2 (meeting): Absent ($ n_{23} = 0 $)\n",
    "  $$\n",
    "  \\psi_{23} = \\frac{0}{4} = 0.0\n",
    "  $$\n",
    "- Word 3 (friend): Present in Emails 6, 8, 10 ($ n_{33} = 3 $)\n",
    "  $$\n",
    "  \\psi_{33} = \\frac{3}{4} = 0.75\n",
    "  $$\n",
    "- Word 4 (family): Present in Emails 6, 7, 10 ($ n_{43} = 3 $)\n",
    "  $$\n",
    "  \\psi_{43} = \\frac{3}{4} = 0.75\n",
    "  $$\n",
    "\n",
    "### Parameter Summary\n",
    "- Priors: $ \\phi_1 = 0.3 $, $ \\phi_2 = 0.3 $, $ \\phi_3 = 0.4 $\n",
    "- Feature probabilities:\n",
    "  | Word $ j $ | Spam ($ \\psi_{j1} $) | Work ($ \\psi_{j2} $) | Personal ($ \\psi_{j3} $) |\n",
    "  |-------------|-----------------------|-----------------------|-------------------------|\n",
    "  | deal        | 1.0                   | 0.0                   | 0.0                     |\n",
    "  | meeting     | 0.333                 | 1.0                   | 0.0                     |\n",
    "  | friend      | 0.333                 | 0.333                 | 0.75                    |\n",
    "  | family      | 0.0                   | 0.333                 | 0.75                    |\n",
    "\n",
    "## Step 5: Predicting a New Email\n",
    "Suppose a new email contains \"friend\" and \"family\" ($ x' = [0, 0, 1, 1] $). We predict its class using Bayes’ rule:\n",
    "$$\n",
    "\\arg\\max_y P_\\theta(y|x) = \\arg\\max_y P_\\theta(x|y)P_\\theta(y)\n",
    "$$\n",
    "\n",
    "Compute $ P_\\theta(x = x' | y=k)P_\\theta(y=k) $ for each class $ k $.\n",
    "\n",
    "### Spam ($ k = 1 $)\n",
    "$$\n",
    "P_\\theta(x = [0, 0, 1, 1] | y=1) = P_\\theta(x_1=0|y=1) \\cdot P_\\theta(x_2=0|y=1) \\cdot P_\\theta(x_3=1|y=1) \\cdot P_\\theta(x_4=1|y=1)\n",
    "$$\n",
    "- $ P_\\theta(x_1=0|y=1) = 1 - \\psi_{11} = 1 - 1.0 = 0.0 $\n",
    "- $ P_\\theta(x_2=0|y=1) = 1 - \\psi_{21} = 1 - 0.333 = 0.667 $\n",
    "- $ P_\\theta(x_3=1|y=1) = \\psi_{31} = 0.333 $\n",
    "- $ P_\\theta(x_4=1|y=1) = \\psi_{41} = 0.0 $\n",
    "\n",
    "Since $ P_\\theta(x_1=0|y=1) = 0.0 $, the product is:\n",
    "$$\n",
    "P_\\theta(x | y=1) = 0.0 \\cdot 0.667 \\cdot 0.333 \\cdot 0.0 = 0.0\n",
    "$$\n",
    "$$\n",
    "P_\\theta(x | y=1)P_\\theta(y=1) = 0.0 \\cdot 0.3 = 0.0\n",
    "$$\n",
    "\n",
    "### Work ($ k = 2 $)\n",
    "$$\n",
    "P_\\theta(x = [0, 0, 1, 1] | y=2) = P_\\theta(x_1=0|y=2) \\cdot P_\\theta(x_2=0|y=2) \\cdot P_\\theta(x_3=1|y=2) \\cdot P_\\theta(x_4=1|y=2)\n",
    "$$\n",
    "- $ P_\\theta(x_1=0|y=2) = 1 - \\psi_{12} = 1 - 0.0 = 1.0 $\n",
    "- $ P_\\theta(x_2=0|y=2) = 1 - \\psi_{22} = 1 - 1.0 = 0.0 $\n",
    "- $ P_\\theta(x_3=1|y=2) = \\psi_{32} = 0.333 $\n",
    "- $ P_\\theta(x_4=1|y=2) = \\psi_{42} = 0.333 $\n",
    "\n",
    "Since $ P_\\theta(x_2=0|y=2) = 0.0 $:\n",
    "$$\n",
    "P_\\theta(x | y=2) = 1.0 \\cdot 0.0 \\cdot 0.333 \\cdot 0.333 = 0.0\n",
    "$$\n",
    "$$\n",
    "P_\\theta(x | y=2)P_\\theta(y=2) = 0.0 \\cdot 0.3 = 0.0\n",
    "$$\n",
    "\n",
    "### Personal ($ k = 3 $)\n",
    "$$\n",
    "P_\\theta(x = [0, 0, 1, 1] | y=3) = P_\\theta(x_1=0|y=3) \\cdot P_\\theta(x_2=0|y=3) \\cdot P_\\theta(x_3=1|y=3) \\cdot P_\\theta(x_4=1|y=3)\n",
    "$$\n",
    "- $ P_\\theta(x_1=0|y=3) = 1 - \\psi_{13} = 1 - 0.0 = 1.0 $\n",
    "- $ P_\\theta(x_2=0|y=3) = 1 - \\psi_{23} = 1 - 0.0 = 1.0 $\n",
    "- $ P_\\theta(x_3=1|y=3) = \\psi_{33} = 0.75 $\n",
    "- $ P_\\theta(x_4=1|y=3) = \\psi_{43} = 0.75 $\n",
    "\n",
    "$$\n",
    "P_\\theta(x | y=3) = 1.0 \\cdot 1.0 \\cdot 0.75 \\cdot 0.75 = 0.5625\n",
    "$$\n",
    "$$\n",
    "P_\\theta(x | y=3)P_\\theta(y=3) = 0.5625 \\cdot 0.4 = 0.225\n",
    "$$\n",
    "\n",
    "### Prediction\n",
    "Compare the scores:\n",
    "- Spam: $ 0.0 $\n",
    "- Work: $ 0.0 $\n",
    "- Personal: $ 0.225 $\n",
    "\n",
    "The highest score is for Personal ($ y = 3 $), so we predict the email is **Personal**.\n",
    "\n",
    "## Why This Works for More Than Two Categories\n",
    "- The Naive Bayes model scales to $ K > 2 $ classes by:\n",
    "  - Estimating a prior $ \\phi_k $ for each class $ k = 1, 2, \\dots, K $.\n",
    "  - Learning $ \\psi_{jk} $ for each word $ j $ and class $ k $, resulting in $ Kd $ feature parameters.\n",
    "  - Computing $ P_\\theta(x | y=k)P_\\theta(y=k) $ for all $ K $ classes during prediction.\n",
    "- In this example, $ K = 3 $, but the same process applies for any $ K $. For $ K = 4 $ (e.g., adding a \"Promotional\" class), you’d count emails in the new class and estimate $ \\phi_4 $ and $ \\psi_{j4} $ similarly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8618d4-7afe-4427-aebf-e35bb5e0aa9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6502f5-df70-41c9-a548-b1434d94bdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d319a4-44f5-4adc-b680-75852b9d8130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
