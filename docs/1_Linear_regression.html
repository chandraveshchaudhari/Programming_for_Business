
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Linear Regression &#8212; Machine Learning for Business</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '1_Linear_regression';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Generalization, Overfitting, Regularization" href="2_Regularization.html" />
    <link rel="prev" title="Common Math Symbols and Notations in Machine Learning" href="symbols.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Machine Learning for Business - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Machine Learning for Business - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="symbols.html">Common Math Symbols and Notations in Machine Learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Linear Regression</a></li>

















<li class="toctree-l1"><a class="reference internal" href="2_Regularization.html">Generalization, Overfitting, Regularization</a></li>



<li class="toctree-l1"><a class="reference internal" href="3_Naive_Bayes.html">Naive Bayes</a></li>



<li class="toctree-l1"><a class="reference internal" href="4_Logistic_Regression.html"><strong>Logistic Regression (Mathematical Explanation)</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="5_Dimensionality_Reduction.html">Dimensionality Reduction: PCA</a></li>






<li class="toctree-l1"><a class="reference internal" href="6_Clustering.html">K-Means and Clustering</a></li>




<li class="toctree-l1"><a class="reference internal" href="7_Gaussian_Mixture_Models.html">Gaussian Mixture Models and EM</a></li>


<li class="toctree-l1"><a class="reference internal" href="8_Nearest_Neighbour_Algorithm.html">Nearest Neighbour Algorithm</a></li>







<li class="toctree-l1"><a class="reference internal" href="10_Support_Vector_Machines.html">Support Vector Machines</a></li>



<li class="toctree-l1"><a class="reference internal" href="11_Decision_Trees.html">Decision Trees</a></li>

<li class="toctree-l1"><a class="reference internal" href="12_Ensemble_Methods.html">Ensemble Methods</a></li>

<li class="toctree-l1"><a class="reference internal" href="13_Neural_Networks.html">Neural Networks: From Perceptron to MLP</a></li>





<li class="toctree-l1"><a class="reference internal" href="14_Arima.html">ARIMA</a></li>





<li class="toctree-l1"><a class="reference internal" href="15_LSTM.html">Advanced DL: ResNets, RNNs</a></li>

<li class="toctree-l1"><a class="reference internal" href="16_CNN.html">Convolutional Neural Networks</a></li>


<li class="toctree-l1"><a class="reference internal" href="17_Resnet.html">1D-ResNet (or TCN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_LLM.html">Transformers and LLMs</a></li>

<li class="toctree-l1"><a class="reference internal" href="19_Reinforcement_Learning.html">Reinforcement Learning</a></li>

<li class="toctree-l1"><a class="reference internal" href="20_Multimodal_Learning.html">Multimodal Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Generative_Models.html">Generative Models</a></li>




<li class="toctree-l1"><a class="reference internal" href="Performance_Metrics.html">Performance Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="Visualization.html">Visualisation</a></li>





</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/chandraveshchaudhari/BusinessML_web/blob/main/notebooks//./1_Linear_regression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>

<li>
  <a href="https://chandraveshchaudhari.github.io/BusinessML_web/jupyterlite/lab/index.html?path=1_Linear_regression.ipynb" target="_blank"
     class="btn btn-sm dropdown-item"
     title="Launch on JupyterLite"
     data-bs-placement="left" data-bs-toggle="tooltip">
    <span class="btn__icon-container" style="display:inline-block; width:20px; height:20px;">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 256">
        <circle cx="128" cy="128" r="128" fill="#f37726"/>
        <ellipse cx="128" cy="128" rx="110" ry="40" fill="white" transform="rotate(-25, 128, 128)"/>
        <ellipse cx="128" cy="128" rx="110" ry="40" fill="white" transform="rotate(25, 128, 128)"/>
        <circle cx="200" cy="60" r="18" fill="white"/>
        <circle cx="60" cy="200" r="18" fill="white"/>
      </svg>
    </span>
    <span class="btn__text-container">JupyterLite</span>
  </a>
</li>

</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/chandraveshchaudhari/BusinessML_web" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/chandraveshchaudhari/BusinessML_web/issues/new?title=Issue%20on%20page%20%2F1_Linear_regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/1_Linear_regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Linear Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contents">Contents</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model-class-linear-model-family">Model Class: Linear Model Family</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation">Explanation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-mathematical-notations-in-markdown-latex">Examples of Mathematical Notations in Markdown (<span class="math notranslate nohighlight">\( LaTeX \)</span>)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-equations-brief-overview">Normal Equations (Brief Overview)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-calculation">Memory Calculation:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-gradient-descent">Why Gradient Descent?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-gradient-descent">Introduction to Gradient Descent</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differentiation-in-gradient-descent">1. Differentiation in Gradient Descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#squared-error-cost-function">2. Squared Error Cost Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-gradient-descent-and-squared-error">3. Combining Gradient Descent and Squared Error</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-of-the-squared-error">Derivative of the Squared Error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quick-intro-calculus-key-concepts-at-a-glance">💡 <strong>Quick Intro:</strong> Calculus: Key concepts at a glance…</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limits">1. Limits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuity">2. Continuity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#differentiation">3. Differentiation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minima-and-maxima">4. Minima and Maxima</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">5. Gradient Descent</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#position-velocity-acceleration">1. Position, Velocity, Acceleration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2. Limits</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">3. Continuity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">4. Differentiation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-continuous-example">5. Non-Continuous Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-connection">6. Gradient Descent Connection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-vs-limit-as-t-to-infty">7. Derivative vs. Limit as <span class="math notranslate nohighlight">\(t\to\infty\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-code-to-visualize-gradient-descent-on-j-t">8. Python Code to Visualize Gradient Descent on <span class="math notranslate nohighlight">\(J(t)\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#business-calculus-with-a-profit-function-example">Business Calculus with a Profit Function Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Limits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Continuity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-continuous-discontinuous-functions">Non-Continuous (Discontinuous) Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#differentiation-marginal-profit">Differentiation (Marginal Profit)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integration">Integration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Gradient Descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-derivative-is-a-local-slope">🌄 The Derivative Is a Local Slope</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-know-which-direction-leads-to-the-minimum">🧭 How Do We Know Which Direction Leads to the Minimum?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-example-walk-on-a-curve">🚶 Simple Example: Walk on a Curve</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-small-intervals">🔍 Why Small Intervals?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">📌 Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#origin-of-the-power-rule-for-differentiation">Origin of the Power Rule for Differentiation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#origin-of-the-power-rule-for-integration">Origin of the Power Rule for Integration</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Gradient descent</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#an-objective-mean-squared-error">An Objective: Mean Squared Error</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-partial-derivatives">Mean Squared Error: Partial Derivatives</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-the-gradient">Mean Squared Error: The Gradient</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#begin-align-small-tiny-nabla-theta-j-i-theta-begin-bmatrix-frac-partial-j-i-theta-partial-theta-0-frac-partial-j-i-theta-partial-theta-1-vdots-frac-partial-j-i-theta-partial-theta-d-end-bmatrix">\begin{align*}
\small
{\tiny \nabla_\theta J^{(i)} (\theta)} = \begin{bmatrix}
\frac{\partial J^{(i)}(\theta)}{\partial \theta_0} \
\frac{\partial J^{(i)}(\theta)}{\partial \theta_1} \
\vdots \
\frac{\partial J^{(i)}(\theta)}{\partial \theta_d}
\end{bmatrix}</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#begin-bmatrix-left-f-theta-x-i-y-i-right-cdot-x-i-0-left-f-theta-x-i-y-i-right-cdot-x-i-1-vdots-left-f-theta-x-i-y-i-right-cdot-x-i-d-end-bmatrix">\begin{bmatrix}
\left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}<em>0 \
\left( f</em>\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}<em>1 \
\vdots \
\left( f</em>\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}_d
\end{bmatrix}</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#begin-align-nabla-theta-j-theta-begin-bmatrix-frac-partial-j-theta-partial-theta-0-frac-partial-j-theta-partial-theta-1-vdots-frac-partial-j-theta-partial-theta-d-end-bmatrix">\begin{align*}
\nabla_\theta J (\theta) = \begin{bmatrix}
\frac{\partial J(\theta)}{\partial \theta_0} \
\frac{\partial J(\theta)}{\partial \theta_1} \
\vdots \
\frac{\partial J(\theta)}{\partial \theta_d}
\end{bmatrix}</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#frac-1-n-sum-i-1-n-begin-bmatrix-frac-partial-j-i-theta-partial-theta-0-frac-partial-j-i-theta-partial-theta-1-vdots-frac-partial-j-i-theta-partial-theta-d-end-bmatrix">\frac{1}{n}\sum_{i=1}^n
\begin{bmatrix}
\frac{\partial J^{(i)}(\theta)}{\partial \theta_0} \
\frac{\partial J^{(i)}(\theta)}{\partial \theta_1} \
\vdots \
\frac{\partial J^{(i)}(\theta)}{\partial \theta_d}
\end{bmatrix}</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-least-squares">Ordinary Least Squares</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#notation-design-matrix">Notation: Design Matrix</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#notation-target-vector">Notation: Target Vector</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#squared-error-in-matrix-form">Squared Error in Matrix Form</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gradient-of-the-squared-error">The Gradient of the Squared Error</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-equations">Normal Equations</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-square-the-difference">Why square the difference?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-frac-1-2m-instead-of-frac-1-m"> Why <span class="math notranslate nohighlight">\(\frac{1}{2m}\)</span> instead of <span class="math notranslate nohighlight">\(\frac{1}{m}\)</span> ?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimising-the-cost-function">Minimising the cost function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#displaystyle-operatorname-argmin-theta-j-theta">$<span class="math notranslate nohighlight">\(\displaystyle \operatorname*{argmin}_\theta J(\theta)\)</span>$</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theta-theta-alpha-frac-partial-partial-theta-j-theta">$<span class="math notranslate nohighlight">\(\theta = \theta - \alpha \frac{\partial}{\partial \theta} J(\theta)\)</span>$</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differentiating-the-loss-function">Differentiating the loss function:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">$<span class="math notranslate nohighlight">\(\theta = \theta - \alpha \frac{\partial}{\partial \theta} J(\theta)\)</span>$</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarly-for-theta-1">Similarly, for <span class="math notranslate nohighlight">\(\theta_1\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#repeat-till-convergence">Repeat till convergence:</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#theta-0-theta-0-frac-1-m-sum-i-0-m-h-theta-x-i-y-i">1.       <span class="math notranslate nohighlight">\(\theta_0= \theta_0 -  \frac{1}{m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)} \)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#theta-1-theta-1-frac-1-m-sum-i-0-m-h-theta-x-i-y-i-x">2.       <span class="math notranslate nohighlight">\(\theta_1= \theta_1 -  \frac{1}{m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)}(x) \)</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-least-squares">Non-Linear Least Squares</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-functions">Polynomial Functions</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-regression">
<h1>Linear Regression<a class="headerlink" href="#linear-regression" title="Link to this heading">#</a></h1>
<p><left><img width=20% src="logo.jpg"></left></p>
<p><strong>Machine Learning for Business</strong> by Chandravesh Chaudhari</p>
<section id="contents">
<h2>Contents<a class="headerlink" href="#contents" title="Link to this heading">#</a></h2>
<p>Linear Regression</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>• Ordinary Least Squares (OLS)
• Matrix Solution to Linear Regression

• Python: Closed-Form vs Gradient Descent
</pre></div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="model-class-linear-model-family">
<h1>Model Class: Linear Model Family<a class="headerlink" href="#model-class-linear-model-family" title="Link to this heading">#</a></h1>
<p>Recall that a linear model has the form
\begin{align*}
y &amp; = \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2 + … + \theta_d \cdot x_d
\end{align*}
where <span class="math notranslate nohighlight">\(x \in \mathbb{R}^d\)</span> is a vector of features and <span class="math notranslate nohighlight">\(y\)</span> is the target. The <span class="math notranslate nohighlight">\(\theta_j\)</span> are the <em>parameters</em> of the model.</p>
<p>Linear regression will find a straight line that will try to best fit the data provided. It does so by learning the slope of the line, and the bais term (y-intercept)</p>
<p>Given a table:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>size of house(int sq. ft) (x)</p></th>
<th class="head"><p>price in $1000(y)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>450</p></td>
<td><p>100</p></td>
</tr>
<tr class="row-odd"><td><p>324</p></td>
<td><p>78</p></td>
</tr>
<tr class="row-even"><td><p>844</p></td>
<td><p>123</p></td>
</tr>
</tbody>
</table>
</div>
<p>Our hypothesis (prediction) is:
$<span class="math notranslate nohighlight">\(h_\theta(x) = \theta_0 + \theta_1x\)</span><span class="math notranslate nohighlight">\(
Will give us an equation of line that will predict the price. The above equation is nothing but the equation of line. __When we say the machine learns, we are actually adjusting the parameters \)</span>\theta_0<span class="math notranslate nohighlight">\( and \)</span>\theta_1<span class="math notranslate nohighlight">\(__. So for a new x (size of house) we will insert the value of x in the above equation and produce a value \)</span>\hat y$ (our prediction)</p>
<p>Below is a Python script that plots the equation <span class="math notranslate nohighlight">\(y = mx + c\)</span> using the provided data points and demonstrates how this equation relates to the linear model in the form of <span class="math notranslate nohighlight">\(\theta\)</span>. The script first plots the data points and a best-fit line calculated using linear regression, then explains the connection between <span class="math notranslate nohighlight">\(y = mx + c\)</span> and the vectorized form <span class="math notranslate nohighlight">\(h_\theta(x) = \theta^\top x\)</span>.</p>
<section id="explanation">
<h2>Explanation<a class="headerlink" href="#explanation" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Plotting <span class="math notranslate nohighlight">\(y = mx + c\)</span></strong>: The script uses the data points provided (| Size of house (x) | Price (y) |) to compute the slope <span class="math notranslate nohighlight">\(m\)</span> (equivalent to <span class="math notranslate nohighlight">\(\theta_1\)</span>) and intercept <span class="math notranslate nohighlight">\(c\)</span> (equivalent to <span class="math notranslate nohighlight">\(\theta_0\)</span>) via linear regression. It then plots these points and the line <span class="math notranslate nohighlight">\(y = mx + c\)</span> using Matplotlib.</p></li>
<li><p><strong>Relation to <span class="math notranslate nohighlight">\(\theta\)</span></strong>:</p>
<ul>
<li><p>The linear equation <span class="math notranslate nohighlight">\(y = mx + c\)</span> is a specific case of the linear model <span class="math notranslate nohighlight">\(h_\theta(x) = \theta_0 + \theta_1 x\)</span>, where:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(c = \theta_0\)</span> (the y-intercept or bias term),</p></li>
<li><p><span class="math notranslate nohighlight">\(m = \theta_1\)</span> (the slope or weight of the feature <span class="math notranslate nohighlight">\(x\)</span>).</p></li>
</ul>
</li>
<li><p>By defining <span class="math notranslate nohighlight">\(x_0 = 1\)</span> as a constant feature, we can extend the input <span class="math notranslate nohighlight">\(x\)</span> to a vector <span class="math notranslate nohighlight">\([1, x]\)</span>, and the parameters to a vector <span class="math notranslate nohighlight">\(\theta = [\theta_0, \theta_1]\)</span>.</p></li>
<li><p>The model then becomes <span class="math notranslate nohighlight">\(h_\theta(x) = \theta^\top x = \theta_0 \cdot 1 + \theta_1 \cdot x\)</span>, which is mathematically equivalent to <span class="math notranslate nohighlight">\(y = mx + c\)</span>.</p></li>
<li><p>This vectorized form <span class="math notranslate nohighlight">\(\theta^\top x\)</span> is commonly used in machine learning to generalize the model to multiple features, e.g., <span class="math notranslate nohighlight">\(h_\theta(x) = \theta_0 + \theta_1 x_1 + \cdots + \theta_d x_d\)</span> for <span class="math notranslate nohighlight">\(d\)</span> features.</p></li>
</ul>
</li>
<li><p><strong>Computation</strong>: The script calculates <span class="math notranslate nohighlight">\(\theta\)</span> using the normal equation, ensuring the line minimizes the Mean Squared Error (MSE) for the given data. The resulting <span class="math notranslate nohighlight">\(\theta_0\)</span> and <span class="math notranslate nohighlight">\(\theta_1\)</span> are printed and used to plot the line.</p></li>
</ul>
<p>This demonstrates both the plotting of <span class="math notranslate nohighlight">\(y = mx + c\)</span> and its representation in the <span class="math notranslate nohighlight">\(\theta\)</span>-based notation of linear regression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Data points from the table</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">450</span><span class="p">,</span> <span class="mi">324</span><span class="p">,</span> <span class="mi">844</span><span class="p">])</span>  <span class="c1"># Size of house in sq. ft</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">78</span><span class="p">,</span> <span class="mi">123</span><span class="p">])</span>   <span class="c1"># Price in $1000</span>

<span class="c1"># Construct the design matrix X with bias term (x_0 = 1)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># Shape: (3, 2), where each row is [1, x_i]</span>

<span class="c1"># Compute optimal parameters theta using the normal equation: theta = (X^T X)^(-1) X^T y</span>
<span class="n">XtX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">XtX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">XtX</span><span class="p">)</span>
<span class="n">Xty</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XtX_inv</span><span class="p">,</span> <span class="n">Xty</span><span class="p">)</span>
<span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span> <span class="o">=</span> <span class="n">theta</span>  <span class="c1"># theta_0 is the intercept (c), theta_1 is the slope (m)</span>

<span class="c1"># Print the parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parameters in theta form: theta_0 (intercept) = </span><span class="si">{</span><span class="n">theta_0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, theta_1 (slope) = </span><span class="si">{</span><span class="n">theta_1</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;In y = mx + c form: c = </span><span class="si">{</span><span class="n">theta_0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, m = </span><span class="si">{</span><span class="n">theta_1</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The model can be written as:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  h_theta(x) = </span><span class="si">{</span><span class="n">theta_0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">theta_1</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> * x&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Or in vectorized form: h_theta(x) = theta^T x, where theta = [theta_0, theta_1] and x = [1, x]&quot;</span><span class="p">)</span>

<span class="c1"># Generate points for the best-fit line</span>
<span class="n">x_line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">900</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># Range covering the data points</span>
<span class="n">y_line</span> <span class="o">=</span> <span class="n">theta_0</span> <span class="o">+</span> <span class="n">theta_1</span> <span class="o">*</span> <span class="n">x_line</span>  <span class="c1"># y = mx + c using computed theta_0 and theta_1</span>

<span class="c1"># Plot the data points and the best-fit line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data points&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_line</span><span class="p">,</span> <span class="n">y_line</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;y = </span><span class="si">{</span><span class="n">theta_1</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">x + </span><span class="si">{</span><span class="n">theta_0</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Size of house (sq. ft)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Price in $1000&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;House Price vs Size with Best-Fit Line&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameters in theta form: theta_0 (intercept) = 57.29, theta_1 (slope) = 0.08
In y = mx + c form: c = 57.29, m = 0.08
The model can be written as:
  h_theta(x) = 57.29 + 0.08 * x
Or in vectorized form: h_theta(x) = theta^T x, where theta = [theta_0, theta_1] and x = [1, x]
</pre></div>
</div>
<img alt="_images/f2131c8318cdd2d4bb17ca43db52ee65f467a89ad458d399981e71ced7936d3c.png" src="_images/f2131c8318cdd2d4bb17ca43db52ee65f467a89ad458d399981e71ced7936d3c.png" />
</div>
</div>
</section>
<section id="examples-of-mathematical-notations-in-markdown-latex">
<h2>Examples of Mathematical Notations in Markdown (<span class="math notranslate nohighlight">\( LaTeX \)</span>)<a class="headerlink" href="#examples-of-mathematical-notations-in-markdown-latex" title="Link to this heading">#</a></h2>
<p><strong>1. Number:</strong></p>
<ul class="simple">
<li><p>A simple number: <span class="math notranslate nohighlight">\(5\)</span></p></li>
<li><p>A negative number: <span class="math notranslate nohighlight">\(-3\)</span></p></li>
<li><p>A decimal number: <span class="math notranslate nohighlight">\(2.718\)</span></p></li>
<li><p>A fraction: <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span></p></li>
</ul>
<p><strong>2. Vector:</strong></p>
<ul class="simple">
<li><p>A column vector: <span class="math notranslate nohighlight">\(\mathbf{v} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}\)</span></p></li>
<li><p>A row vector: <span class="math notranslate nohighlight">\(\mathbf{u} = \begin{bmatrix} a &amp; b &amp; c \end{bmatrix}\)</span></p></li>
</ul>
<p><strong>3. Dot Product:</strong></p>
<ul class="simple">
<li><p>The dot product of two vectors <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>: <span class="math notranslate nohighlight">\(\mathbf{a} \cdot \mathbf{b} = a_1b_1 + a_2b_2 + \cdots + a_nb_n\)</span></p></li>
<li><p>Example with specific vectors: <span class="math notranslate nohighlight">\(\begin{bmatrix} 1 \\ 2 \end{bmatrix} \cdot \begin{bmatrix} 3 \\ 4 \end{bmatrix} = (1 \times 3) + (2 \times 4) = 3 + 8 = 11\)</span></p></li>
</ul>
<p><strong>4. Multiplication:</strong></p>
<ul class="simple">
<li><p>Scalar multiplication: <span class="math notranslate nohighlight">\(3 \times 4 = 12\)</span> or <span class="math notranslate nohighlight">\(3 \cdot 4 = 12\)</span></p></li>
<li><p>Matrix-vector multiplication: <span class="math notranslate nohighlight">\(A \mathbf{x} = \mathbf{b}\)</span>, where <span class="math notranslate nohighlight">\(A\)</span> is a matrix and <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{b}\)</span> are vectors.
Example: <span class="math notranslate nohighlight">\(\begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix} \begin{bmatrix} 5 \\ 6 \end{bmatrix} = \begin{bmatrix} (1 \times 5) + (2 \times 6) \\ (3 \times 5) + (4 \times 6) \end{bmatrix} = \begin{bmatrix} 17 \\ 39 \end{bmatrix}\)</span></p></li>
<li><p>Matrix-matrix multiplication: <span class="math notranslate nohighlight">\(AB = C\)</span>
Example: <span class="math notranslate nohighlight">\(\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} \begin{bmatrix} 2 &amp; 3 \\ 4 &amp; 5 \end{bmatrix} = \begin{bmatrix} (1 \times 2) + (0 \times 4) &amp; (1 \times 3) + (0 \times 5) \\ (0 \times 2) + (1 \times 4) &amp; (0 \times 3) + (1 \times 5) \end{bmatrix} = \begin{bmatrix} 2 &amp; 3 \\ 4 &amp; 5 \end{bmatrix}\)</span></p></li>
</ul>
<p><strong>5. Inverse:</strong></p>
<ul class="simple">
<li><p>The inverse of a matrix <span class="math notranslate nohighlight">\(A\)</span> is denoted as <span class="math notranslate nohighlight">\(A^{-1}\)</span>, such that <span class="math notranslate nohighlight">\(AA^{-1} = A^{-1}A = I\)</span>, where <span class="math notranslate nohighlight">\(I\)</span> is the identity matrix.</p></li>
<li><p>Example of a <span class="math notranslate nohighlight">\(2 \times 2\)</span> inverse: If <span class="math notranslate nohighlight">\(A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\)</span>, then <span class="math notranslate nohighlight">\(A^{-1} = \frac{1}{ad - bc} \begin{bmatrix} d &amp; -b \\ -c &amp; a \end{bmatrix}\)</span>, provided <span class="math notranslate nohighlight">\(ad - bc \neq 0\)</span>.
Example with numbers: If <span class="math notranslate nohighlight">\(A = \begin{bmatrix} 2 &amp; 1 \\ 4 &amp; 3 \end{bmatrix}\)</span>, then <span class="math notranslate nohighlight">\(ad-bc = (2 \times 3) - (1 \times 4) = 6 - 4 = 2\)</span>, and <span class="math notranslate nohighlight">\(A^{-1} = \frac{1}{2} \begin{bmatrix} 3 &amp; -1 \\ -4 &amp; 2 \end{bmatrix} = \begin{bmatrix} 1.5 &amp; -0.5 \\ -2 &amp; 1 \end{bmatrix}\)</span>.</p></li>
</ul>
<p><strong>6. Identity Matrix:</strong></p>
<ul class="simple">
<li><p>A <span class="math notranslate nohighlight">\(2 \times 2\)</span> identity matrix: <span class="math notranslate nohighlight">\(I_2 = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span></p></li>
<li><p>A <span class="math notranslate nohighlight">\(3 \times 3\)</span> identity matrix: <span class="math notranslate nohighlight">\(I_3 = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\)</span></p></li>
</ul>
<p><strong>7. Identity and Inverse Multiplication:</strong></p>
<ul class="simple">
<li><p>Multiplication of a matrix by its inverse results in the identity matrix: <span class="math notranslate nohighlight">\(AA^{-1} = I\)</span>
Example: <span class="math notranslate nohighlight">\(\begin{bmatrix} 2 &amp; 1 \\ 4 &amp; 3 \end{bmatrix} \begin{bmatrix} 1.5 &amp; -0.5 \\ -2 &amp; 1 \end{bmatrix} = \begin{bmatrix} (2 \times 1.5) + (1 \times -2) &amp; (2 \times -0.5) + (1 \times 1) \\ (4 \times 1.5) + (3 \times -2) &amp; (4 \times -0.5) + (3 \times 1) \end{bmatrix} = \begin{bmatrix} 3 - 2 &amp; -1 + 1 \\ 6 - 6 &amp; -2 + 3 \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} = I\)</span></p></li>
</ul>
<p><strong>8. Transpose:</strong></p>
<ul class="simple">
<li><p>The transpose of a matrix <span class="math notranslate nohighlight">\(A\)</span> is denoted as <span class="math notranslate nohighlight">\(A^T\)</span> or <span class="math notranslate nohighlight">\(A'\)</span>. The rows of <span class="math notranslate nohighlight">\(A\)</span> become the columns of <span class="math notranslate nohighlight">\(A^T\)</span>, and vice versa.</p></li>
<li><p>Example: If <span class="math notranslate nohighlight">\(A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{bmatrix}\)</span>, then <span class="math notranslate nohighlight">\(A^T = \begin{bmatrix} 1 &amp; 4 \\ 2 &amp; 5 \\ 3 &amp; 6 \end{bmatrix}\)</span>.</p></li>
<li><p>Transpose of a vector: If <span class="math notranslate nohighlight">\(\mathbf{v} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}\)</span>, then <span class="math notranslate nohighlight">\(\mathbf{v}^T = \begin{bmatrix} 1 &amp; 2 &amp; 3 \end{bmatrix}\)</span>.</p></li>
</ul>
<p><span style="color: Blue;">✏️ <strong>Exercise:</strong> Check how line theta was calculated by running code sequentially</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Define the slope (m) and y-intercept (c)</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">c</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Generate x values for the line</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Calculate the corresponding y values using the equation y = mx + c</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">c</span>

<span class="c1"># Create the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;y = </span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s1">x + </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Plot of a Linear Equation: y = mx + c&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># Add x-axis</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># Add y-axis</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c5980ae3de6d2e15e10367a95fbf6698441ae13a4f4ad60cff56b9a1b13b0259.png" src="_images/c5980ae3de6d2e15e10367a95fbf6698441ae13a4f4ad60cff56b9a1b13b0259.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">X</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Adjusted y to be a single column</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>  <span class="c1"># Increased figure height for better visualization</span>

<span class="c1"># Plot the scatter points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Data Points&quot;</span><span class="p">)</span>

<span class="c1"># Generate points for the curve (without noise)</span>
<span class="n">X_curve</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_curve</span> <span class="o">=</span> <span class="n">X_curve</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">X_curve</span> <span class="o">+</span> <span class="mi">5</span>

<span class="c1"># Plot the underlying curve</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_curve</span><span class="p">,</span> <span class="n">y_curve</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Underlying Curve: $x^2 + x + 5$&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$y$&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Scatter Plot with Underlying Curve&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2e6aeac0eabf575b349ad01275678327248e087b7423ac7d368369c8e4d6ae1d.png" src="_images/2e6aeac0eabf575b349ad01275678327248e087b7423ac7d368369c8e4d6ae1d.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">polynomial_evaluate</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Define a polynomial: 2x^3 - 3x^2 + 4x - 1</span>
<span class="n">coeffs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Evaluate the polynomial at x = 2</span>
<span class="n">x_eval</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">polynomial_evaluate</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">x_eval</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;p(</span><span class="si">{</span><span class="n">x_eval</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Plot the polynomial with scatter points</span>
<span class="n">x_curve</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_curve</span> <span class="o">=</span> <span class="n">polynomial_evaluate</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">x_curve</span><span class="p">)</span>

<span class="c1"># Generate scatter points along the curve with noise</span>
<span class="n">num_points</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">x_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
<span class="n">noise_amplitude</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># Adjust for the amount of scatter</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">noise_amplitude</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_points</span><span class="p">)</span>
<span class="n">y_points</span> <span class="o">=</span> <span class="n">polynomial_evaluate</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">x_points</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_curve</span><span class="p">,</span> <span class="n">y_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Polynomial Curve&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_points</span><span class="p">,</span> <span class="n">y_points</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Scatter Points&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Polynomial Curve with Scatter Points&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p(2) = 11
</pre></div>
</div>
<img alt="_images/ccbf9f48daa048b7b0a58852b1b6ee9e33b2f82a845fec4040ebcb43c948b550.png" src="_images/ccbf9f48daa048b7b0a58852b1b6ee9e33b2f82a845fec4040ebcb43c948b550.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">polynomial_evaluate</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Define a polynomial: 2x^3 - 3x^2 + 4x</span>
<span class="c1"># Removed the -1 to make it pass through origin</span>
<span class="n">coeffs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Changed the coefficients</span>

<span class="c1"># Evaluate the polynomial at x = 2</span>
<span class="n">x_eval</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">polynomial_evaluate</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">x_eval</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;p(</span><span class="si">{</span><span class="n">x_eval</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Plot the polynomial and scatter points</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># Adjusted x range to start from 0</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">polynomial_evaluate</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Generate scatter points</span>
<span class="n">num_points</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">x_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
<span class="n">noise_amplitude</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">noise_amplitude</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_points</span><span class="p">)</span>
<span class="n">y_points</span> <span class="o">=</span> <span class="n">polynomial_evaluate</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">x_points</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Polynomial Curve&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_points</span><span class="p">,</span> <span class="n">y_points</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Scatter Points&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Polynomial Curve Starting from Origin with Scatter Points&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="k">def</span><span class="w"> </span><span class="nf">polynomial_evaluate</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;numpy&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># for reproducibility</span>

<span class="c1"># Define the range for theta (in degrees)</span>
<span class="n">theta_deg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">theta_rad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">deg2rad</span><span class="p">(</span><span class="n">theta_deg</span><span class="p">)</span>

<span class="c1"># Choose between sine or cosine</span>
<span class="n">function_type</span> <span class="o">=</span> <span class="s2">&quot;sin&quot;</span>  <span class="c1"># You can change this to &quot;cos&quot;</span>

<span class="k">if</span> <span class="n">function_type</span> <span class="o">==</span> <span class="s2">&quot;sin&quot;</span><span class="p">:</span>
    <span class="n">y_base</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta_rad</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">function_type</span> <span class="o">==</span> <span class="s2">&quot;cos&quot;</span><span class="p">:</span>
    <span class="n">y_base</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta_rad</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid function_type. Choose &#39;sin&#39; or &#39;cos&#39;.&quot;</span><span class="p">)</span>

<span class="c1"># Adjust the curve to be in the positive quadrant</span>
<span class="c1"># We&#39;ll shift and scale the curve</span>
<span class="n">amplitude</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># Adjust for the vertical range of the curve</span>
<span class="n">vertical_shift</span> <span class="o">=</span> <span class="n">amplitude</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># Shift up so the minimum is above zero</span>
<span class="n">y_base_positive</span> <span class="o">=</span> <span class="n">amplitude</span> <span class="o">*</span> <span class="n">y_base</span> <span class="o">+</span> <span class="n">vertical_shift</span>

<span class="c1"># Generate random noise</span>
<span class="n">num_points</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">random_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">theta_deg</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">num_points</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">theta_points_deg</span> <span class="o">=</span> <span class="n">theta_deg</span><span class="p">[</span><span class="n">random_indices</span><span class="p">]</span>
<span class="n">theta_points_rad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">deg2rad</span><span class="p">(</span><span class="n">theta_points_deg</span><span class="p">)</span>
<span class="n">noise_amplitude</span> <span class="o">=</span> <span class="mf">1.5</span>  <span class="c1"># Adjust for the amount of scatter</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">noise_amplitude</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_points</span><span class="p">)</span>

<span class="c1"># Calculate the y-coordinates for the scattered points</span>
<span class="k">if</span> <span class="n">function_type</span> <span class="o">==</span> <span class="s2">&quot;sin&quot;</span><span class="p">:</span>
    <span class="n">y_points</span> <span class="o">=</span> <span class="n">amplitude</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta_points_rad</span><span class="p">)</span> <span class="o">+</span> <span class="n">vertical_shift</span> <span class="o">+</span> <span class="n">noise</span>
<span class="k">elif</span> <span class="n">function_type</span> <span class="o">==</span> <span class="s2">&quot;cos&quot;</span><span class="p">:</span>
    <span class="n">y_points</span> <span class="o">=</span> <span class="n">amplitude</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta_points_rad</span><span class="p">)</span> <span class="o">+</span> <span class="n">vertical_shift</span> <span class="o">+</span> <span class="n">noise</span>

<span class="c1"># Ensure all y_points are in the positive quadrant (just a safety measure)</span>
<span class="n">y_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_points</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="c1"># Adding a small offset to ensure positivity</span>

<span class="c1"># Create the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_deg</span><span class="p">,</span> <span class="n">y_base_positive</span><span class="p">,</span> <span class="s2">&quot;g-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">function_type</span><span class="si">}</span><span class="s2">(θ) curve (shifted)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">theta_points_deg</span><span class="p">,</span> <span class="n">y_points</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Data points with noise&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;θ (degrees)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Positive Quadrant </span><span class="si">{</span><span class="n">function_type</span><span class="si">}</span><span class="s2">(θ) Curve with Noisy Points&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y_points</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># Adjust y-axis limits</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># for reproducibility</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;numpy&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Define x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Define a 4th-degree polynomial to approximate the curve</span>
<span class="n">y_base</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1"># Add an increasing trend (optional, but seems present in the image)</span>
<span class="n">increasing_slope</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">y_trend</span> <span class="o">=</span> <span class="n">increasing_slope</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">y_base_increasing</span> <span class="o">=</span> <span class="n">y_base</span> <span class="o">+</span> <span class="n">y_trend</span>

<span class="c1"># Shift the curve to be in the positive quadrant</span>
<span class="n">vertical_shift</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">y_base_increasing</span><span class="p">))</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">y_positive</span> <span class="o">=</span> <span class="n">y_base_increasing</span> <span class="o">+</span> <span class="n">vertical_shift</span>

<span class="c1"># Generate random points</span>
<span class="n">num_points</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">random_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">num_points</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Allow replacement</span>
<span class="n">x_points</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">random_indices</span><span class="p">]</span>
<span class="n">noise_amplitude</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Adjust for the amount of scatter</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">noise_amplitude</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_points</span><span class="p">)</span>
<span class="n">y_points</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">x_points</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">x_points</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x_points</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x_points</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">increasing_slope</span> <span class="o">*</span> <span class="n">x_points</span> <span class="o">+</span> <span class="n">vertical_shift</span> <span class="o">+</span> <span class="n">noise</span>
<span class="n">y_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_points</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span>  <span class="c1"># Ensure positivity</span>

<span class="c1"># Create the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_positive</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Approximate Curve&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_points</span><span class="p">,</span> <span class="n">y_points</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Scatter Points&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Scatter Points Based on Curve Shape&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y_points</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2d607a19909222da4acd575411f544e7a76997c74851ecfdc7628257cdc28dd5.png" src="_images/2d607a19909222da4acd575411f544e7a76997c74851ecfdc7628257cdc28dd5.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.path</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.patches</span><span class="w"> </span><span class="kn">import</span> <span class="n">PathPatch</span>

<span class="c1"># Seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>

<span class="c1"># Number of samples</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Advertisement spend (x1)</span>
<span class="n">X1</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Define Bezier curve control points for x1 and y</span>
<span class="n">verts</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)]</span>

<span class="c1"># Function to calculate Bezier curve points using de Casteljau&#39;s algorithm</span>
<span class="k">def</span><span class="w"> </span><span class="nf">bezier</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">control_points</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">control_points</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">control_points</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">r</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">points</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">points</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">points</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">points</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Generate the Bezier curve</span>
<span class="n">t_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="n">bezier_curve</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">bezier</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">verts</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_values</span><span class="p">])</span>

<span class="c1"># Extract x2 from Bezier curve (marketing efficiency)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">bezier_curve</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># x2 is the first coordinate</span>

<span class="c1"># Revenue as a function of advertisement spend and marketing efficiency, with added noise</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">X1</span> <span class="o">+</span> <span class="mi">10</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.3</span>  <span class="c1"># Revenue</span>

<span class="c1"># Plotting the data in 3D</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">80</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

<span class="c1"># Plot the dataset</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>

<span class="c1"># Labels and grid</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Advertisement Spend ($x_1$)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Marketing Efficiency ($x_2$)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Revenue ($y$)&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Display the grid</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Show plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/36c5b79e1e7e0b7d93a7288f089e626b34d02215175a6e68d964431a9095271b.png" src="_images/36c5b79e1e7e0b7d93a7288f089e626b34d02215175a6e68d964431a9095271b.png" />
</div>
</div>
</section>
<section id="normal-equations-brief-overview">
<h2>Normal Equations (Brief Overview)<a class="headerlink" href="#normal-equations-brief-overview" title="Link to this heading">#</a></h2>
<p>To solve a linear regression model, we aim to minimize the squared error between predicted and actual values:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left\{
\begin{array}{rcrcl}
x^{(1)}_1 \theta_1 &amp; + x^{(1)}_2 \theta_2 &amp; + \cdots &amp; + x^{(1)}_n \theta_n &amp; = y^{(1)} \\
x^{(2)}_1 \theta_1 &amp; + x^{(2)}_2 \theta_2 &amp; + \cdots &amp; + x^{(2)}_n \theta_n &amp; = y^{(2)} \\
x^{(3)}_1 \theta_1 &amp; + x^{(3)}_2 \theta_2 &amp; + \cdots &amp; + x^{(3)}_n \theta_n &amp; = y^{(3)} \\
      \vdots       &amp;          \vdots      &amp;   \ddots &amp;          \vdots      &amp; = \vdots \\
x^{(m)}_1 \theta_1 &amp; + x^{(m)}_2 \theta_2 &amp; + \cdots &amp; + x^{(m)}_n \theta_n &amp; = y^{(m)}
\end{array}
\right.\,.
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
\vec{X} \cdot \vec{\theta} \approx \vec{y}
\]</div>
<p>This leads to the <strong>normal equation</strong>:</p>
<div class="math notranslate nohighlight">
\[\vec{X}^\intercal\vec{X} \hat{\theta} = \vec{X}^\intercal\vec{y}\]</div>
<div class="math notranslate nohighlight">
\[
\vec{\theta} = (\vec{X}^\intercal \vec{X})^{-1} \vec{X}^\intercal \vec{y}
\]</div>
<ul class="simple">
<li><p>Works when <span class="math notranslate nohighlight">\((\vec{X}^\intercal \vec{X})\)</span> is invertible.</p></li>
<li><p>Not ideal for large or non-invertible <span class="math notranslate nohighlight">\(\vec{X}\)</span>.</p></li>
<li><p>Often replaced with <strong>gradient descent</strong> for scalability and numerical stability.</p></li>
</ul>
<blockquote>
<div><p>Note: <span class="math notranslate nohighlight">\((\vec{X}^\intercal \vec{X})^{-1} \vec{X}^\intercal\)</span> is the <strong>Moore-Penrose pseudoinverse</strong> of <span class="math notranslate nohighlight">\(\vec{X}\)</span>.</p>
</div></blockquote>
<p>In business, finance, and management, the normal equation is often avoided in favor of gradient descent for large datasets due to computational and memory constraints. Here’s an example:</p>
<p>Suppose you’re building a financial model to predict stock prices using a dataset with 1 million transactions, each with 500 features (e.g., price, volume, market indicators, sentiment scores). Using the normal equation requires computing the matrix <span class="math notranslate nohighlight">\(X^TX\)</span>, where <span class="math notranslate nohighlight">\(X\)</span> is the feature matrix of size <span class="math notranslate nohighlight">\(1,000,000 \times 500\)</span>.</p>
<section id="memory-calculation">
<h3>Memory Calculation:<a class="headerlink" href="#memory-calculation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Matrix size</strong>: <span class="math notranslate nohighlight">\(X^TX\)</span> results in a <span class="math notranslate nohighlight">\(500 \times 500\)</span> matrix.</p></li>
<li><p><strong>Memory per element</strong>: Assuming double-precision floating-point (8 bytes per element).</p></li>
<li><p><strong>Total memory</strong>: <span class="math notranslate nohighlight">\(500 \times 500 \times 8 = 2,000,000\)</span> bytes = <span class="math notranslate nohighlight">\(1.907\)</span> MB for <span class="math notranslate nohighlight">\(X^TX\)</span> alone.</p></li>
<li><p><strong>Additional overhead</strong>: Storing <span class="math notranslate nohighlight">\(X\)</span> (<span class="math notranslate nohighlight">\(1,000,000 \times 500 \times 8\)</span> bytes = <span class="math notranslate nohighlight">\(3.725\)</span> GB) and computing the inverse of <span class="math notranslate nohighlight">\(X^TX\)</span> further increases memory and computational costs.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_transactions</span> <span class="o">=</span> <span class="mi">1_000_000</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">memory_size_xtx</span> <span class="o">=</span> <span class="n">n_features</span> <span class="o">*</span> <span class="n">n_features</span> <span class="o">*</span> <span class="mi">8</span>  <span class="c1"># bytes for $X^TX$</span>
<span class="n">memory_size_x</span> <span class="o">=</span> <span class="n">n_transactions</span> <span class="o">*</span> <span class="n">n_features</span> <span class="o">*</span> <span class="mi">8</span>  <span class="c1"># bytes for $X$</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Memory for X^TX: </span><span class="si">%.3f</span><span class="s2"> MB&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">memory_size_xtx</span> <span class="o">/</span> <span class="mi">2</span><span class="o">**</span><span class="mi">20</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Memory for X: </span><span class="si">%.3f</span><span class="s2"> GB&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">memory_size_x</span> <span class="o">/</span> <span class="mi">2</span><span class="o">**</span><span class="mi">30</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Memory for X^TX: 1.907 MB
Memory for X: 3.725 GB
</pre></div>
</div>
</div>
</div>
</section>
<section id="why-gradient-descent">
<h3>Why Gradient Descent?<a class="headerlink" href="#why-gradient-descent" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Scalability</strong>: Gradient descent processes data iteratively (e.g., in mini-batches), requiring memory only for a small subset of data at a time (e.g., <span class="math notranslate nohighlight">\(128 \times 500 \times 8\)</span> bytes = <span class="math notranslate nohighlight">\(0.061\)</span> MB per batch).</p></li>
<li><p><strong>Speed</strong>: For large datasets, computing the inverse of <span class="math notranslate nohighlight">\(X^TX\)</span> is computationally expensive (<span class="math notranslate nohighlight">\(O(n^3)\)</span>), while gradient descent is linear in the number of iterations.</p></li>
<li><p><strong>Real-time updates</strong>: In finance, new data (e.g., trades) arrives constantly. Gradient descent can update the model incrementally, while the normal equation requires recomputing everything.</p></li>
</ul>
<p>Thus, gradient descent is preferred for large-scale financial modeling, risk management, or customer segmentation in business applications where datasets are massive and computational resources are limited.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="introduction-to-gradient-descent">
<h1>Introduction to Gradient Descent<a class="headerlink" href="#introduction-to-gradient-descent" title="Link to this heading">#</a></h1>
<p>Gradient descent is an <strong>iterative optimization algorithm</strong> used to minimize a function, typically a loss/cost function in machine learning. It adjusts parameters by moving in the direction of the steepest descent, as defined by the negative gradient of the function.</p>
<hr class="docutils" />
<section id="differentiation-in-gradient-descent">
<h2>1. Differentiation in Gradient Descent<a class="headerlink" href="#differentiation-in-gradient-descent" title="Link to this heading">#</a></h2>
<p>To minimize a function <span class="math notranslate nohighlight">\(J(\theta)\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> represents model parameters, gradient descent uses <strong>derivatives</strong> to determine the slope of the cost function at a given point. The derivative <span class="math notranslate nohighlight">\(\frac{\partial J(\theta)}{\partial \theta_j}\)</span> quantifies how <span class="math notranslate nohighlight">\(J(\theta)\)</span> changes as <span class="math notranslate nohighlight">\(\theta_j\)</span> changes.</p>
<p>The update rule for gradient descent is:
$<span class="math notranslate nohighlight">\(
\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}
\)</span>$
where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is the <strong>learning rate</strong> (step size).</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{\partial J(\theta)}{\partial \theta_j}\)</span> is the partial derivative of <span class="math notranslate nohighlight">\(J(\theta)\)</span> with respect to <span class="math notranslate nohighlight">\(\theta_j\)</span>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="squared-error-cost-function">
<h2>2. Squared Error Cost Function<a class="headerlink" href="#squared-error-cost-function" title="Link to this heading">#</a></h2>
<p>A common loss function is the <strong>mean squared error (MSE)</strong>, used in linear regression. For <span class="math notranslate nohighlight">\(m\)</span> training examples, it measures the average squared difference between predicted values <span class="math notranslate nohighlight">\(h_\theta(x^{(i)})\)</span> and actual values <span class="math notranslate nohighlight">\(y^{(i)}\)</span>:
$<span class="math notranslate nohighlight">\(
J(\theta) = \frac{1}{2m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2
\)</span>$
Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(h_\theta(x^{(i)}) = \theta^T x^{(i)}\)</span> (linear model prediction).</p></li>
<li><p>The <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> term simplifies derivatives.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="combining-gradient-descent-and-squared-error">
<h2>3. Combining Gradient Descent and Squared Error<a class="headerlink" href="#combining-gradient-descent-and-squared-error" title="Link to this heading">#</a></h2>
<p>To minimize <span class="math notranslate nohighlight">\(J(\theta)\)</span>, we compute the gradient of the squared error term.</p>
<section id="derivative-of-the-squared-error">
<h3>Derivative of the Squared Error<a class="headerlink" href="#derivative-of-the-squared-error" title="Link to this heading">#</a></h3>
<p>For a single parameter <span class="math notranslate nohighlight">\(\theta_j\)</span>, the partial derivative of <span class="math notranslate nohighlight">\(J(\theta)\)</span> is:
$<span class="math notranslate nohighlight">\(
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_j^{(i)}
\)</span><span class="math notranslate nohighlight">\(
This derivative is used in the gradient descent update rule:
\)</span><span class="math notranslate nohighlight">\(
\theta_j := \theta_j - \alpha \cdot \frac{1}{m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_j^{(i)}
\)</span>$</p>
</section>
<section id="intuition">
<h3>Intuition<a class="headerlink" href="#intuition" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Squared error</strong>: Penalizes large errors quadratically, ensuring smooth optimization.</p></li>
<li><p><strong>Gradient</strong>: Points in the direction of steepest ascent, so moving in the <em>negative</em> gradient direction minimizes the error.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>Gradient descent iteratively adjusts parameters <span class="math notranslate nohighlight">\(\theta\)</span> using the gradient of the squared error. The squared error’s convexity guarantees convergence to a global minimum (for linear models), and differentiation provides the necessary direction for updates.</p>
</section>
<section id="quick-intro-calculus-key-concepts-at-a-glance">
<h2>💡 <strong>Quick Intro:</strong> Calculus: Key concepts at a glance…<a class="headerlink" href="#quick-intro-calculus-key-concepts-at-a-glance" title="Link to this heading">#</a></h2>
<p>Let’s define a total cost function:
$<span class="math notranslate nohighlight">\(
C(q) = 5q^2 - 40q + 200
\)</span>$</p>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(q\)</span> is the quantity of goods produced (e.g., number of items),</p></li>
<li><p><span class="math notranslate nohighlight">\(C(q)\)</span> is the total cost of producing <span class="math notranslate nohighlight">\(q\)</span> items.</p></li>
</ul>
<p>This quadratic function helps us demonstrate <strong>limits, continuity, differentiation, minima</strong>, and <strong>gradient descent</strong>, and is easy to connect with <strong>real business decisions</strong>.</p>
<section id="limits">
<h3>1. Limits<a class="headerlink" href="#limits" title="Link to this heading">#</a></h3>
<p>A limit tells us what value the cost function is approaching as quantity <span class="math notranslate nohighlight">\(q\)</span> approaches some value.</p>
<p>Example:</p>
<div class="math notranslate nohighlight">
\[
\lim_{q \to 4} C(q) = \lim_{q \to 4} (5q^2 - 40q + 200)
\]</div>
<p>Calculating:</p>
<div class="math notranslate nohighlight">
\[
C(4) = 5(4)^2 - 40(4) + 200 = 80 - 160 + 200 = 120
\]</div>
<p>So, the limit as <span class="math notranslate nohighlight">\(q\)</span> approaches 4 is 120.</p>
</section>
<section id="continuity">
<h3>2. Continuity<a class="headerlink" href="#continuity" title="Link to this heading">#</a></h3>
<p>The cost function is a polynomial and therefore <strong>continuous for all real values</strong> of <span class="math notranslate nohighlight">\(q\)</span>.</p>
<p>That means:</p>
<ul class="simple">
<li><p>The function has no gaps, jumps, or holes.</p></li>
<li><p>You can evaluate limits simply by plugging in the number:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\lim_{q \to a} C(q) = C(a)
\]</div>
<p>Example: At <span class="math notranslate nohighlight">\(q = 5\)</span>, we get:</p>
<div class="math notranslate nohighlight">
\[
C(5) = 5(5)^2 - 40(5) + 200 = 125 - 200 + 200 = 125
\]</div>
</section>
<section id="differentiation">
<h3>3. Differentiation<a class="headerlink" href="#differentiation" title="Link to this heading">#</a></h3>
<p>The first derivative of the cost function gives us the <strong>marginal cost</strong>:
How much the total cost changes when we produce one more unit.</p>
<p>Given:</p>
<div class="math notranslate nohighlight">
\[
C(q) = 5q^2 - 40q + 200
\]</div>
<p>Then:</p>
<div class="math notranslate nohighlight">
\[
C'(q) = \frac{d}{dq}C(q) = 10q - 40
\]</div>
<p>So at <span class="math notranslate nohighlight">\(q = 6\)</span>:</p>
<div class="math notranslate nohighlight">
\[
C'(6) = 10(6) - 40 = 20
\]</div>
<p>The marginal cost is 20 currency units per item.</p>
</section>
<section id="minima-and-maxima">
<h3>4. Minima and Maxima<a class="headerlink" href="#minima-and-maxima" title="Link to this heading">#</a></h3>
<p>To find the optimal quantity <span class="math notranslate nohighlight">\(q\)</span> that minimizes cost, we set:</p>
<div class="math notranslate nohighlight">
\[
C'(q) = 0 \Rightarrow 10q - 40 = 0 \Rightarrow q = 4
\]</div>
<p>Then, we check the second derivative:</p>
<div class="math notranslate nohighlight">
\[
C''(q) = \frac{d^2}{dq^2} C(q) = 10 &gt; 0
\]</div>
<p>Since <span class="math notranslate nohighlight">\(C''(q) &gt; 0\)</span>, this point is a <strong>minimum</strong>.</p>
<p>Therefore, <strong>minimum total cost</strong> occurs when <span class="math notranslate nohighlight">\(q = 4\)</span>:</p>
<div class="math notranslate nohighlight">
\[
C(4) = 5(4)^2 - 40(4) + 200 = 120
\]</div>
</section>
<section id="gradient-descent">
<h3>5. Gradient Descent<a class="headerlink" href="#gradient-descent" title="Link to this heading">#</a></h3>
<p>Gradient descent finds the value of <span class="math notranslate nohighlight">\(q\)</span> that minimizes <span class="math notranslate nohighlight">\(C(q)\)</span>.</p>
<p>Update rule:</p>
<div class="math notranslate nohighlight">
\[
q_{\text{new}} = q_{\text{old}} - \eta \cdot C'(q_{\text{old}})
\]</div>
<p>Where <span class="math notranslate nohighlight">\(\eta\)</span> is the learning rate, and <span class="math notranslate nohighlight">\(C'(q)\)</span> is the marginal cost.</p>
<p>Starting from <span class="math notranslate nohighlight">\(q = 8\)</span> and using <span class="math notranslate nohighlight">\(\eta = 0.1\)</span>, we iteratively reduce cost by following the slope.</p>
<p>Below is a <strong>single, physics‐based theme</strong>—using <strong>position</strong>, <strong>velocity</strong>, and <strong>acceleration</strong>—to illustrate limits, continuity, differentiation, non-continuous functions, and gradient descent. At the end, you’ll see a Python snippet to <strong>visualize gradient descent</strong> on a simple cost function derived from velocity.</p>
</section>
</section>
<hr class="docutils" />
<section id="position-velocity-acceleration">
<h2>1. Position, Velocity, Acceleration<a class="headerlink" href="#position-velocity-acceleration" title="Link to this heading">#</a></h2>
<p>Let an object move along a line so that its <strong>position</strong> at time <span class="math notranslate nohighlight">\(t\)</span> is
$<span class="math notranslate nohighlight">\(
s(t) = t^2 + 2t + 1\,,\quad t\in\mathbb{R}.
\)</span>$</p>
<ul class="simple">
<li><p>Its <strong>velocity</strong> is the derivative of position:
$<span class="math notranslate nohighlight">\(
v(t) = s'(t) = \frac{d}{dt}(t^2 + 2t + 1) = 2t + 2.
\)</span>$</p></li>
<li><p>Its <strong>acceleration</strong> is the derivative of velocity:
$<span class="math notranslate nohighlight">\(
a(t) = v'(t) = \frac{d}{dt}(2t + 2) = 2.
\)</span>$</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id1">
<h2>2. Limits<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>A limit tells us what a function “approaches” as <span class="math notranslate nohighlight">\(t\)</span> nears some value.</p>
<blockquote>
<div><p><strong>Example:</strong>
$<span class="math notranslate nohighlight">\(\lim_{t\to -1} v(t) = \lim_{t\to -1}(2t + 2) = 2(-1) + 2 = 0.\)</span>$</p>
</div></blockquote>
<p>Intuitively, as <span class="math notranslate nohighlight">\(t\)</span> approaches <span class="math notranslate nohighlight">\(-1\)</span>, the velocity approaches <span class="math notranslate nohighlight">\(0\,\)</span>.</p>
</section>
<hr class="docutils" />
<section id="id2">
<h2>3. Continuity<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>A function <span class="math notranslate nohighlight">\(f(t)\)</span> is continuous at <span class="math notranslate nohighlight">\(t=a\)</span> if</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(f(a)\)</span> is defined,</p></li>
<li><p><span class="math notranslate nohighlight">\(\lim_{t\to a}f(t)\)</span> exists,</p></li>
<li><p><span class="math notranslate nohighlight">\(\lim_{t\to a}f(t) = f(a)\)</span>.</p></li>
</ol>
<p>Since <span class="math notranslate nohighlight">\(s(t)=t^2+2t+1\)</span> is a polynomial, it’s continuous for <strong>all</strong> <span class="math notranslate nohighlight">\(t\)</span>.
Thus
$<span class="math notranslate nohighlight">\(
\lim_{t\to 3}s(t) = s(3) = 3^2 + 2\cdot3 + 1 = 16.
\)</span>$</p>
</section>
<hr class="docutils" />
<section id="id3">
<h2>4. Differentiation<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>First derivative</strong> <span class="math notranslate nohighlight">\(s'(t)=v(t)\)</span> gives the instantaneous <strong>velocity</strong>.</p></li>
<li><p><strong>Second derivative</strong> <span class="math notranslate nohighlight">\(s''(t)=a(t)\)</span> gives the instantaneous <strong>acceleration</strong>.</p></li>
</ul>
<blockquote>
<div><p><strong>At</strong> <span class="math notranslate nohighlight">\(t=1\)</span>:
$<span class="math notranslate nohighlight">\(
v(1)=2\cdot1+2=4,\quad a(1)=2.
\)</span>$</p>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="non-continuous-example">
<h2>5. Non-Continuous Example<a class="headerlink" href="#non-continuous-example" title="Link to this heading">#</a></h2>
<p>Consider the <strong>piecewise velocity</strong>:
$<span class="math notranslate nohighlight">\(
v_{\rm disc}(t) =
\begin{cases}
2t + 2, &amp; t &lt; 1,\\
5,       &amp; t = 1,\\
2t - 1,  &amp; t &gt; 1.
\end{cases}
\)</span>$</p>
<ul class="simple">
<li><p>At <span class="math notranslate nohighlight">\(t=1\)</span>,
$<span class="math notranslate nohighlight">\(\lim_{t\to1^-}v_{\rm disc}(t)=2\cdot1+2=4,\quad
  \lim_{t\to1^+}v_{\rm disc}(t)=2\cdot1-1=1,\)</span><span class="math notranslate nohighlight">\(
but \)</span>v_{\rm disc}(1)=5$.</p></li>
<li><p><strong>Conclusion:</strong> left‐limit <span class="math notranslate nohighlight">\(\neq\)</span> right‐limit <span class="math notranslate nohighlight">\(\neq\)</span> function value → <strong>discontinuity</strong>.</p></li>
</ul>
<p>Gradient‐based methods <strong>fail</strong> at such jumps, since the slope is undefined there.</p>
</section>
<hr class="docutils" />
<section id="gradient-descent-connection">
<h2>6. Gradient Descent Connection<a class="headerlink" href="#gradient-descent-connection" title="Link to this heading">#</a></h2>
<p>We often want to <strong>tune a parameter</strong> to make velocity hit a target.
Define a <strong>cost</strong> measuring squared error from a desired velocity <span class="math notranslate nohighlight">\(v^*\)</span>:
$<span class="math notranslate nohighlight">\(
J(t) = \bigl(v(t) - v^*\bigr)^2 = \bigl(2t+2 - v^*\bigr)^2.
\)</span>$</p>
<ul class="simple">
<li><p>Its derivative (gradient) is
$<span class="math notranslate nohighlight">\(
J'(t) = 2\bigl(2t+2 - v^*\bigr)\cdot 2 = 4\bigl(2t+2 - v^*\bigr).
\)</span>$</p></li>
<li><p><strong>Gradient descent</strong> updates
$<span class="math notranslate nohighlight">\(
t_{\rm new} = t_{\rm old} - \eta\,J'(t_{\rm old}),
\)</span><span class="math notranslate nohighlight">\(
stepping “downhill” in \)</span>J(t)<span class="math notranslate nohighlight">\( until \)</span>v(t)\approx v^*$.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="derivative-vs-limit-as-t-to-infty">
<h2>7. Derivative vs. Limit as <span class="math notranslate nohighlight">\(t\to\infty\)</span><a class="headerlink" href="#derivative-vs-limit-as-t-to-infty" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>A <strong>derivative</strong> <span class="math notranslate nohighlight">\(f'(t)\)</span> is the <strong>slope</strong> of <span class="math notranslate nohighlight">\(f\)</span> at a specific <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p>A <strong>limit</strong> <span class="math notranslate nohighlight">\(\lim_{t\to\infty}f(t)\)</span> describes the behavior <strong>far out</strong> as <span class="math notranslate nohighlight">\(t\)</span> grows without bound.</p></li>
</ul>
<blockquote>
<div><p><strong>Example</strong> for <span class="math notranslate nohighlight">\(s(t)=t^2\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(s'(t)=2t\)</span> gives slope at each <span class="math notranslate nohighlight">\(t\)</span> (e.g.\ <span class="math notranslate nohighlight">\(s'(3)=6\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\lim_{t\to\infty}s(t)=\infty\)</span> tells us the position grows arbitrarily large—but says nothing about the instantaneous slope at any finite <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ul>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="python-code-to-visualize-gradient-descent-on-j-t">
<h2>8. Python Code to Visualize Gradient Descent on <span class="math notranslate nohighlight">\(J(t)\)</span><a class="headerlink" href="#python-code-to-visualize-gradient-descent-on-j-t" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What it shows:</strong> the quadratic cost <span class="math notranslate nohighlight">\(J(t)\)</span> (blue curve) and how successive gradient‐descent iterations (markers) march toward the minimum, where the object’s velocity matches <span class="math notranslate nohighlight">\(v^*\)</span>.</p></li>
</ul>
<p>Perfect! I’ll create a conversational, easy-to-teach explanation covering limits, continuity, differentiation, non-continuous functions, integration, and gradient descent — all using a business profit function as the main example. I’ll organize it so you can walk your students smoothly through each idea.
I’ll get it ready for you shortly!</p>
<section id="business-calculus-with-a-profit-function-example">
<h3>Business Calculus with a Profit Function Example<a class="headerlink" href="#business-calculus-with-a-profit-function-example" title="Link to this heading">#</a></h3>
<p>Consider a simple business profit model: the profit <span class="math notranslate nohighlight">\(P(x)\)</span> from selling <span class="math notranslate nohighlight">\(x\)</span> units is revenue minus cost, i.e., <span class="math notranslate nohighlight">\(P(x) = R(x) - C(x)\)</span> (Calculus I - Business Applications). For example, if each item sells at price <span class="math notranslate nohighlight">\(p\)</span> and total revenue is <span class="math notranslate nohighlight">\(R(x) = p \cdot x\)</span>, then <span class="math notranslate nohighlight">\(P(x) = p \cdot x - C(x)\)</span>. In general <span class="math notranslate nohighlight">\(R(x)\)</span> and <span class="math notranslate nohighlight">\(C(x)\)</span> could be curved (e.g., price may fall at higher <span class="math notranslate nohighlight">\(x\)</span> and costs may rise nonlinearly). Throughout, think of <span class="math notranslate nohighlight">\(x\)</span> as quantity and <span class="math notranslate nohighlight">\(P(x)\)</span> as profit. We will use this unified profit function example to introduce limits, continuity, derivatives, discontinuities, integration, and even gradient descent, keeping the math clear but in everyday language.</p>
</section>
<section id="id4">
<h3>Limits<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>The limit of a function describes its behavior as the input <span class="math notranslate nohighlight">\(x\)</span> approaches some value. Intuitively, “<span class="math notranslate nohighlight">\(\lim_{x \to a} P(x) = L\)</span>” means that as <span class="math notranslate nohighlight">\(x\)</span> gets closer and closer to <span class="math notranslate nohighlight">\(a\)</span>, the profit <span class="math notranslate nohighlight">\(P(x)\)</span> gets arbitrarily close to some number <span class="math notranslate nohighlight">\(L\)</span> (Limit of a function - Wikipedia). In business terms, we might ask “what happens to profit as production approaches a certain level?” The limit formalizes this. For example, if there were a huge fixed cost or piecewise change at <span class="math notranslate nohighlight">\(x=a\)</span>, the profit might approach different values from the left or right. More concretely, if <span class="math notranslate nohighlight">\(R(x)\)</span> and <span class="math notranslate nohighlight">\(C(x)\)</span> are smooth, then as <span class="math notranslate nohighlight">\(x \to a\)</span> the profit smoothly approaches <span class="math notranslate nohighlight">\(P(a)\)</span>. The Wikipedia definition says: for any target distance around <span class="math notranslate nohighlight">\(L\)</span>, we can keep <span class="math notranslate nohighlight">\(f(x)\)</span> within that target by choosing <span class="math notranslate nohighlight">\(x\)</span> close enough to <span class="math notranslate nohighlight">\(a\)</span> (Limit of a function - Wikipedia).
In practice, one might consider limits like <span class="math notranslate nohighlight">\(\lim_{x \to 0} P(x)\)</span> (profit at very low output) or <span class="math notranslate nohighlight">\(\lim_{x \to \infty} P(x)\)</span> (profit if output grows without bound). For instance, if costs grow faster than revenue at high <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(\lim_{x \to \infty} P(x)\)</span> could be <span class="math notranslate nohighlight">\(-\infty\)</span> (huge losses). Limits are the foundation for defining continuity and derivatives, as we see next.</p>
</section>
<section id="id5">
<h3>Continuity<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>A function is continuous if its value doesn’t jump suddenly as <span class="math notranslate nohighlight">\(x\)</span> changes. Formally, <span class="math notranslate nohighlight">\(P(x)\)</span> is continuous at <span class="math notranslate nohighlight">\(x=a\)</span> if <span class="math notranslate nohighlight">\(\lim_{x \to a} P(x) = P(a)\)</span> (Limit of a function - Wikipedia). In plain terms, small changes in <span class="math notranslate nohighlight">\(x\)</span> (production) cause only small changes in profit – the graph of profit is unbroken. For most simple profit models (like polynomial cost and revenue), <span class="math notranslate nohighlight">\(P(x)\)</span> will be continuous everywhere in their domain. In business terms, continuity means no sudden surprises in profit: e.g., incremental production smoothly increases/decreases profit.
However, if there are abrupt changes – such as a step cost (perhaps a new factory upgrade kicks in at <span class="math notranslate nohighlight">\(x=100\)</span>) or a sudden tariff – the profit graph could have breaks or jumps, meaning discontinuities. A helpful picture is that a continuous graph can be drawn without lifting a pen (Discontinuous Function - Meaning, Types, Examples). If you must pick up your pen (there’s a gap or jump), the function is discontinuous. We discuss those next.</p>
</section>
<section id="non-continuous-discontinuous-functions">
<h3>Non-Continuous (Discontinuous) Functions<a class="headerlink" href="#non-continuous-discontinuous-functions" title="Link to this heading">#</a></h3>
<p>A discontinuous profit function has gaps, jumps, or holes. As Cuemath explains, a discontinuous function “has breaks/gaps on its graph” (Discontinuous Function - Meaning, Types, Examples). In business, imagine this: your factory can produce up to 100 units at one cost structure, but producing the 101st unit requires renting additional equipment. Suddenly cost jumps at <span class="math notranslate nohighlight">\(x=100\)</span>, so the profit function has a jump at that point. Mathematically, either the profit is not defined at some <span class="math notranslate nohighlight">\(x=a\)</span>, or <span class="math notranslate nohighlight">\(\lim_{x \to a^-} P(x) \neq \lim_{x \to a^+} P(x)\)</span>, or the limit doesn’t equal <span class="math notranslate nohighlight">\(P(a)\)</span> (Discontinuous Function - Meaning, Types, Examples).
For example, if a demand price changes abruptly at a certain quantity or a tax kicks in, profit can drop sharply. Discontinuous profit functions are more complex to analyze, but the key idea is intuitive: if your profit model has built-in jumps (like step costs or price tiers), it’s discontinuous. In such cases, classic calculus tools like setting derivatives to zero may fail at the jump point. We can still study limits on each side or use piecewise analysis. But typically for optimization we assume the profit is continuous (no gaps), so calculus works smoothly.</p>
</section>
<section id="differentiation-marginal-profit">
<h3>Differentiation (Marginal Profit)<a class="headerlink" href="#differentiation-marginal-profit" title="Link to this heading">#</a></h3>
<p>The derivative <span class="math notranslate nohighlight">\(P'(x)\)</span> measures the instantaneous rate of change of profit with respect to quantity. It is the slope of the tangent line to <span class="math notranslate nohighlight">\(P(x)\)</span> at <span class="math notranslate nohighlight">\(x\)</span> (Derivative - Wikipedia). In practice, <span class="math notranslate nohighlight">\(P'(x)\)</span> is called the marginal profit: approximately how much additional profit you get by selling one more unit (or a tiny increment of units). If <span class="math notranslate nohighlight">\(P'(x)\)</span> is large and positive, a small increase in production yields a large profit gain; if <span class="math notranslate nohighlight">\(P'(x)\)</span> is negative, making one more item reduces profit.
We often write the derivative definition as
<span class="math notranslate nohighlight">\(P'(x) = \lim_{h \to 0} \frac{P(x+h) - P(x)}{h}\)</span>, but conceptually it’s “instantaneous” change. The Wikipedia article sums it up: the derivative is a “fundamental tool that quantifies the sensitivity to change” – essentially the rate at which profit changes for a small change in <span class="math notranslate nohighlight">\(x\)</span> (Derivative - Wikipedia).
Business interpretation:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(P'(x) &gt; 0\)</span>, producing one more unit increases profit.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(P'(x) &lt; 0\)</span>, producing one more unit decreases profit.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(P'(x) = 0\)</span>, profit is at a local maximum or minimum (a critical point).</p></li>
</ul>
<p>These rules can be bulleted as key takeaways:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P'(x) &gt; 0\)</span>: Profit is increasing in <span class="math notranslate nohighlight">\(x\)</span>, so it pays to produce more.</p></li>
<li><p><span class="math notranslate nohighlight">\(P'(x) &lt; 0\)</span>: Profit is decreasing in <span class="math notranslate nohighlight">\(x\)</span>, so reduce production.</p></li>
<li><p><span class="math notranslate nohighlight">\(P'(x) = 0\)</span>: You may have reached an optimum (e.g., maximum profit or minimum loss).</p></li>
</ul>
<p>In fact, to maximize profit, we set <span class="math notranslate nohighlight">\(P'(x) = 0\)</span> and check the result. This is equivalent to the well-known economic rule “marginal revenue = marginal cost.” Since <span class="math notranslate nohighlight">\(P(x) = R(x) - C(x)\)</span>, we have <span class="math notranslate nohighlight">\(P'(x) = R'(x) - C'(x)\)</span>. Paul’s notes explain that if <span class="math notranslate nohighlight">\(P'' &lt; 0\)</span> (concave down), then the maximum occurs when <span class="math notranslate nohighlight">\(R'(x) = C'(x)\)</span> – i.e., marginal profit is zero (Calculus I - Business Applications). In other words, you find the quantity <span class="math notranslate nohighlight">\(x\)</span> where increasing production neither increases nor decreases profit. That will be the peak of the profit curve under normal concave conditions.
Once we solve <span class="math notranslate nohighlight">\(P'(x) = 0\)</span>, we may check <span class="math notranslate nohighlight">\(P''(x)\)</span> (the second derivative) to ensure it is a maximum (usually <span class="math notranslate nohighlight">\(P'' &lt; 0\)</span> there). In our example, you would compute <span class="math notranslate nohighlight">\(P'(x)\)</span> and solve for <span class="math notranslate nohighlight">\(x\)</span>. For instance, if <span class="math notranslate nohighlight">\(P'(x) = 100 + 0.05x - 0.000012x^2\)</span> (as in Paul’s example (Calculus I - Business Applications)), setting this to zero and solving gives candidate maximizers. Then checking concavity confirms which is a maximum.
Thus, differentiation turns profit curves into actionable advice: the sign of <span class="math notranslate nohighlight">\(P'(x)\)</span> tells you to increase or decrease output, and <span class="math notranslate nohighlight">\(P'(x) = 0\)</span> finds the optimal production for peak profit (Calculus I - Business Applications).</p>
</section>
<section id="integration">
<h3>Integration<a class="headerlink" href="#integration" title="Link to this heading">#</a></h3>
<p>Integration is the reverse process of differentiation. It accumulates small quantities. In calculus, the integral of <span class="math notranslate nohighlight">\(P'(x)\)</span> gives the total change in profit. The Fundamental Theorem of Calculus tells us that for a continuous marginal-profit function <span class="math notranslate nohighlight">\(P'\)</span>, we can recover profit from it (Fundamental theorem of calculus - Wikipedia). In intuitive terms, “integrating” means summing up tiny bits of profit. The Wikipedia description says integration computes the area under the graph of a function or the cumulative effect of small contributions (Fundamental theorem of calculus - Wikipedia).
Concretely, if you know the marginal profit <span class="math notranslate nohighlight">\(P'(x)\)</span> at every <span class="math notranslate nohighlight">\(x\)</span>, then
<span class="math notranslate nohighlight">\(P(x) = \int P'(x) \, dx + \text{constant}\)</span>.
If we set a reference point (say <span class="math notranslate nohighlight">\(P(0) = 0\)</span> when selling nothing yields zero profit), then the definite integral from 0 to <span class="math notranslate nohighlight">\(x\)</span> gives total profit:
<span class="math notranslate nohighlight">\(P(x) = \int_0^x P'(t) \, dt\)</span>.
In other words, the area under the marginal-profit curve from 0 to <span class="math notranslate nohighlight">\(x\)</span> is the profit at <span class="math notranslate nohighlight">\(x\)</span>. More generally, the Fundamental Theorem states
<span class="math notranslate nohighlight">\(\int_a^b P'(x) \, dx = P(b) - P(a)\)</span>,
meaning the integral of marginal profit from <span class="math notranslate nohighlight">\(a\)</span> to <span class="math notranslate nohighlight">\(b\)</span> equals the change in actual profit (Fundamental theorem of calculus - Wikipedia).
Business interpretation: If you know how each additional item contributes to profit (the marginal profit function <span class="math notranslate nohighlight">\(P'(x)\)</span>), then integrating it tells you overall profit. For example, if a new product’s marginal profit is <span class="math notranslate nohighlight">\(P'(x) \approx 150\)</span> at <span class="math notranslate nohighlight">\(x=2500\)</span> (meaning each unit around 2500 adds <span class="math notranslate nohighlight">\(150 profit), then integrating \)</span>P’(x)$ up to 2500 shows the total profit (minus any base profit at 0). Integration also applies to costs: the total cost is the integral of marginal cost. In summary, integration lets us find accumulated profit or cost by summing up marginal contributions (Fundamental theorem of calculus - Wikipedia) (Fundamental theorem of calculus - Wikipedia).</p>
</section>
<section id="id6">
<h3>Gradient Descent<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>Gradient descent is an iterative method to optimize functions using derivatives (Gradient descent - Wikipedia). Think of it as algorithmically “following the slope” to find a minimum. In our context, we often want to maximize profit. Gradient descent as defined in calculus actually finds local minima (it moves against the gradient). However, to find a maximum profit, one can simply take the opposite approach (“gradient ascent”) by stepping in the positive gradient direction (Gradient descent - Wikipedia).
The basic rule of gradient descent is: start with some initial <span class="math notranslate nohighlight">\(x\)</span> (production level) and update it by moving against the gradient of the function. In formula form, one step is
<span class="math notranslate nohighlight">\(x_{\text{new}} = x_{\text{old}} - \gamma \, P'(x_{\text{old}})\)</span>,
where <span class="math notranslate nohighlight">\(\gamma\)</span> is a small positive step size (learning rate) (Gradient descent - Wikipedia). Here <span class="math notranslate nohighlight">\(P'(x)\)</span> is the derivative (slope) of profit. The Wikipedia description notes that by moving in the negative gradient direction, the function value decreases fastest (Gradient descent - Wikipedia). In our profit context, we would usually use the positive gradient (add <span class="math notranslate nohighlight">\(\gamma P'(x)\)</span>) to move uphill towards higher profit (or equivalently minimize <span class="math notranslate nohighlight">\(-P\)</span>).
In practice, one might adjust <span class="math notranslate nohighlight">\(x\)</span> iteratively: if <span class="math notranslate nohighlight">\(P'(x)\)</span> is positive, increase <span class="math notranslate nohighlight">\(x\)</span> (moving up the profit slope); if <span class="math notranslate nohighlight">\(P'(x)\)</span> is negative, decrease <span class="math notranslate nohighlight">\(x\)</span>. The update formula ensures each step moves towards the optimum. For sufficiently small <span class="math notranslate nohighlight">\(\gamma\)</span>, this process converges toward the local maximum (like climbing a hill one small step at a time). We must choose a suitable <span class="math notranslate nohighlight">\(\gamma\)</span> so steps aren’t too large (overshooting) or too small (too slow).
Business tie-in: Gradient descent (or ascent) is useful when the profit function is complicated and we can’t easily solve <span class="math notranslate nohighlight">\(P' = 0\)</span> algebraically. It suggests a rule: adjust production gradually in the direction that increases profit. If you notice profit rising as you add units, keep adding; if profit falls, cut back. By iterating this idea, you hone in on the best production level. Although businesses often solve for <span class="math notranslate nohighlight">\(P' = 0\)</span> directly, the gradient approach is analogous to trial-and-error tuning of output to maximize profit or minimize cost.
In summary, limits and continuity ensure our profit models behave sensibly, derivatives (marginal profit) tell us how small changes affect profit, integration sums up those changes to total profit, and gradient descent is a practical algorithm to adjust production toward optimal profit. This suite of calculus ideas—limits, continuity, differentiation, integration, and gradient-based optimization—provides a powerful toolkit for making intuitive business decisions like “should I produce more or less?” and “how do I find the output that maximizes profit?” (Derivative - Wikipedia) (Gradient descent - Wikipedia) (Calculus I - Business Applications).
Sources: Definitions and key concepts are supported by calculus references (Limit of a function - Wikipedia) (Limit of a function - Wikipedia) (Derivative - Wikipedia) (Fundamental theorem of calculus - Wikipedia) (Gradient descent - Wikipedia), and the profit formula <span class="math notranslate nohighlight">\(P(x) = R(x) - C(x)\)</span> by Paul’s calculus notes (Calculus I - Business Applications).</p>
<p>That’s a great final question — and it gets to the <strong>core idea of optimization</strong> using derivatives and gradient descent.</p>
</section>
<hr class="docutils" />
<section id="the-derivative-is-a-local-slope">
<h3>🌄 The Derivative Is a Local Slope<a class="headerlink" href="#the-derivative-is-a-local-slope" title="Link to this heading">#</a></h3>
<p>The <strong>derivative</strong> of a function <span class="math notranslate nohighlight">\(f(x)\)</span> at a point <span class="math notranslate nohighlight">\(x\)</span> tells us the <strong>slope</strong> (or rate of change) of the function <strong>right at that point</strong>. It’s like standing on a hill and asking:</p>
<blockquote>
<div><p>“If I take one small step forward or backward, will I go uphill or downhill?”</p>
</div></blockquote>
<p>Mathematically:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(f'(x) &gt; 0\)</span>, the function is <strong>increasing</strong> at <span class="math notranslate nohighlight">\(x\)</span> → slope tilts <strong>upward</strong></p></li>
<li><p>If <span class="math notranslate nohighlight">\(f'(x) &lt; 0\)</span>, the function is <strong>decreasing</strong> at <span class="math notranslate nohighlight">\(x\)</span> → slope tilts <strong>downward</strong></p></li>
<li><p>If <span class="math notranslate nohighlight">\(f'(x) = 0\)</span>, we’re at a <strong>flat</strong> spot — it could be a minimum, maximum, or inflection point</p></li>
</ul>
<p>So:</p>
</section>
<section id="how-do-we-know-which-direction-leads-to-the-minimum">
<h3>🧭 How Do We Know Which Direction Leads to the Minimum?<a class="headerlink" href="#how-do-we-know-which-direction-leads-to-the-minimum" title="Link to this heading">#</a></h3>
<p>We use the <strong>sign of the derivative</strong>.</p>
<p>In <strong>gradient descent</strong>, we always move <strong>opposite</strong> the direction of the slope, because that’s the way to go <strong>downhill</strong> (toward the minimum).</p>
</section>
<hr class="docutils" />
<section id="simple-example-walk-on-a-curve">
<h3>🚶 Simple Example: Walk on a Curve<a class="headerlink" href="#simple-example-walk-on-a-curve" title="Link to this heading">#</a></h3>
<p>Let’s say we have a cost function:</p>
<div class="math notranslate nohighlight">
\[
C(x) = (x - 3)^2 + 5
\]</div>
<p>This is a simple parabola with its <strong>minimum</strong> at <span class="math notranslate nohighlight">\(x = 3\)</span>.</p>
<p>The derivative is:</p>
<div class="math notranslate nohighlight">
\[
C'(x) = 2(x - 3)
\]</div>
<p>So at different points:</p>
<ul class="simple">
<li><p>At <span class="math notranslate nohighlight">\(x = 5\)</span>: <span class="math notranslate nohighlight">\(C'(5) = 2(5 - 3) = 4\)</span> → positive slope → go <strong>left</strong> to reduce cost</p></li>
<li><p>At <span class="math notranslate nohighlight">\(x = 1\)</span>: <span class="math notranslate nohighlight">\(C'(1) = 2(1 - 3) = -4\)</span> → negative slope → go <strong>right</strong> to reduce cost</p></li>
<li><p>At <span class="math notranslate nohighlight">\(x = 3\)</span>: <span class="math notranslate nohighlight">\(C'(3) = 0\)</span> → flat spot → minimum!</p></li>
</ul>
<p>We use the derivative as a compass:</p>
<blockquote>
<div><p><strong>Go opposite the sign of the derivative to reduce the function.</strong></p>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="why-small-intervals">
<h3>🔍 Why Small Intervals?<a class="headerlink" href="#why-small-intervals" title="Link to this heading">#</a></h3>
<p>Derivatives are <strong>local</strong> — they only tell you what’s happening <strong>right now</strong>. So in gradient descent, we take <strong>small steps</strong> in the negative gradient direction. Over many steps, we spiral toward the minimum.</p>
<p>This works well <strong>as long as</strong>:</p>
<ul class="simple">
<li><p>The function is <strong>smooth</strong></p></li>
<li><p>The learning rate <span class="math notranslate nohighlight">\(\gamma\)</span> is <strong>small enough</strong> not to overshoot</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id7">
<h3>📌 Summary<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Derivative tells us the <strong>slope</strong> — the direction and steepness of the curve at one point</p></li>
<li><p>We use its <strong>sign</strong> to decide direction (go opposite to minimize)</p></li>
<li><p><strong>Gradient descent</strong> uses this info to take many small steps toward a minimum</p></li>
<li><p>The process <strong>doesn’t know the whole curve</strong>, but follows the slope like walking downhill</p></li>
</ul>
<p>Would you like a simple Python plot showing this visually with arrows and updates step-by-step?</p>
</section>
<section id="origin-of-the-power-rule-for-differentiation">
<h3>Origin of the Power Rule for Differentiation<a class="headerlink" href="#origin-of-the-power-rule-for-differentiation" title="Link to this heading">#</a></h3>
<p>The power rule for differentiation, <span class="math notranslate nohighlight">\(\frac{d}{dx}(x^n) = nx^{n-1}\)</span>, comes directly from the definition of the derivative using limits:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{dx}f(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}\]</div>
<p>Let <span class="math notranslate nohighlight">\(f(x) = x^n\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is a positive integer. Then:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{dx}(x^n) = \lim_{h \to 0} \frac{(x+h)^n - x^n}{h}\]</div>
<p>We can expand <span class="math notranslate nohighlight">\((x+h)^n\)</span> using the binomial theorem:</p>
<div class="math notranslate nohighlight">
\[(x+h)^n = x^n + nx^{n-1}h + \frac{n(n-1)}{2!}x^{n-2}h^2 + \dots + h^n\]</div>
<p>Substituting this back into the limit expression:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{dx}(x^n) = \lim_{h \to 0} \frac{(x^n + nx^{n-1}h + \frac{n(n-1)}{2!}x^{n-2}h^2 + \dots + h^n) - x^n}{h}\]</div>
<div class="math notranslate nohighlight">
\[= \lim_{h \to 0} \frac{nx^{n-1}h + \frac{n(n-1)}{2!}x^{n-2}h^2 + \dots + h^n}{h}\]</div>
<p>Now, we can divide each term in the numerator by <span class="math notranslate nohighlight">\(h\)</span>:</p>
<div class="math notranslate nohighlight">
\[= \lim_{h \to 0} \left( nx^{n-1} + \frac{n(n-1)}{2!}x^{n-2}h + \dots + h^{n-1} \right)\]</div>
<p>As <span class="math notranslate nohighlight">\(h\)</span> approaches 0, all terms containing <span class="math notranslate nohighlight">\(h\)</span> will also approach 0. Therefore, we are left with:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{dx}(x^n) = nx^{n-1}\]</div>
<p>This proves the power rule for positive integer values of <span class="math notranslate nohighlight">\(n\)</span>. The rule can be extended to other real numbers using more advanced techniques like logarithmic differentiation.</p>
</section>
<section id="origin-of-the-power-rule-for-integration">
<h3>Origin of the Power Rule for Integration<a class="headerlink" href="#origin-of-the-power-rule-for-integration" title="Link to this heading">#</a></h3>
<p>The power rule for integration, <span class="math notranslate nohighlight">\(\int x^n \, dx = \frac{x^{n+1}}{n+1} + C\)</span> (for <span class="math notranslate nohighlight">\(n \neq -1\)</span>), is essentially the reverse process of the power rule for differentiation.</p>
<p>If we differentiate <span class="math notranslate nohighlight">\(\frac{x^{n+1}}{n+1}\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{dx} \left( \frac{x^{n+1}}{n+1} \right) = \frac{1}{n+1} \frac{d}{dx} (x^{n+1})\]</div>
<p>Applying the power rule for differentiation (with the exponent being <span class="math notranslate nohighlight">\(n+1\)</span>):</p>
<div class="math notranslate nohighlight">
\[= \frac{1}{n+1} ((n+1)x^{(n+1)-1}) = \frac{1}{n+1} (n+1)x^n = x^n\]</div>
<p>Since the derivative of <span class="math notranslate nohighlight">\(\frac{x^{n+1}}{n+1}\)</span> is <span class="math notranslate nohighlight">\(x^n\)</span>, it follows by the definition of the antiderivative (indefinite integral) that:</p>
<div class="math notranslate nohighlight">
\[\int x^n \, dx = \frac{x^{n+1}}{n+1} + C\]</div>
<p>The constant of integration, <span class="math notranslate nohighlight">\(C\)</span>, arises because the derivative of a constant is always zero.</p>
<hr class="docutils" />
<p>The indefinite integral of <span class="math notranslate nohighlight">\(x+1\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span> is found by applying the power rule of integration and the linearity of integration. Here’s the step-by-step process:</p>
<p>We want to find:
$<span class="math notranslate nohighlight">\(\int (x + 1) \, dx\)</span>$</p>
<p>Using the linearity of integration, which states that <span class="math notranslate nohighlight">\(\int [f(x) + g(x)] \, dx = \int f(x) \, dx + \int g(x) \, dx\)</span>, we can split the integral into two parts:</p>
<div class="math notranslate nohighlight">
\[\int (x + 1) \, dx = \int x \, dx + \int 1 \, dx\]</div>
<p>Now, let’s integrate each part separately using the power rule for integration, which states that <span class="math notranslate nohighlight">\(\int x^n \, dx = \frac{x^{n+1}}{n+1} + C\)</span> (for <span class="math notranslate nohighlight">\(n \neq -1\)</span>).</p>
<p>For the first part, <span class="math notranslate nohighlight">\(\int x \, dx\)</span>, we have <span class="math notranslate nohighlight">\(n = 1\)</span>:
$<span class="math notranslate nohighlight">\(\int x^1 \, dx = \frac{x^{1+1}}{1+1} + C_1 = \frac{x^2}{2} + C_1\)</span>$</p>
<p>For the second part, <span class="math notranslate nohighlight">\(\int 1 \, dx\)</span>, we can think of <span class="math notranslate nohighlight">\(1\)</span> as <span class="math notranslate nohighlight">\(x^0\)</span>, so <span class="math notranslate nohighlight">\(n = 0\)</span>:
$<span class="math notranslate nohighlight">\(\int x^0 \, dx = \frac{x^{0+1}}{0+1} + C_2 = \frac{x^1}{1} + C_2 = x + C_2\)</span>$</p>
<p>Combining the results of the two integrals, we get:</p>
<div class="math notranslate nohighlight">
\[\int (x + 1) \, dx = \frac{x^2}{2} + C_1 + x + C_2\]</div>
<p>Since <span class="math notranslate nohighlight">\(C_1\)</span> and <span class="math notranslate nohighlight">\(C_2\)</span> are arbitrary constants, their sum is also an arbitrary constant, which we can denote as <span class="math notranslate nohighlight">\(C\)</span>:</p>
<div class="math notranslate nohighlight">
\[\int (x + 1) \, dx = \frac{x^2}{2} + x + C\]</div>
<p>Therefore, the indefinite integral of <span class="math notranslate nohighlight">\(x+1\)</span> is <span class="math notranslate nohighlight">\(\frac{x^2}{2} + x + C\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Function to plot and show derivative as tangent</span>
<span class="k">def</span><span class="w"> </span><span class="nf">visualize_derivative</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">/</span> <span class="n">h</span>
    <span class="n">tangent</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">slope</span> <span class="o">*</span> <span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span>

    <span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
    <span class="n">y_vals</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)</span>
    <span class="n">tangent_vals</span> <span class="o">=</span> <span class="n">tangent</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Point (x, f(x)) = (</span><span class="si">{</span><span class="n">x</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">y</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">tangent_vals</span><span class="p">,</span> <span class="s1">&#39;--g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Tangent at x=</span><span class="si">{</span><span class="n">x</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">, slope=</span><span class="si">{</span><span class="n">slope</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Visualizing Derivative as Tangent&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">y_vals</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">y_vals</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Function to approximate integral using rectangles</span>
<span class="k">def</span><span class="w"> </span><span class="nf">visualize_integral</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
    <span class="n">x_mid</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">y_mid</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">x_mid</span><span class="p">)</span>
    <span class="n">area</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_mid</span> <span class="o">*</span> <span class="n">dx</span><span class="p">)</span>

    <span class="n">x_fine</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
    <span class="n">y_fine</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">x_fine</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_fine</span><span class="p">,</span> <span class="n">y_fine</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_mid</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">dx</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Approximate Area = </span><span class="si">{</span><span class="n">area</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> (n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Visualizing Integral as Area Under Curve&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Example function: f(x) = x^2</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Visualize the derivative at x = 2</span>
<span class="n">visualize_derivative</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Visualize the integral of f(x) from 0 to 3</span>
<span class="n">visualize_integral</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/880bac75538e31073b12564019f538072547bfa6ff8c6523f82801ac7bc5b3f6.png" src="_images/880bac75538e31073b12564019f538072547bfa6ff8c6523f82801ac7bc5b3f6.png" />
<img alt="_images/749020443553ca85c78c3e4314f891bbe846bdb9e3c969b14ec77288faeeba69.png" src="_images/749020443553ca85c78c3e4314f891bbe846bdb9e3c969b14ec77288faeeba69.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Desired velocity</span>
<span class="n">v_star</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Define cost and its gradient</span>
<span class="k">def</span><span class="w"> </span><span class="nf">J</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">t</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">v_star</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">dJ</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">t</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">v_star</span><span class="p">)</span>

<span class="c1"># Plot J(t)</span>
<span class="n">t_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">J_vals</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">t_vals</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_vals</span><span class="p">,</span> <span class="n">J_vals</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$J(t)=(2t+2-v^*)^2$&#39;</span><span class="p">)</span>

<span class="c1"># Gradient descent path</span>
<span class="n">t</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">path</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="p">]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">t</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dJ</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">path</span><span class="p">)),</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;GD steps&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Gradient Descent on Velocity‐Error Cost&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$t$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$J(t)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/33a561e22754bce9bd932800355681e828762a9e6299a4b7d499da3294810118.png" src="_images/33a561e22754bce9bd932800355681e828762a9e6299a4b7d499da3294810118.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Define the cost function and its derivative</span>
<span class="k">def</span><span class="w"> </span><span class="nf">C</span><span class="p">(</span><span class="n">q</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">5</span><span class="o">*</span><span class="n">q</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">40</span><span class="o">*</span><span class="n">q</span> <span class="o">+</span> <span class="mi">200</span>

<span class="k">def</span><span class="w"> </span><span class="nf">dC</span><span class="p">(</span><span class="n">q</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">10</span><span class="o">*</span><span class="n">q</span> <span class="o">-</span> <span class="mi">40</span>

<span class="c1"># Define quantity values</span>
<span class="n">q_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">cost_vals</span> <span class="o">=</span> <span class="n">C</span><span class="p">(</span><span class="n">q_vals</span><span class="p">)</span>

<span class="c1"># Plot the cost function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">q_vals</span><span class="p">,</span> <span class="n">cost_vals</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$C(q) = 5q^2 - 40q + 200$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>

<span class="c1"># Mark the minimum point</span>
<span class="n">q_min</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">q_min</span><span class="p">],</span> <span class="p">[</span><span class="n">C</span><span class="p">(</span><span class="n">q_min</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Minimum Cost at $q=4$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">q_min</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>

<span class="c1"># Tangent line at q = 6</span>
<span class="n">q_tangent</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">slope</span> <span class="o">=</span> <span class="n">dC</span><span class="p">(</span><span class="n">q_tangent</span><span class="p">)</span>
<span class="n">y_tangent</span> <span class="o">=</span> <span class="n">C</span><span class="p">(</span><span class="n">q_tangent</span><span class="p">)</span>
<span class="n">q_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">q_tangent</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">q_tangent</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">tangent_line</span> <span class="o">=</span> <span class="n">slope</span> <span class="o">*</span> <span class="p">(</span><span class="n">q_range</span> <span class="o">-</span> <span class="n">q_tangent</span><span class="p">)</span> <span class="o">+</span> <span class="n">y_tangent</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">q_range</span><span class="p">,</span> <span class="n">tangent_line</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Tangent at $q=6$&#39;</span><span class="p">)</span>

<span class="c1"># Gradient descent visualization</span>
<span class="n">q</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">q_path</span> <span class="o">=</span> <span class="p">[</span><span class="n">q</span><span class="p">]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dC</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
    <span class="n">q_path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">q_path</span><span class="p">,</span> <span class="n">C</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">q_path</span><span class="p">)),</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Gradient Descent Steps&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">)</span>

<span class="c1"># Final plot settings</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Business Cost Function and Gradient Descent&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Quantity $q$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost $C(q)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c37dac91732fb8022f4d2525403e30dc6a33011a8f8e79e57b01fd0e91b3385b.png" src="_images/c37dac91732fb8022f4d2525403e30dc6a33011a8f8e79e57b01fd0e91b3385b.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.animation</span><span class="w"> </span><span class="kn">import</span> <span class="n">FuncAnimation</span><span class="p">,</span> <span class="n">PillowWriter</span>

<span class="c1"># Cost function and derivative</span>
<span class="k">def</span><span class="w"> </span><span class="nf">J</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">dJ</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># Hyperparameters</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">theta</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Record history for animation</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta</span><span class="p">]</span>
<span class="n">costs</span> <span class="o">=</span> <span class="p">[</span><span class="n">J</span><span class="p">(</span><span class="n">theta</span><span class="p">)]</span>

<span class="c1"># Perform gradient descent</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dJ</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">thetas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">J</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

<span class="c1"># Setup figure and axis</span>
<span class="n">theta_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">cost_vals</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span> <span class="n">cost_vals</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$J(\theta) = (\theta - 3)^2$&#39;</span><span class="p">)</span>
<span class="n">point</span><span class="p">,</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([],</span> <span class="p">[],</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Current Step&#39;</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">cost_vals</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$J(\theta)$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Gradient Descent Minimizing Squared Error&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Initialization function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">():</span>
    <span class="n">point</span><span class="o">.</span><span class="n">set_data</span><span class="p">([],</span> <span class="p">[])</span>
    <span class="n">text</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">point</span><span class="p">,</span> <span class="n">text</span>

<span class="c1"># Update function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="n">frame</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">thetas</span><span class="p">[</span><span class="n">frame</span><span class="p">]]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">costs</span><span class="p">[</span><span class="n">frame</span><span class="p">]]</span>
    <span class="n">point</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">text</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Step: </span><span class="si">{</span><span class="n">frame</span><span class="si">}</span><span class="se">\n</span><span class="s1">θ: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="se">\n</span><span class="s1">J(θ): </span><span class="si">{</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">point</span><span class="p">,</span> <span class="n">text</span>

<span class="c1"># Create animation</span>
<span class="n">ani</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">update</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">thetas</span><span class="p">),</span> <span class="n">init_func</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>


<span class="c1"># 10. Save as GIF</span>
<span class="n">ani</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;gradientdescent.gif&#39;</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="n">PillowWriter</span><span class="p">(</span><span class="n">fps</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="c1"># 11. Display saved GIF inside notebook</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;gradientdescent.gif&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b537c692b58277ca989fecbd78001f56469be1f992f5f37914239d3f8749cf81.gif" src="_images/b537c692b58277ca989fecbd78001f56469be1f992f5f37914239d3f8749cf81.gif" />
<img alt="_images/b4c97a0a3e727bc2761e226f79acbf03feec202203fc7395234ef047c6748d3e.png" src="_images/b4c97a0a3e727bc2761e226f79acbf03feec202203fc7395234ef047c6748d3e.png" />
</div>
</div>
<section id="id8">
<h4>Gradient descent<a class="headerlink" href="#id8" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Requires the calculation of <span class="math notranslate nohighlight">\(m\)</span> gradients in each step.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.animation</span><span class="w"> </span><span class="kn">import</span> <span class="n">FuncAnimation</span><span class="p">,</span> <span class="n">PillowWriter</span>

<span class="c1"># Cost function and derivative</span>
<span class="k">def</span><span class="w"> </span><span class="nf">J</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">dJ</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># Hyperparameters</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">theta</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">thetas</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta</span><span class="p">]</span>
<span class="n">costs</span> <span class="o">=</span> <span class="p">[</span><span class="n">J</span><span class="p">(</span><span class="n">theta</span><span class="p">)]</span>

<span class="c1"># Gradient descent</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dJ</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">thetas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">J</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

<span class="c1"># Plot setup</span>
<span class="n">theta_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">cost_vals</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span> <span class="n">cost_vals</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$J(\theta) = (\theta - 3)^2$&#39;</span><span class="p">)</span>
<span class="n">point</span><span class="p">,</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([],</span> <span class="p">[],</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Current Step&#39;</span><span class="p">)</span>
<span class="n">arrow</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">cost_vals</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$J(\theta)$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Gradient Descent Direction Toward Minimum&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Init function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">():</span>
    <span class="n">point</span><span class="o">.</span><span class="n">set_data</span><span class="p">([],</span> <span class="p">[])</span>
    <span class="n">arrow</span><span class="o">.</span><span class="n">set_position</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">arrow</span><span class="o">.</span><span class="n">xy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">text</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">point</span><span class="p">,</span> <span class="n">arrow</span><span class="p">,</span> <span class="n">text</span>

<span class="c1"># Update function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="n">frame</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">thetas</span><span class="p">[</span><span class="n">frame</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">costs</span><span class="p">[</span><span class="n">frame</span><span class="p">]</span>
    <span class="n">point</span><span class="o">.</span><span class="n">set_data</span><span class="p">([</span><span class="n">x</span><span class="p">],</span> <span class="p">[</span><span class="n">y</span><span class="p">])</span>

    <span class="c1"># Draw arrow if not last frame</span>
    <span class="k">if</span> <span class="n">frame</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">thetas</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">next_x</span> <span class="o">=</span> <span class="n">thetas</span><span class="p">[</span><span class="n">frame</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">next_y</span> <span class="o">=</span> <span class="n">costs</span><span class="p">[</span><span class="n">frame</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">arrow</span><span class="o">.</span><span class="n">xy</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_x</span><span class="p">,</span> <span class="n">next_y</span><span class="p">)</span>
        <span class="n">arrow</span><span class="o">.</span><span class="n">set_position</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">arrow</span><span class="o">.</span><span class="n">xy</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">arrow</span><span class="o">.</span><span class="n">set_position</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

    <span class="n">text</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step: </span><span class="si">{</span><span class="n">frame</span><span class="si">}</span><span class="se">\n</span><span class="s2">θ: </span><span class="si">{</span><span class="n">x</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\n</span><span class="s2">J(θ): </span><span class="si">{</span><span class="n">y</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">point</span><span class="p">,</span> <span class="n">arrow</span><span class="p">,</span> <span class="n">text</span>

<span class="c1"># Animate</span>
<span class="n">ani</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">update</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">thetas</span><span class="p">),</span> <span class="n">init_func</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="c1"># 10. Save as GIF</span>
<span class="n">ani</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;gradientdescentarrow.gif&#39;</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="n">PillowWriter</span><span class="p">(</span><span class="n">fps</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="c1"># 11. Display saved GIF inside notebook</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;gradientdescentarrow.gif&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/eaae875d6e62d006c20fc62bd07a573159e5c29b67274d2d81a23a1b5d518da4.gif" src="_images/eaae875d6e62d006c20fc62bd07a573159e5c29b67274d2d81a23a1b5d518da4.gif" />
<img alt="_images/814981ae298c958e54600439a51f0cb57b70e3458a24a03dd6866cbdbd993001.png" src="_images/814981ae298c958e54600439a51f0cb57b70e3458a24a03dd6866cbdbd993001.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.animation</span><span class="w"> </span><span class="kn">import</span> <span class="n">FuncAnimation</span><span class="p">,</span> <span class="n">PillowWriter</span>

<span class="c1"># Define the true function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">true_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1"># Generate some data points</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">true_function</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

<span class="c1"># Define the figure and axes</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>

<span class="c1"># Plot the data points</span>
<span class="n">scatter</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">)</span>

<span class="c1"># Create an empty line object for the predicted line</span>
<span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([],</span> <span class="p">[],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Create empty line objects for the error lines</span>
<span class="n">error_lines</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_data</span><span class="p">)):</span>
    <span class="n">err_line</span><span class="p">,</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([],</span> <span class="p">[],</span> <span class="s1">&#39;g--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">error_lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">err_line</span><span class="p">)</span>

<span class="c1"># Function to initialize the animation</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">():</span>
    <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">([],</span> <span class="p">[])</span>
    <span class="k">for</span> <span class="n">err_line</span> <span class="ow">in</span> <span class="n">error_lines</span><span class="p">:</span>
        <span class="n">err_line</span><span class="o">.</span><span class="n">set_data</span><span class="p">([],</span> <span class="p">[])</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="o">*</span><span class="n">error_lines</span><span class="p">)</span>

<span class="c1"># Function to update the animation frame</span>
<span class="k">def</span><span class="w"> </span><span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="c1"># Define the predicted function with varying slope and intercept</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">predicted_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">intercept</span>

    <span class="c1"># Define slope and intercept for the current frame</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mf">0.2</span>

    <span class="c1"># Calculate predicted y values</span>
    <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">predicted_function</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">)</span>

    <span class="c1"># Update the line data</span>
    <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">)</span>

    <span class="c1"># Update error lines</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">err_line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">error_lines</span><span class="p">):</span>
        <span class="n">err_line</span><span class="o">.</span><span class="n">set_data</span><span class="p">([</span><span class="n">x_data</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">x_data</span><span class="p">[</span><span class="n">j</span><span class="p">]],</span> <span class="p">[</span><span class="n">y_data</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">y_predicted</span><span class="p">[</span><span class="n">j</span><span class="p">]])</span>
    
    <span class="k">return</span> <span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="o">*</span><span class="n">error_lines</span><span class="p">)</span>

<span class="c1"># Create the animation</span>
<span class="n">ani</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">25</span><span class="p">),</span> <span class="n">init_func</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 10. Save as GIF</span>
<span class="n">ani</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;squared_error.gif&#39;</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="n">PillowWriter</span><span class="p">(</span><span class="n">fps</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="c1"># 11. Display saved GIF inside notebook</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;squared_error.gif&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/90eadbc7dc98d63496114ca502b877f1875a529ce5a2dab628441a8cfec2604e.gif" src="_images/90eadbc7dc98d63496114ca502b877f1875a529ce5a2dab628441a8cfec2604e.gif" />
<img alt="_images/47496e4c20b13f64cdc0e0f24174ccd094e7e7badf9e5b8c8772995dbe2b8eaa.png" src="_images/47496e4c20b13f64cdc0e0f24174ccd094e7e7badf9e5b8c8772995dbe2b8eaa.png" />
</div>
</div>
<img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/02_least_squares.png" alt="ml" style="margin: 0 auto; width: 750px;"/>
</section>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="an-objective-mean-squared-error">
<h1>An Objective: Mean Squared Error<a class="headerlink" href="#an-objective-mean-squared-error" title="Link to this heading">#</a></h1>
<p>We pick <span class="math notranslate nohighlight">\(\theta\)</span> to minimize the mean squared error (MSE). Slight variants of this objective are also known as the residual sum of squares (RSS) or the sum of squared residuals (SSR).
$<span class="math notranslate nohighlight">\(J(\theta)= \frac{1}{2n} \sum_{i=1}^n(y^{(i)}-\theta^\top x^{(i)})^2\)</span><span class="math notranslate nohighlight">\(
In other words, we are looking for the best compromise in \)</span>\theta$ over all the data points.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="mean-squared-error-partial-derivatives">
<h1>Mean Squared Error: Partial Derivatives<a class="headerlink" href="#mean-squared-error-partial-derivatives" title="Link to this heading">#</a></h1>
<p>Let’s work out the derivatives for <span class="math notranslate nohighlight">\(\frac{1}{2} \left( f_\theta(x^{(i)}) - y^{(i)} \right)^2,\)</span> the MSE of a linear model <span class="math notranslate nohighlight">\(f_\theta\)</span> for one training example <span class="math notranslate nohighlight">\((x^{(i)}, y^{(i)})\)</span>, which we denote <span class="math notranslate nohighlight">\(J^{(i)}(\theta)\)</span>.</p>
<p>\begin{align*}
\frac{\partial}{\partial \theta_j} J^{(i)}(\theta) &amp; = \frac{\partial}{\partial \theta_j} \left(\frac{1}{2} \left( f_\theta(x^{(i)}) - y^{(i)} \right)^2\right) \
&amp; = \left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot \frac{\partial}{\partial \theta_j} \left( f_\theta(x^{(i)}) - y^{(i)} \right) \
&amp; = \left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot \frac{\partial}{\partial \theta_j} \left( \sum_{k=0}^d \theta_k \cdot x^{(i)}<em>k - y^{(i)} \right) \
&amp; = \left( f</em>\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}_j
\end{align*}</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="mean-squared-error-the-gradient">
<h1>Mean Squared Error: The Gradient<a class="headerlink" href="#mean-squared-error-the-gradient" title="Link to this heading">#</a></h1>
<p>We can use this derivation to obtain an expression for the gradient of the MSE for a linear model</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="begin-align-small-tiny-nabla-theta-j-i-theta-begin-bmatrix-frac-partial-j-i-theta-partial-theta-0-frac-partial-j-i-theta-partial-theta-1-vdots-frac-partial-j-i-theta-partial-theta-d-end-bmatrix">
<h1>\begin{align*}
\small
{\tiny \nabla_\theta J^{(i)} (\theta)} = \begin{bmatrix}
\frac{\partial J^{(i)}(\theta)}{\partial \theta_0} \
\frac{\partial J^{(i)}(\theta)}{\partial \theta_1} \
\vdots \
\frac{\partial J^{(i)}(\theta)}{\partial \theta_d}
\end{bmatrix}<a class="headerlink" href="#begin-align-small-tiny-nabla-theta-j-i-theta-begin-bmatrix-frac-partial-j-i-theta-partial-theta-0-frac-partial-j-i-theta-partial-theta-1-vdots-frac-partial-j-i-theta-partial-theta-d-end-bmatrix" title="Link to this heading">#</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="begin-bmatrix-left-f-theta-x-i-y-i-right-cdot-x-i-0-left-f-theta-x-i-y-i-right-cdot-x-i-1-vdots-left-f-theta-x-i-y-i-right-cdot-x-i-d-end-bmatrix">
<h1>\begin{bmatrix}
\left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}<em>0 \
\left( f</em>\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}<em>1 \
\vdots \
\left( f</em>\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}_d
\end{bmatrix}<a class="headerlink" href="#begin-bmatrix-left-f-theta-x-i-y-i-right-cdot-x-i-0-left-f-theta-x-i-y-i-right-cdot-x-i-1-vdots-left-f-theta-x-i-y-i-right-cdot-x-i-d-end-bmatrix" title="Link to this heading">#</a></h1>
<p>\left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}
\end{align*}</p>
<p>Note that the MSE over the entire dataset is <span class="math notranslate nohighlight">\(J(\theta) = \frac{1}{n}\sum_{i=1}^n J^{(i)}(\theta)\)</span>. Therefore:</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="begin-align-nabla-theta-j-theta-begin-bmatrix-frac-partial-j-theta-partial-theta-0-frac-partial-j-theta-partial-theta-1-vdots-frac-partial-j-theta-partial-theta-d-end-bmatrix">
<h1>\begin{align*}
\nabla_\theta J (\theta) = \begin{bmatrix}
\frac{\partial J(\theta)}{\partial \theta_0} \
\frac{\partial J(\theta)}{\partial \theta_1} \
\vdots \
\frac{\partial J(\theta)}{\partial \theta_d}
\end{bmatrix}<a class="headerlink" href="#begin-align-nabla-theta-j-theta-begin-bmatrix-frac-partial-j-theta-partial-theta-0-frac-partial-j-theta-partial-theta-1-vdots-frac-partial-j-theta-partial-theta-d-end-bmatrix" title="Link to this heading">#</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="frac-1-n-sum-i-1-n-begin-bmatrix-frac-partial-j-i-theta-partial-theta-0-frac-partial-j-i-theta-partial-theta-1-vdots-frac-partial-j-i-theta-partial-theta-d-end-bmatrix">
<h1>\frac{1}{n}\sum_{i=1}^n
\begin{bmatrix}
\frac{\partial J^{(i)}(\theta)}{\partial \theta_0} \
\frac{\partial J^{(i)}(\theta)}{\partial \theta_1} \
\vdots \
\frac{\partial J^{(i)}(\theta)}{\partial \theta_d}
\end{bmatrix}<a class="headerlink" href="#frac-1-n-sum-i-1-n-begin-bmatrix-frac-partial-j-i-theta-partial-theta-0-frac-partial-j-i-theta-partial-theta-1-vdots-frac-partial-j-i-theta-partial-theta-d-end-bmatrix" title="Link to this heading">#</a></h1>
<p>\frac{1}{n} \sum_{i=1}^n \left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}
\end{align*}</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="ordinary-least-squares">
<h1>Ordinary Least Squares<a class="headerlink" href="#ordinary-least-squares" title="Link to this heading">#</a></h1>
<p>The gradient <span class="math notranslate nohighlight">\(\nabla_\theta f\)</span> further extends the derivative to multivariate functions <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span>, and is defined at a point <span class="math notranslate nohighlight">\(\theta_0\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{split} \nabla_\theta f (\theta_0) = \begin{bmatrix}
\frac{\partial f(\theta_0)}{\partial \theta_1} \\
\frac{\partial f(\theta_0)}{\partial \theta_2} \\
\vdots \\
\frac{\partial f(\theta_0)}{\partial \theta_d}
\end{bmatrix}.\end{split}\]</div>
<p>In other words, the <span class="math notranslate nohighlight">\(j\)</span>-th entry of the vector <span class="math notranslate nohighlight">\(\nabla_\theta f (\theta_0)\)</span> is the partial derivative <span class="math notranslate nohighlight">\(\frac{\partial f(\theta_0)}{\partial \theta_j}\)</span> of <span class="math notranslate nohighlight">\(f\)</span> with respect to the <span class="math notranslate nohighlight">\(j\)</span>-th component of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="notation-design-matrix">
<h1>Notation: Design Matrix<a class="headerlink" href="#notation-design-matrix" title="Link to this heading">#</a></h1>
<!-- Suppose that we have a dataset of size $n$ (e.g., $n$ patients), indexed by $i=1,2,...,n$. Each $x^{(i)}$ is a vector of $d$ features. -->
<p>Machine learning algorithms are most easily defined in the language of linear algebra. Therefore, it will be useful to represent the entire dataset as one matrix <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times d}\)</span>, of the form:</p>
<div class="math notranslate nohighlight">
\[\begin{split} X = \begin{bmatrix}
x^{(1)}_1 &amp; x^{(1)}_2 &amp; \ldots &amp; x^{(1)}_d \\
x^{(2)}_1 &amp; x^{(2)}_2 &amp; \ldots &amp; x^{(2)}_d \\
\vdots \\
x^{(n)}_1 &amp; x^{(n)}_2 &amp; \ldots &amp; x^{(n)}_d
\end{bmatrix}
=
\begin{bmatrix}
- &amp; (x^{(1)})^\top &amp; - \\
- &amp; (x^{(2)})^\top &amp; - \\
&amp; \vdots &amp; \\
- &amp; (x^{(n)})^\top &amp; - \\
\end{bmatrix}
.\end{split}\]</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="notation-target-vector">
<h1>Notation: Target Vector<a class="headerlink" href="#notation-target-vector" title="Link to this heading">#</a></h1>
<p>Similarly, we can vectorize the target variables into a vector <span class="math notranslate nohighlight">\(y \in \mathbb{R}^n\)</span> of the form
$<span class="math notranslate nohighlight">\( y = \begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(n)}
\end{bmatrix}.\)</span>$</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="squared-error-in-matrix-form">
<h1>Squared Error in Matrix Form<a class="headerlink" href="#squared-error-in-matrix-form" title="Link to this heading">#</a></h1>
<p>Recall that we may fit a linear model by choosing <span class="math notranslate nohighlight">\(\theta\)</span> that minimizes the squared error:
$<span class="math notranslate nohighlight">\(J(\theta)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^\top x^{(i)})^2\)</span>$</p>
<p>We can write this sum in matrix-vector form as:
$<span class="math notranslate nohighlight">\(J(\theta) = \frac{1}{2} (y-X\theta)^\top(y-X\theta) = \frac{1}{2} \|y-X\theta\|^2,\)</span><span class="math notranslate nohighlight">\(
where \)</span>X<span class="math notranslate nohighlight">\( is the design matrix and \)</span>|\cdot|$ denotes the Euclidean norm.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="the-gradient-of-the-squared-error">
<h1>The Gradient of the Squared Error<a class="headerlink" href="#the-gradient-of-the-squared-error" title="Link to this heading">#</a></h1>
<p>We can compute the gradient of the mean squared error as follows.</p>
<p>\begin{align*}
\nabla_\theta J(\theta)
&amp; = \nabla_\theta \frac{1}{2} (X \theta - y)^\top  (X \theta - y) \
&amp; = \frac{1}{2} \nabla_\theta \left( (X \theta)^\top  (X \theta) - (X \theta)^\top y - y^\top (X \theta) + y^\top y \right) \
&amp; = \frac{1}{2} \nabla_\theta \left( \theta^\top  (X^\top X) \theta - 2(X \theta)^\top y \right) \
&amp; = \frac{1}{2} \left( 2(X^\top X) \theta - 2X^\top y \right) \
&amp; = (X^\top X) \theta - X^\top y
\end{align*}</p>
<p>We used the facts that <span class="math notranslate nohighlight">\(a^\top b = b^\top a\)</span> (line 3), that <span class="math notranslate nohighlight">\(\nabla_x b^\top x = b\)</span> (line 4), and that <span class="math notranslate nohighlight">\(\nabla_x x^\top A x = 2 A x\)</span> for a symmetric matrix <span class="math notranslate nohighlight">\(A\)</span> (line 4).</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="normal-equations">
<h1>Normal Equations<a class="headerlink" href="#normal-equations" title="Link to this heading">#</a></h1>
<!-- We know from calculus that a function is minimized when its derivative is set to zero. In our case, our objective function is a (multivariate) quadratic; hence it only has one minimum, which is the global minimum.
 -->
<p>Setting the above derivative to zero, we obtain the <em>normal equations</em>:
$<span class="math notranslate nohighlight">\( (X^\top X) \theta = X^\top y.\)</span>$</p>
<p>Hence, the value <span class="math notranslate nohighlight">\(\theta^*\)</span> that minimizes this objective is given by:
$<span class="math notranslate nohighlight">\( \theta^* = (X^\top X)^{-1} X^\top y.\)</span>$</p>
<p>Note that we assumed that the matrix <span class="math notranslate nohighlight">\((X^\top X)\)</span> is invertible; we will soon see a simple way of dealing with non-invertible matrices.</p>
<section id="why-square-the-difference">
<h2>Why square the difference?<a class="headerlink" href="#why-square-the-difference" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>The error will be positive</p></li>
<li><p>If you take the absolute function (to cover point 1), the absolute function isn’t differentiable at the origin. Hence, we square the error.</p></li>
</ol>
</section>
<section id="why-frac-1-2m-instead-of-frac-1-m">
<h2> Why <span class="math notranslate nohighlight">\(\frac{1}{2m}\)</span> instead of <span class="math notranslate nohighlight">\(\frac{1}{m}\)</span> ?<a class="headerlink" href="#why-frac-1-2m-instead-of-frac-1-m" title="Link to this heading">#</a></h2>
<p>As we will see later, when we differentiate the squared error, the <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> will cancel out. If we don’t do that we’ll be stuck with a <span class="math notranslate nohighlight">\(2\)</span> in the equation which is useless.</p>
</section>
<section id="minimising-the-cost-function">
<h2>Minimising the cost function<a class="headerlink" href="#minimising-the-cost-function" title="Link to this heading">#</a></h2>
<p>Our objective function is</p>
</section>
<section id="displaystyle-operatorname-argmin-theta-j-theta">
<h2>$<span class="math notranslate nohighlight">\(\displaystyle \operatorname*{argmin}_\theta J(\theta)\)</span>$<a class="headerlink" href="#displaystyle-operatorname-argmin-theta-j-theta" title="Link to this heading">#</a></h2>
<p>Which simply means, find the value of <span class="math notranslate nohighlight">\(\theta\)</span> that minimises the error function <span class="math notranslate nohighlight">\(J(\theta)\)</span>. In order to do that, we will differentiate our cost function. When we differentiate it, it will give us gradient, which is the direction in which the error will be reduced. Upon having the gradient, we will simply update our <span class="math notranslate nohighlight">\(\theta\)</span> values to reflect that step (a step in the direction of lower error)</p>
<p>So, the update rule is the following equation</p>
</section>
<section id="theta-theta-alpha-frac-partial-partial-theta-j-theta">
<h2>$<span class="math notranslate nohighlight">\(\theta = \theta - \alpha \frac{\partial}{\partial \theta} J(\theta)\)</span>$<a class="headerlink" href="#theta-theta-alpha-frac-partial-partial-theta-j-theta" title="Link to this heading">#</a></h2>
<p>Where,</p>
<p><span class="math notranslate nohighlight">\(\alpha\)</span> = learning rate. Which is the rate at which we will travel to the direction of the lower error.</p>
<p>This process is nothing but <strong>Gradient Descent</strong>. There are few version of gradient descent, few of them are:</p>
<ol class="arabic simple">
<li><p><strong>Batch Gradient Descent</strong>: Go through <strong>all</strong> your input samples, compute the gradient once, and then update <span class="math notranslate nohighlight">\(\theta\)</span>s.</p></li>
<li><p><strong>Stochastic Gradient Descent</strong>: Go through a <strong>single</strong> sample, compute gradient, update <span class="math notranslate nohighlight">\(\theta\)</span>s, repeat <span class="math notranslate nohighlight">\(m\)</span> times</p></li>
<li><p><strong>Mini Batch Gradient Descent</strong>: Go through a <strong>batch</strong> of <span class="math notranslate nohighlight">\(k\)</span> samples, compute gradient, update <span class="math notranslate nohighlight">\(\theta\)</span>s, repeat <span class="math notranslate nohighlight">\(\frac{m}{k}\)</span> times.</p></li>
</ol>
</section>
<section id="differentiating-the-loss-function">
<h2>Differentiating the loss function:<a class="headerlink" href="#differentiating-the-loss-function" title="Link to this heading">#</a></h2>
<p>In the update rule:</p>
</section>
<section id="id9">
<h2>$<span class="math notranslate nohighlight">\(\theta = \theta - \alpha \frac{\partial}{\partial \theta} J(\theta)\)</span>$<a class="headerlink" href="#id9" title="Link to this heading">#</a></h2>
<p>The important part is calculating the derivative. Since we have two variables, we will have two derivatives, one for <span class="math notranslate nohighlight">\(\theta_0\)</span> and another for <span class="math notranslate nohighlight">\(\theta_1\)</span>.</p>
<p>So the first equation is:</p>
<p><span class="math notranslate nohighlight">\(
\begin{align}
\notag
\theta_0 &amp;= \theta_0 - \frac{\partial}{\partial \theta_0} J(\theta) \\
\notag
&amp;= \theta_0 -  \frac{\partial}{\partial \theta_0}(\frac{1}{2m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)^2}) \\
\notag
&amp;= \theta_0 -  \frac{2}{2m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)}\frac{\partial}{\partial \theta_0}{(h_\theta(x_i) - y_i)} \\
\notag
&amp;= \theta_0 -  \frac{1}{m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)}\frac{\partial}{\partial \theta_0}{(h_\theta(x_i) - y_i)} \\
\notag
&amp;= \theta_0 -  \frac{1}{m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)}\frac{\partial}{\partial \theta_0}{(\theta_0 + \theta_1x- y_i)} \\
\notag
&amp;= \theta_0 -  \frac{1}{m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)}(1 + 0 - 0)
\end{align}
\)</span></p>
<div class="math notranslate nohighlight">
\[\therefore \theta_0= \theta_0 -  \frac{1}{m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)} \]</div>
<section id="similarly-for-theta-1">
<h3>Similarly, for <span class="math notranslate nohighlight">\(\theta_1\)</span><a class="headerlink" href="#similarly-for-theta-1" title="Link to this heading">#</a></h3>
<p><span class="math notranslate nohighlight">\(
\begin{align}
\notag
\theta_1 &amp;= \theta_1 - \frac{\partial}{\partial \theta_1} J(\theta) \\
\notag
&amp;= \theta_1 -  \frac{\partial}{\partial \theta_1}(\frac{1}{2m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)^2}) \\
\notag
&amp;= \theta_1 -  \frac{2}{2m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)}\frac{\partial}{\partial \theta_0}{(h_\theta(x_i) - y_i)} \\
\notag
&amp;= \theta_1 -  \frac{1}{m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)}\frac{\partial}{\partial \theta_0}{(h_\theta(x_i) - y_i)} \\
\notag
&amp;= \theta_1 -  \frac{1}{m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)}\frac{\partial}{\partial \theta_0}{(\theta_0 + \theta_1x- y_i)} \\
\notag
&amp;= \theta_1 -  \frac{1}{m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)}(x + 0 - 0)
\end{align}
\)</span></p>
<div class="math notranslate nohighlight">
\[\therefore \theta_1= \theta_1 -  \frac{1}{m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)}(x) \]</div>
<p>We will implement <strong>Batch Gradient Descent</strong> i.e. we’ll update the gradients after 1 pass through the entire dataset. Our Algorithm hence becomes:</p>
</section>
</section>
<section id="repeat-till-convergence">
<h2>Repeat till convergence:<a class="headerlink" href="#repeat-till-convergence" title="Link to this heading">#</a></h2>
<section id="theta-0-theta-0-frac-1-m-sum-i-0-m-h-theta-x-i-y-i">
<h3>1.       <span class="math notranslate nohighlight">\(\theta_0= \theta_0 -  \frac{1}{m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)} \)</span><a class="headerlink" href="#theta-0-theta-0-frac-1-m-sum-i-0-m-h-theta-x-i-y-i" title="Link to this heading">#</a></h3>
</section>
<section id="theta-1-theta-1-frac-1-m-sum-i-0-m-h-theta-x-i-y-i-x">
<h3>2.       <span class="math notranslate nohighlight">\(\theta_1= \theta_1 -  \frac{1}{m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)}(x) \)</span><a class="headerlink" href="#theta-1-theta-1-frac-1-m-sum-i-0-m-h-theta-x-i-y-i-x" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span><span class="p">,</span> <span class="n">theta_prev</span> <span class="o">=</span> <span class="n">random_initialization</span><span class="p">()</span>
<span class="k">while</span> <span class="n">norm</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">theta_prev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">convergence_threshold</span><span class="p">:</span>
    <span class="n">theta_prev</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta_prev</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">(</span><span class="n">theta_prev</span><span class="p">)</span>
</pre></div>
</div>
<p>In the above algorithm, we stop when <span class="math notranslate nohighlight">\(||\theta_i - \theta_{i-1}||\)</span> is small.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.animation</span><span class="w"> </span><span class="kn">import</span> <span class="n">FuncAnimation</span><span class="p">,</span> <span class="n">PillowWriter</span>

<span class="c1"># 1. Generate synthetic data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">true_w</span> <span class="o">=</span> <span class="mf">2.5</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">true_w</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># 2. Define loss function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">compute_loss</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">X</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># 3. Precompute loss curve</span>
<span class="n">w_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">loss_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">w_values</span><span class="p">]</span>

<span class="c1"># 4. Gradient of the loss w.r.t w</span>
<span class="k">def</span><span class="w"> </span><span class="nf">compute_gradient</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">X</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">grad</span>

<span class="c1"># 5. Initialize</span>
<span class="n">w_current</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">w_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">w_current</span><span class="p">]</span>
<span class="n">loss_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">w_current</span><span class="p">)]</span>

<span class="c1"># 6. Simulate Gradient Descent Steps</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">w_current</span><span class="p">)</span>
    <span class="n">w_current</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>
    <span class="n">w_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w_current</span><span class="p">)</span>
    <span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">w_current</span><span class="p">))</span>

<span class="c1"># 7. Plot setup</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w_values</span><span class="p">,</span> <span class="n">loss_values</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Loss Curve&#39;</span><span class="p">)</span>
<span class="n">point</span><span class="p">,</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([],</span> <span class="p">[],</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Current Weight&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Weight (w)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Gradient Descent on Loss Curve&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># 8. Animation function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="n">frame</span><span class="p">):</span>
    <span class="n">point</span><span class="o">.</span><span class="n">set_data</span><span class="p">([</span><span class="n">w_history</span><span class="p">[</span><span class="n">frame</span><span class="p">]],</span> <span class="p">[</span><span class="n">loss_history</span><span class="p">[</span><span class="n">frame</span><span class="p">]])</span>
    <span class="k">return</span> <span class="n">point</span><span class="p">,</span>

<span class="c1"># 9. Create animation</span>
<span class="n">ani</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">update</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">w_history</span><span class="p">),</span> <span class="n">interval</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 10. Save as GIF</span>
<span class="n">ani</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;gradient_descent.gif&#39;</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="n">PillowWriter</span><span class="p">(</span><span class="n">fps</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="c1"># 11. Display saved GIF inside notebook</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;gradient_descent.gif&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1405d04dcfef6b49a1ca08aef3d74bf6b86e89806cf4fd7061e44e756adc80ee.gif" src="_images/1405d04dcfef6b49a1ca08aef3d74bf6b86e89806cf4fd7061e44e756adc80ee.gif" />
</div>
</div>
<p><a class="reference external" href="http://127.0.0.1:8050/">http://127.0.0.1:8050/</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">plotly.graph_objects</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">go</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dash</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dash</span><span class="p">,</span> <span class="n">dcc</span><span class="p">,</span> <span class="n">html</span><span class="p">,</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Output</span>

<span class="c1"># Ackley function: complex, non-convex objective function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">objective_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">term1</span> <span class="o">=</span> <span class="o">-</span><span class="mi">20</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>
    <span class="n">term2</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">y</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">term1</span> <span class="o">+</span> <span class="n">term2</span> <span class="o">+</span> <span class="mi">20</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">e</span>

<span class="c1"># Gradient of the objective function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">dx_term1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
    <span class="n">dx_term2</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">dx_term1</span> <span class="o">+</span> <span class="n">dx_term2</span>

    <span class="n">dy_term1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y</span>
    <span class="n">dy_term2</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">dy</span> <span class="o">=</span> <span class="n">dy_term1</span> <span class="o">+</span> <span class="n">dy_term2</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">])</span>

<span class="c1"># Clip points to stay within bounds</span>
<span class="k">def</span><span class="w"> </span><span class="nf">clip_point</span><span class="p">(</span><span class="n">point</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">point</span><span class="p">,</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Compute average step length</span>
<span class="k">def</span><span class="w"> </span><span class="nf">average_step_length</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.0</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span> <span class="k">if</span> <span class="n">steps</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.0</span>

<span class="c1"># SGD with Momentum</span>
<span class="k">def</span><span class="w"> </span><span class="nf">sgd_momentum</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">point</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">velocity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">point</span><span class="p">)</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">[</span><span class="n">point</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">point</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">point</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">velocity</span> <span class="o">=</span> <span class="n">momentum</span> <span class="o">*</span> <span class="n">velocity</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="n">point</span> <span class="o">=</span> <span class="n">clip_point</span><span class="p">(</span><span class="n">point</span> <span class="o">+</span> <span class="n">velocity</span><span class="p">)</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">point</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">objective_function</span><span class="p">(</span><span class="n">point</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">point</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">history</span><span class="p">),</span> <span class="n">steps</span>

<span class="c1"># AdaGrad</span>
<span class="k">def</span><span class="w"> </span><span class="nf">adagrad</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">point</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">[</span><span class="n">point</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>
    <span class="n">squared_grad_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">point</span><span class="p">)</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">point</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">point</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">squared_grad_sum</span> <span class="o">+=</span> <span class="n">grad</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">adjusted_lr</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">squared_grad_sum</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
        <span class="n">point</span> <span class="o">=</span> <span class="n">clip_point</span><span class="p">(</span><span class="n">point</span> <span class="o">-</span> <span class="n">adjusted_lr</span> <span class="o">*</span> <span class="n">grad</span><span class="p">)</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">point</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">objective_function</span><span class="p">(</span><span class="n">point</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">point</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">history</span><span class="p">),</span> <span class="n">steps</span>

<span class="c1"># RMSProp</span>
<span class="k">def</span><span class="w"> </span><span class="nf">rmsprop</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">point</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">[</span><span class="n">point</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>
    <span class="n">squared_grad_avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">point</span><span class="p">)</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">point</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">point</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">squared_grad_avg</span> <span class="o">=</span> <span class="n">decay_rate</span> <span class="o">*</span> <span class="n">squared_grad_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">point</span> <span class="o">=</span> <span class="n">clip_point</span><span class="p">(</span><span class="n">point</span> <span class="o">-</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">squared_grad_avg</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">))</span> <span class="o">*</span> <span class="n">grad</span><span class="p">)</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">point</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">objective_function</span><span class="p">(</span><span class="n">point</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">point</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">history</span><span class="p">),</span> <span class="n">steps</span>

<span class="c1"># Adam</span>
<span class="k">def</span><span class="w"> </span><span class="nf">adam</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">point</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">[</span><span class="n">point</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">point</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">point</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">point</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">point</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">m_hat</span> <span class="o">=</span> <span class="n">m</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="o">**</span><span class="n">t</span><span class="p">)</span>
        <span class="n">v_hat</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="o">**</span><span class="n">t</span><span class="p">)</span>
        <span class="n">point</span> <span class="o">=</span> <span class="n">clip_point</span><span class="p">(</span><span class="n">point</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">m_hat</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v_hat</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">))</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">point</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">objective_function</span><span class="p">(</span><span class="n">point</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">point</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">history</span><span class="p">),</span> <span class="n">steps</span>

<span class="c1"># Create Plotly figure</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_plotly_figure</span><span class="p">(</span><span class="n">sgd_path</span><span class="p">,</span> <span class="n">adagrad_path</span><span class="p">,</span> <span class="n">rmsprop_path</span><span class="p">,</span> <span class="n">adam_path</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">objective_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

    <span class="n">sgd_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">objective_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">sgd_path</span><span class="p">])</span>
    <span class="n">adagrad_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">objective_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">adagrad_path</span><span class="p">])</span>
    <span class="n">rmsprop_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">objective_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">rmsprop_path</span><span class="p">])</span>
    <span class="n">adam_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">objective_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">adam_path</span><span class="p">])</span>

    <span class="n">surface</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Surface</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="n">Z</span><span class="p">,</span>
        <span class="n">colorscale</span><span class="o">=</span><span class="s1">&#39;Viridis&#39;</span><span class="p">,</span>
        <span class="n">opacity</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
        <span class="n">colorbar</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Function Value&#39;</span><span class="p">,</span>
            <span class="n">x</span><span class="o">=</span><span class="mf">1.05</span><span class="p">,</span>  <span class="c1"># Position colorbar to the right</span>
            <span class="n">xanchor</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span>
            <span class="nb">len</span><span class="o">=</span><span class="mf">0.6</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="n">sgd_trace</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">sgd_path</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">sgd_path</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">z</span><span class="o">=</span><span class="n">sgd_z</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines+markers&#39;</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;SGD (Momentum)&#39;</span><span class="p">,</span>
        <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">adagrad_trace</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">adagrad_path</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">adagrad_path</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">z</span><span class="o">=</span><span class="n">adagrad_z</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines+markers&#39;</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;AdaGrad&#39;</span><span class="p">,</span>
        <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">rmsprop_trace</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">rmsprop_path</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">rmsprop_path</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">z</span><span class="o">=</span><span class="n">rmsprop_z</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines+markers&#39;</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;RMSProp&#39;</span><span class="p">,</span>
        <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">adam_trace</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">adam_path</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">adam_path</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">z</span><span class="o">=</span><span class="n">adam_z</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines+markers&#39;</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span>
        <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;yellow&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">start_point</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">sgd_path</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">sgd_path</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">z</span><span class="o">=</span><span class="p">[</span><span class="n">sgd_z</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
        <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;markers&#39;</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Start&#39;</span><span class="p">,</span>
        <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">symbol</span><span class="o">=</span><span class="s1">&#39;circle&#39;</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">global_minimum</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;markers&#39;</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Global Minimum&#39;</span><span class="p">,</span>
        <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">symbol</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">updatemenus</span> <span class="o">=</span> <span class="p">[</span>
        <span class="nb">dict</span><span class="p">(</span>
            <span class="n">buttons</span><span class="o">=</span><span class="p">[</span>
                <span class="nb">dict</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;visible&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">]}],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;All&#39;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;update&#39;</span><span class="p">),</span>
                <span class="nb">dict</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;visible&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">]}],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SGD (Momentum)&#39;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;update&#39;</span><span class="p">),</span>
                <span class="nb">dict</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;visible&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">]}],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;AdaGrad&#39;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;update&#39;</span><span class="p">),</span>
                <span class="nb">dict</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;visible&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">]}],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;RMSProp&#39;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;update&#39;</span><span class="p">),</span>
                <span class="nb">dict</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;visible&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">]}],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;update&#39;</span><span class="p">),</span>
            <span class="p">],</span>
            <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;down&#39;</span><span class="p">,</span>
            <span class="n">showactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">x</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
            <span class="n">xanchor</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span>
            <span class="n">y</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span>
            <span class="n">yanchor</span><span class="o">=</span><span class="s1">&#39;top&#39;</span>
        <span class="p">)</span>
    <span class="p">]</span>

    <span class="n">layout</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Layout</span><span class="p">(</span>
        <span class="n">title</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span>
        <span class="n">scene</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">xaxis_title</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span>
            <span class="n">yaxis_title</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span>
            <span class="n">zaxis_title</span><span class="o">=</span><span class="s1">&#39;f(x, y)&#39;</span><span class="p">,</span>
            <span class="n">aspectmode</span><span class="o">=</span><span class="s1">&#39;manual&#39;</span><span class="p">,</span>
            <span class="n">aspectratio</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="p">),</span>
        <span class="n">showlegend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">legend</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>  <span class="c1"># Move legend to avoid colorbar</span>
            <span class="n">y</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
            <span class="n">xanchor</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span>
            <span class="n">yanchor</span><span class="o">=</span><span class="s1">&#39;top&#39;</span>
        <span class="p">),</span>
        <span class="n">updatemenus</span><span class="o">=</span><span class="n">updatemenus</span><span class="p">,</span>
        <span class="n">margin</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>  <span class="c1"># Add right margin to accommodate colorbar</span>
    <span class="p">)</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">surface</span><span class="p">,</span> <span class="n">sgd_trace</span><span class="p">,</span> <span class="n">adagrad_trace</span><span class="p">,</span> <span class="n">rmsprop_trace</span><span class="p">,</span> <span class="n">adam_trace</span><span class="p">,</span> <span class="n">start_point</span><span class="p">,</span> <span class="n">global_minimum</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">layout</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fig</span>

<span class="c1"># Initialize Dash app</span>
<span class="n">app</span> <span class="o">=</span> <span class="n">Dash</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="c1"># Layout of the Dash app</span>
<span class="n">app</span><span class="o">.</span><span class="n">layout</span> <span class="o">=</span> <span class="n">html</span><span class="o">.</span><span class="n">Div</span><span class="p">([</span>
    <span class="n">html</span><span class="o">.</span><span class="n">H1</span><span class="p">(</span><span class="s2">&quot;Gradient Descent Optimization on Ackley Function&quot;</span><span class="p">),</span>
    <span class="n">html</span><span class="o">.</span><span class="n">Div</span><span class="p">([</span>
        <span class="n">html</span><span class="o">.</span><span class="n">Label</span><span class="p">(</span><span class="s2">&quot;Learning Rate:&quot;</span><span class="p">),</span>
        <span class="n">dcc</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s1">&#39;learning-rate&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;number&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
    <span class="p">]),</span>
    <span class="n">html</span><span class="o">.</span><span class="n">Div</span><span class="p">([</span>
        <span class="n">html</span><span class="o">.</span><span class="n">Label</span><span class="p">(</span><span class="s2">&quot;Number of Iterations:&quot;</span><span class="p">),</span>
        <span class="n">dcc</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s1">&#39;num-iterations&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;number&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="p">]),</span>
    <span class="n">html</span><span class="o">.</span><span class="n">Div</span><span class="p">([</span>
        <span class="n">html</span><span class="o">.</span><span class="n">Label</span><span class="p">(</span><span class="s2">&quot;Momentum (SGD):&quot;</span><span class="p">),</span>
        <span class="n">dcc</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;number&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span>
    <span class="p">]),</span>
    <span class="n">html</span><span class="o">.</span><span class="n">Div</span><span class="p">([</span>
        <span class="n">html</span><span class="o">.</span><span class="n">Label</span><span class="p">(</span><span class="s2">&quot;Decay Rate (RMSProp):&quot;</span><span class="p">),</span>
        <span class="n">dcc</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s1">&#39;decay-rate&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;number&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span>
    <span class="p">]),</span>
    <span class="n">html</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="s1">&#39;Run Optimization&#39;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s1">&#39;run-button&#39;</span><span class="p">,</span> <span class="n">n_clicks</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">dcc</span><span class="o">.</span><span class="n">Graph</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s1">&#39;optimization-plot&#39;</span><span class="p">),</span>
    <span class="n">html</span><span class="o">.</span><span class="n">Div</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s1">&#39;steps-output&#39;</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Callback to update plot and steps</span>
<span class="nd">@app</span><span class="o">.</span><span class="n">callback</span><span class="p">(</span>
    <span class="p">[</span><span class="n">Output</span><span class="p">(</span><span class="s1">&#39;optimization-plot&#39;</span><span class="p">,</span> <span class="s1">&#39;figure&#39;</span><span class="p">),</span>
     <span class="n">Output</span><span class="p">(</span><span class="s1">&#39;steps-output&#39;</span><span class="p">,</span> <span class="s1">&#39;children&#39;</span><span class="p">)],</span>
    <span class="p">[</span><span class="n">Input</span><span class="p">(</span><span class="s1">&#39;run-button&#39;</span><span class="p">,</span> <span class="s1">&#39;n_clicks&#39;</span><span class="p">)],</span>
    <span class="p">[</span><span class="n">Input</span><span class="p">(</span><span class="s1">&#39;learning-rate&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">),</span>
     <span class="n">Input</span><span class="p">(</span><span class="s1">&#39;num-iterations&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">),</span>
     <span class="n">Input</span><span class="p">(</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">),</span>
     <span class="n">Input</span><span class="p">(</span><span class="s1">&#39;decay-rate&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">)]</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">update_plot</span><span class="p">(</span><span class="n">n_clicks</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">):</span>
    <span class="n">start_point</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]</span>
    <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">0.01</span>

    <span class="k">if</span> <span class="n">learning_rate</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">num_iterations</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">momentum</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">decay_rate</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sgd_path</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">start_point</span><span class="p">])</span>
        <span class="n">adagrad_path</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">start_point</span><span class="p">])</span>
        <span class="n">rmsprop_path</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">start_point</span><span class="p">])</span>
        <span class="n">adam_path</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">start_point</span><span class="p">])</span>
        <span class="n">steps_text</span> <span class="o">=</span> <span class="s2">&quot;Please provide valid inputs.&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sgd_path</span><span class="p">,</span> <span class="n">sgd_steps</span> <span class="o">=</span> <span class="n">sgd_momentum</span><span class="p">(</span><span class="n">start_point</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">),</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">tolerance</span><span class="p">)</span>
        <span class="n">adagrad_path</span><span class="p">,</span> <span class="n">adagrad_steps</span> <span class="o">=</span> <span class="n">adagrad</span><span class="p">(</span><span class="n">start_point</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">),</span> <span class="n">tolerance</span><span class="o">=</span><span class="n">tolerance</span><span class="p">)</span>
        <span class="n">rmsprop_path</span><span class="p">,</span> <span class="n">rmsprop_steps</span> <span class="o">=</span> <span class="n">rmsprop</span><span class="p">(</span><span class="n">start_point</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">),</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="n">tolerance</span><span class="p">)</span>
        <span class="n">adam_path</span><span class="p">,</span> <span class="n">adam_steps</span> <span class="o">=</span> <span class="n">adam</span><span class="p">(</span><span class="n">start_point</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">),</span> <span class="n">tolerance</span><span class="o">=</span><span class="n">tolerance</span><span class="p">)</span>

        <span class="c1"># Compute final function values and average step lengths</span>
        <span class="n">sgd_final</span> <span class="o">=</span> <span class="n">objective_function</span><span class="p">(</span><span class="n">sgd_path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sgd_path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">adagrad_final</span> <span class="o">=</span> <span class="n">objective_function</span><span class="p">(</span><span class="n">adagrad_path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">adagrad_path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">rmsprop_final</span> <span class="o">=</span> <span class="n">objective_function</span><span class="p">(</span><span class="n">rmsprop_path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">rmsprop_path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">adam_final</span> <span class="o">=</span> <span class="n">objective_function</span><span class="p">(</span><span class="n">adam_path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">adam_path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="n">sgd_step_length</span> <span class="o">=</span> <span class="n">average_step_length</span><span class="p">(</span><span class="n">sgd_path</span><span class="p">)</span>
        <span class="n">adagrad_step_length</span> <span class="o">=</span> <span class="n">average_step_length</span><span class="p">(</span><span class="n">adagrad_path</span><span class="p">)</span>
        <span class="n">rmsprop_step_length</span> <span class="o">=</span> <span class="n">average_step_length</span><span class="p">(</span><span class="n">rmsprop_path</span><span class="p">)</span>
        <span class="n">adam_step_length</span> <span class="o">=</span> <span class="n">average_step_length</span><span class="p">(</span><span class="n">adam_path</span><span class="p">)</span>

        <span class="n">steps_text</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">html</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SGD with Momentum: </span><span class="si">{</span><span class="n">sgd_steps</span><span class="si">}</span><span class="s2"> steps, Final f(x, y) = </span><span class="si">{</span><span class="n">sgd_final</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Avg Step Length = </span><span class="si">{</span><span class="n">sgd_step_length</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">),</span>
            <span class="n">html</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;AdaGrad: </span><span class="si">{</span><span class="n">adagrad_steps</span><span class="si">}</span><span class="s2"> steps, Final f(x, y) = </span><span class="si">{</span><span class="n">adagrad_final</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Avg Step Length = </span><span class="si">{</span><span class="n">adagrad_step_length</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">),</span>
            <span class="n">html</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;RMSProp: </span><span class="si">{</span><span class="n">rmsprop_steps</span><span class="si">}</span><span class="s2"> steps, Final f(x, y) = </span><span class="si">{</span><span class="n">rmsprop_final</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Avg Step Length = </span><span class="si">{</span><span class="n">rmsprop_step_length</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">),</span>
            <span class="n">html</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adam: </span><span class="si">{</span><span class="n">adam_steps</span><span class="si">}</span><span class="s2"> steps, Final f(x, y) = </span><span class="si">{</span><span class="n">adam_final</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Avg Step Length = </span><span class="si">{</span><span class="n">adam_step_length</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">create_plotly_figure</span><span class="p">(</span><span class="n">sgd_path</span><span class="p">,</span> <span class="n">adagrad_path</span><span class="p">,</span> <span class="n">rmsprop_path</span><span class="p">,</span> <span class="n">adam_path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fig</span><span class="p">,</span> <span class="n">steps_text</span>

<span class="c1"># Run the app</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">app</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">debug</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="100%"
            height="650"
            src="http://127.0.0.1:8050/"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.animation</span><span class="w"> </span><span class="kn">import</span> <span class="n">FuncAnimation</span><span class="p">,</span> <span class="n">PillowWriter</span>

<span class="c1"># Define the true function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">true_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1"># Generate some data points</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">true_function</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

<span class="c1"># Define the figure and axes</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>

<span class="c1"># Plot the data points</span>
<span class="n">scatter</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">)</span>

<span class="c1"># Create an empty line object for the predicted line</span>
<span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([],</span> <span class="p">[],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Create empty line objects for the error lines</span>
<span class="n">error_lines</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_data</span><span class="p">)):</span>
    <span class="n">err_line</span><span class="p">,</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([],</span> <span class="p">[],</span> <span class="s1">&#39;g--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">error_lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">err_line</span><span class="p">)</span>

<span class="c1"># Create a vertical line for minimum loss</span>
<span class="n">min_loss_line</span><span class="p">,</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([],</span> <span class="p">[],</span> <span class="s1">&#39;b--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Min Loss&#39;</span><span class="p">)</span>

<span class="c1"># Create a text annotation for loss value</span>
<span class="n">loss_text</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">)</span>

<span class="c1"># Store loss values and parameters</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">slopes</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">intercepts</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Function to calculate squared loss</span>
<span class="k">def</span><span class="w"> </span><span class="nf">calculate_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Function to initialize the animation</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">():</span>
    <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">([],</span> <span class="p">[])</span>
    <span class="n">min_loss_line</span><span class="o">.</span><span class="n">set_data</span><span class="p">([],</span> <span class="p">[])</span>
    <span class="k">for</span> <span class="n">err_line</span> <span class="ow">in</span> <span class="n">error_lines</span><span class="p">:</span>
        <span class="n">err_line</span><span class="o">.</span><span class="n">set_data</span><span class="p">([],</span> <span class="p">[])</span>
    <span class="n">loss_text</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">min_loss_line</span><span class="p">,</span> <span class="o">*</span><span class="n">error_lines</span><span class="p">,</span> <span class="n">loss_text</span><span class="p">)</span>

<span class="c1"># Function to update the animation frame</span>
<span class="k">def</span><span class="w"> </span><span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="c1"># Define the predicted function with varying slope and intercept</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">predicted_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">intercept</span>

    <span class="c1"># Define slope and intercept for the current frame</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mf">0.2</span>

    <span class="c1"># Calculate predicted y values</span>
    <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">predicted_function</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">)</span>

    <span class="c1"># Calculate and store loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">calculate_loss</span><span class="p">(</span><span class="n">y_data</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">)</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">slopes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">slope</span><span class="p">)</span>
    <span class="n">intercepts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">intercept</span><span class="p">)</span>

    <span class="c1"># Update the line data</span>
    <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">)</span>

    <span class="c1"># Update error lines</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">err_line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">error_lines</span><span class="p">):</span>
        <span class="n">err_line</span><span class="o">.</span><span class="n">set_data</span><span class="p">([</span><span class="n">x_data</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">x_data</span><span class="p">[</span><span class="n">j</span><span class="p">]],</span> <span class="p">[</span><span class="n">y_data</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">y_predicted</span><span class="p">[</span><span class="n">j</span><span class="p">]])</span>

    <span class="c1"># Update minimum loss line</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">25</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">min_loss_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
        <span class="n">min_slope</span> <span class="o">=</span> <span class="n">slopes</span><span class="p">[</span><span class="n">min_loss_idx</span><span class="p">]</span>
        <span class="n">min_intercept</span> <span class="o">=</span> <span class="n">intercepts</span><span class="p">[</span><span class="n">min_loss_idx</span><span class="p">]</span>
        <span class="n">y_min</span> <span class="o">=</span> <span class="n">predicted_function</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">min_slope</span><span class="p">,</span> <span class="n">min_intercept</span><span class="p">)</span>
        <span class="n">min_loss_line</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_min</span><span class="p">)</span>

    <span class="c1"># Update loss text</span>
    <span class="n">loss_text</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Squared Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">min_loss_line</span><span class="p">,</span> <span class="o">*</span><span class="n">error_lines</span><span class="p">,</span> <span class="n">loss_text</span><span class="p">)</span>

<span class="c1"># Create the animation</span>
<span class="n">ani</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">25</span><span class="p">),</span> <span class="n">init_func</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Add legend</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Save as GIF</span>
<span class="n">ani</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;squared_error_with_min_loss.gif&#39;</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="n">PillowWriter</span><span class="p">(</span><span class="n">fps</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="c1"># Display saved GIF inside notebook</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;squared_error_with_min_loss.gif&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7e62c0fceb61a0af42e57714b6fd7d82372ed7ca7bdfe043e38933b916dd572c.gif" src="_images/7e62c0fceb61a0af42e57714b6fd7d82372ed7ca7bdfe043e38933b916dd572c.gif" />
<img alt="_images/21b3aa5b0d67a1a9f3f342242e5835a7464dfaf60ae29de9a180b1e7b3ddf8ae.png" src="_images/21b3aa5b0d67a1a9f3f342242e5835a7464dfaf60ae29de9a180b1e7b3ddf8ae.png" />
</div>
</div>
<p><a class="github reference external" href="https://github.com/jiupinjia/Visualize-Optimization-Algorithms">jiupinjia/Visualize-Optimization-Algorithms</a></p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="non-linear-least-squares">
<h1>Non-Linear Least Squares<a class="headerlink" href="#non-linear-least-squares" title="Link to this heading">#</a></h1>
<p>Ordinary Least Squares can only learn linear relationships in the data. Can we also use it to model more complex relationships?</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="polynomial-functions">
<h1>Polynomial Functions<a class="headerlink" href="#polynomial-functions" title="Link to this heading">#</a></h1>
<p>a polynomial of degree <span class="math notranslate nohighlight">\(p\)</span> is a function of the form
$<span class="math notranslate nohighlight">\(
a_p x^p + a_{p-1} x^{p-1} + ... + a_{1} x + a_0.
\)</span>$</p>
<p>Below are some examples of polynomial functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">x_vars</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Quadratic Function&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vars</span><span class="p">,</span> <span class="n">x_vars</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;$x^2$&quot;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cubic Function&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vars</span><span class="p">,</span> <span class="n">x_vars</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;$x^3$&quot;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Third Degree Polynomial&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vars</span><span class="p">,</span> <span class="n">x_vars</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x_vars</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x_vars</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;$x^3 + 2 x^2 + x + 1$&quot;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span> <span class="c1"># This helps prevent titles from overlapping</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/16d4ac79f4b1bd51c36cd5cd94278851cc157baa6fdfc835de8941c0d0482b01.png" src="_images/16d4ac79f4b1bd51c36cd5cd94278851cc157baa6fdfc835de8941c0d0482b01.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">x_vars</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cosine Function&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vars</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x_vars</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;$cos(x)$&quot;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Sine Function&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vars</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x_vars</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;$x^3$&quot;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Combination of Sines and Cosines&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vars</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x_vars</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x_vars</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">x_vars</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;$cos(x) + sin(2x) + cos(4x)$&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x10f36a540&gt;
</pre></div>
</div>
<img alt="_images/29dd0050d5bfc73654ad1fb2023ed29d7baec69565d6813c6076c2a64c244b3d.png" src="_images/29dd0050d5bfc73654ad1fb2023ed29d7baec69565d6813c6076c2a64c244b3d.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">generate_polynomial_features</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">degree</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generates polynomial features for a given input array.</span>

<span class="sd">    Args:</span>
<span class="sd">        X (np.ndarray): The input array of shape (n_samples,).</span>
<span class="sd">        degree (int): The degree of the polynomial.</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: A matrix of shape (n_samples, degree) where each column</span>
<span class="sd">                      represents X raised to the power of (1, 2, ..., degree).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">poly_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">degree</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">degree</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">poly_features</span><span class="p">[:,</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span> <span class="o">**</span> <span class="n">i</span>
    <span class="k">return</span> <span class="n">poly_features</span>

<span class="k">def</span><span class="w"> </span><span class="nf">fit_polynomial_regression</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fits a polynomial regression model using gradient descent.</span>

<span class="sd">    Args:</span>
<span class="sd">        X_poly (np.ndarray): The polynomial features of shape (n_samples, degree).</span>
<span class="sd">        y (np.ndarray): The target values of shape (n_samples,).</span>
<span class="sd">        learning_rate (float): The learning rate for gradient descent.</span>
<span class="sd">        n_iterations (int): The number of iterations for gradient descent.</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: The learned coefficients of the polynomial.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X_poly</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># Initialize coefficients with random values</span>
    <span class="n">coefficients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>
    <span class="c1"># Add a bias term (intercept) to the polynomial features</span>
    <span class="n">X_poly</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">X_poly</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">coefficients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="mi">0</span><span class="p">],</span> <span class="n">coefficients</span><span class="p">))</span> <span class="c1"># Initialize bias to 0</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
        <span class="c1"># Calculate predictions</span>
        <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">coefficients</span><span class="p">)</span>
        <span class="c1"># Calculate the error</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span>
        <span class="c1"># Calculate gradients</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_poly</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
        <span class="c1"># Update coefficients</span>
        <span class="n">coefficients</span> <span class="o">=</span> <span class="n">coefficients</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span>

    <span class="k">return</span> <span class="n">coefficients</span>

<span class="k">def</span><span class="w"> </span><span class="nf">predict_polynomial</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">coefficients</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Predicts values using the learned polynomial regression coefficients.</span>

<span class="sd">    Args:</span>
<span class="sd">        X (np.ndarray): The input array for prediction.</span>
<span class="sd">        coefficients (np.ndarray): The learned coefficients.</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: The predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">degree</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">coefficients</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">X_poly</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">degree</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">degree</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">X_poly</span><span class="p">[:,</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span> <span class="o">**</span> <span class="n">i</span>
    <span class="c1"># Add bias term</span>
    <span class="n">X_poly</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">X_poly</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">coefficients</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>
<span class="n">true_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">true_fn</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

<span class="n">degrees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">degree</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">degrees</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">degrees</span><span class="p">),</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Generate polynomial features</span>
    <span class="n">X_poly</span> <span class="o">=</span> <span class="n">generate_polynomial_features</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>

    <span class="c1"># Fit the polynomial regression model</span>
    <span class="n">coefficients</span> <span class="o">=</span> <span class="n">fit_polynomial_regression</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iterations</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

    <span class="c1"># Predict on the test data</span>
    <span class="n">X_test_poly</span> <span class="o">=</span> <span class="n">generate_polynomial_features</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>
    <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">predict_polynomial</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">coefficients</span><span class="p">)</span>

    <span class="c1"># Plotting</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">true_fn</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True function&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Model (Degree </span><span class="si">{</span><span class="n">degree</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Samples&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;best&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Polynomial of Degree </span><span class="si">{</span><span class="n">degree</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/270d0f54ae2bba8b076d60b122a94347ffe081ebd1453090cdc84905903b1d2c.png" src="_images/270d0f54ae2bba8b076d60b122a94347ffe081ebd1453090cdc84905903b1d2c.png" />
</div>
</div>
<p>Absolutely! Let’s dive into the intuition behind generating polynomial features.</p>
<p>Imagine you have a set of data points, and when you plot them, they clearly don’t fall on a straight line. Instead, they seem to follow a curve.</p>
<p><strong>The Limitation of Linear Regression:</strong></p>
<p>Linear regression, at its core, tries to find the best <em>straight line</em> that fits your data. It models the relationship between your input features (<span class="math notranslate nohighlight">\(X\)</span>) and your output (<span class="math notranslate nohighlight">\(y\)</span>) as:</p>
<div class="math notranslate nohighlight">
\[y \approx \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta_0, \beta_1, ..., \beta_n\)</span> are the coefficients it learns. This equation represents a hyperplane (a line in 2D, a plane in 3D, and so on). If the underlying relationship in your data is not linear, a straight line will likely do a poor job of capturing the pattern.</p>
<p><strong>The Idea of Polynomial Regression:</strong></p>
<p>Polynomial regression comes into play when you suspect a <em>non-linear</em> relationship between your variables. The fundamental idea is to transform your original input features into a higher-dimensional space by adding polynomial terms.</p>
<p><strong>How it Works (Intuition with a Single Feature):</strong></p>
<p>Let’s say you have a single input feature, <span class="math notranslate nohighlight">\(x\)</span>, and your data looks like a parabola (a U-shape). A simple linear model would be <span class="math notranslate nohighlight">\(y \approx \beta_0 + \beta_1 x\)</span>. This can only fit a straight line.</p>
<p>To capture the curvature, we can introduce a new feature: <span class="math notranslate nohighlight">\(x^2\)</span>. Now, our model becomes:</p>
<div class="math notranslate nohighlight">
\[y \approx \beta_0 + \beta_1 x + \beta_2 x^2\]</div>
<p>Notice that although the model itself is still <em>linear in terms of the coefficients</em> (<span class="math notranslate nohighlight">\(\beta_0, \beta_1, \beta_2\)</span>), it can now represent a parabolic curve because the input <span class="math notranslate nohighlight">\(x\)</span> is transformed non-linearly.</p>
<p><strong>Visualizing the Transformation:</strong></p>
<p>Think of it this way:</p>
<ol class="arabic simple">
<li><p><strong>Original Space:</strong> You have your data points in a 2D space with axes <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. The relationship is curved.</p></li>
<li><p><strong>Feature Transformation:</strong> You create a new “feature” by squaring the original <span class="math notranslate nohighlight">\(x\)</span> values. Now, for each data point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>, you can think of it as existing in a new 3D space with axes <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(x^2\)</span>, and <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p><strong>Linear Regression in the Transformed Space:</strong> In this new 3D space, the relationship between <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(x^2\)</span>, and <span class="math notranslate nohighlight">\(y\)</span> might be linear. Our linear regression algorithm finds a plane that best fits these points in the 3D space: <span class="math notranslate nohighlight">\(y \approx \beta_0 \cdot 1 + \beta_1 \cdot x + \beta_2 \cdot x^2\)</span>.</p></li>
<li><p><strong>Mapping Back to the Original Space:</strong> When you plot the resulting equation <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1 x + \beta_2 x^2\)</span> in the original 2D space (with axes <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>), you get a parabola!</p></li>
</ol>
<p><strong>Generalizing to Higher Degrees:</strong></p>
<p>You can extend this idea to even higher degree polynomials. For a degree-<span class="math notranslate nohighlight">\(d\)</span> polynomial with a single feature <span class="math notranslate nohighlight">\(x\)</span>, you would generate features <span class="math notranslate nohighlight">\(x^1, x^2, x^3, ..., x^d\)</span>. Your model would then be:</p>
<div class="math notranslate nohighlight">
\[y \approx \beta_0 + \beta_1 x + \beta_2 x^2 + ... + \beta_d x^d\]</div>
<p>By including these higher-order terms, the model gains the flexibility to fit more complex curves in your data.</p>
<p><strong>With Multiple Features:</strong></p>
<p>If you have multiple input features (<span class="math notranslate nohighlight">\(x_1, x_2, ...\)</span>), generating polynomial features involves creating terms that are products of the original features raised to various powers (up to the specified degree). For example, for degree 2 with two features, you’d have terms like:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_1^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_2^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_1 x_2\)</span></p></li>
</ul>
<p>This allows the model to capture not only the individual non-linear effects of each feature but also their interactions.</p>
<p><strong>In essence, generating polynomial features allows us to use the power of linear models to fit non-linear relationships by transforming the input data into a higher-dimensional space where the relationship might be linear.</strong> The linear regression algorithm then learns the coefficients in this transformed space, which, when mapped back to the original feature space, results in a polynomial curve.</p>
<p>Think of it like bending a straight ruler. A straight ruler can only fit straight lines. But if you could somehow manipulate your data so that the relationship <em>appears</em> straight in a different “bent” space, then a straight ruler (linear regression) could fit it perfectly in that space. When you unbend the space back to normal, the straight line becomes a curve. Polynomial feature generation is a way to mathematically “bend” the space of your features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Axes3D</span>  <span class="c1"># Import for 3D plotting</span>

<span class="c1"># Generate sample data with two features</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X_multi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">-</span> <span class="mi">2</span>  <span class="c1"># Features between -2 and 2</span>
<span class="n">y_multi</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

<span class="c1"># Fit a polynomial regression model (degree 2 for simplicity in visualization)</span>
<span class="n">degree_multi</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">poly_features_multi</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree_multi</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">X_poly_multi</span> <span class="o">=</span> <span class="n">poly_features_multi</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_multi</span><span class="p">)</span>
<span class="n">lin_reg_multi</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lin_reg_multi</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly_multi</span><span class="p">,</span> <span class="n">y_multi</span><span class="p">)</span>

<span class="c1"># Create a grid of x1 and x2 values for plotting the prediction surface</span>
<span class="n">x1_surf</span><span class="p">,</span> <span class="n">x2_surf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">50</span><span class="p">),</span>
                               <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">50</span><span class="p">))</span>

<span class="c1"># Create the polynomial features for the grid</span>
<span class="n">X_surf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">x1_surf</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">x2_surf</span><span class="o">.</span><span class="n">ravel</span><span class="p">()]</span>
<span class="n">X_poly_surf</span> <span class="o">=</span> <span class="n">poly_features_multi</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_surf</span><span class="p">)</span>

<span class="c1"># Predict the y values for the grid</span>
<span class="n">y_surf</span> <span class="o">=</span> <span class="n">lin_reg_multi</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_poly_surf</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x1_surf</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Create the 3D plot</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

<span class="c1"># Scatter plot of the data points</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_multi</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Data&#39;</span><span class="p">)</span>

<span class="c1"># Plot the regression surface</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x1_surf</span><span class="p">,</span> <span class="n">x2_surf</span><span class="p">,</span> <span class="n">y_surf</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Polynomial Regression (Degree </span><span class="si">{</span><span class="n">degree_multi</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Polynomial Regression with Two Features (Degree </span><span class="si">{</span><span class="n">degree_multi</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/45468064f414e3aa5b1d9d5511dc6ce23ed4ab096fc56ca5f94c2aa707f4d339.png" src="_images/45468064f414e3aa5b1d9d5511dc6ce23ed4ab096fc56ca5f94c2aa707f4d339.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.animation</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">animation</span>

<span class="c1"># ... (rest of your data generation and fitting code) ...</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_multi</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Data&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x1_surf</span><span class="p">,</span> <span class="n">x2_surf</span><span class="p">,</span> <span class="n">y_surf</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Polynomial Regression (Degree </span><span class="si">{</span><span class="n">degree_multi</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Polynomial Regression with Two Features (Degree </span><span class="si">{</span><span class="n">degree_multi</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">rotate</span><span class="p">(</span><span class="n">angle</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="n">angle</span><span class="p">)</span>

<span class="n">ani</span> <span class="o">=</span> <span class="n">animation</span><span class="o">.</span><span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">rotate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">interval</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># To save the animation (requires a writer like &#39;ffmpeg&#39; or &#39; PillowWriter&#39;):</span>
<span class="n">ani</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;rotation.gif&#39;</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="s1">&#39;pillow&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/45468064f414e3aa5b1d9d5511dc6ce23ed4ab096fc56ca5f94c2aa707f4d339.png" src="_images/45468064f414e3aa5b1d9d5511dc6ce23ed4ab096fc56ca5f94c2aa707f4d339.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;rotation.gif&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/49e6db234fc6072a14d69c1c7bf0bab94fb2a1dbf7e8d4f6f1404add2a885675.gif" src="_images/49e6db234fc6072a14d69c1c7bf0bab94fb2a1dbf7e8d4f6f1404add2a885675.gif" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.linalg</span><span class="w"> </span><span class="kn">import</span> <span class="n">lstsq</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span>

<span class="k">class</span><span class="w"> </span><span class="nc">PolynomialRegressionSVD</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">=</span> <span class="n">degree</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># Generate polynomial features</span>
        <span class="n">X_poly</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># Scale features (except bias term)</span>
        <span class="k">if</span> <span class="n">X_poly</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Only scale if there are features beyond the bias</span>
            <span class="n">X_poly</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_poly</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>

        <span class="c1"># Solve using SVD (via scipy.linalg.lstsq)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">lstsq</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">X_poly</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">poly</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">X_poly</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">X_poly</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_poly</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>
        <span class="k">return</span> <span class="n">X_poly</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span>

<span class="c1"># Example usage</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

    <span class="c1"># Generate sample data</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="o">-</span> <span class="mi">3</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">X</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="c1"># Test high-degree polynomial</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">PolynomialRegressionSVD</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Plot results</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_new</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="s2">&quot;m-&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Degree 10 Prediction&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;b.&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Data&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Polynomial Regression (Degree 10) with SVD&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/93/7lt42x5j7m39kz7wxbcghvrm0000gn/T/ipykernel_1061/1992350972.py:29: RuntimeWarning: divide by zero encountered in matmul
  return X_poly @ self.coef_
/var/folders/93/7lt42x5j7m39kz7wxbcghvrm0000gn/T/ipykernel_1061/1992350972.py:29: RuntimeWarning: overflow encountered in matmul
  return X_poly @ self.coef_
/var/folders/93/7lt42x5j7m39kz7wxbcghvrm0000gn/T/ipykernel_1061/1992350972.py:29: RuntimeWarning: invalid value encountered in matmul
  return X_poly @ self.coef_
</pre></div>
</div>
<img alt="_images/0af9a30e82c722aeb4d196a3d6ad356757bbc7e94d116dd2620674682c124ba6.png" src="_images/0af9a30e82c722aeb4d196a3d6ad356757bbc7e94d116dd2620674682c124ba6.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolynomialFeatures</span>

<span class="c1"># --- Polynomial Features with a Single Feature ---</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Polynomial Features with a Single Feature ---&quot;</span><span class="p">)</span>

<span class="c1"># Sample data with a single feature</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="o">-</span> <span class="mi">3</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">X</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="c1"># Let&#39;s explore polynomial features of different degrees for this single feature</span>
<span class="n">degrees_single</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">feature_names_single</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="n">degrees_single</span><span class="p">:</span>
    <span class="n">poly_features</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly_features</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Degree: </span><span class="si">{</span><span class="n">degree</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original features (first 5 samples):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Polynomial features (first 5 samples):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">X_poly</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Feature names after transformation:&quot;</span><span class="p">,</span> <span class="n">poly_features</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">(</span><span class="n">feature_names_single</span><span class="p">))</span>

<span class="c1"># Plotting the results of polynomial regression with a single feature</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">X_new_single</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">style</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">degree</span> <span class="ow">in</span> <span class="p">((</span><span class="s2">&quot;r-+&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;b--&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;g-&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;m-&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)):</span>
    <span class="n">polybig_features</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">std_scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
    <span class="n">lin_reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">polynomial_regression</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">polybig_features</span><span class="p">,</span> <span class="n">std_scaler</span><span class="p">,</span> <span class="n">lin_reg</span><span class="p">)</span>
    <span class="n">polynomial_regression</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">y_newbig</span> <span class="o">=</span> <span class="n">polynomial_regression</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new_single</span><span class="p">)</span>
    <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">degree</span><span class="si">}</span><span class="s2"> degree</span><span class="si">{</span><span class="s1">&#39;s&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">degree</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_new_single</span><span class="p">,</span> <span class="n">y_newbig</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="n">width</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;b.&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$y$&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Polynomial Regression with a Single Feature (Different Degrees)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># --- Polynomial Features with Multiple Features ---</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Polynomial Features with Multiple Features ---&quot;</span><span class="p">)</span>

<span class="c1"># Sample data with two features</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X_multi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">-</span> <span class="mi">2</span>  <span class="c1"># Features between -2 and 2</span>
<span class="n">y_multi</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">X_multi</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

<span class="c1"># Let&#39;s explore polynomial features of degree 2 for these two features</span>
<span class="n">degree_multi</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">poly_features_multi</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree_multi</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">X_poly_multi</span> <span class="o">=</span> <span class="n">poly_features_multi</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_multi</span><span class="p">)</span>
<span class="n">feature_names_multi</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;x1&quot;</span><span class="p">,</span> <span class="s2">&quot;x2&quot;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Degree: </span><span class="si">{</span><span class="n">degree_multi</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original features (first 5 samples):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">X_multi</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Polynomial features (first 5 samples):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">X_poly_multi</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Feature names after transformation:&quot;</span><span class="p">,</span> <span class="n">poly_features_multi</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">(</span><span class="n">feature_names_multi</span><span class="p">))</span>

<span class="c1"># To visualize the regression with multiple features, we&#39;d typically need a 3D plot.</span>
<span class="c1"># However, let&#39;s focus on understanding the feature transformation here.</span>

<span class="c1"># Example of how the features are generated for degree 2 with two original features (x1, x2):</span>
<span class="c1"># For degree 2, the features generated are:</span>
<span class="c1"># [x1, x2, x1^2, x1 x2, x2^2]</span>

<span class="c1"># Let&#39;s look at one sample to illustrate:</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">X_multi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Example for a single data point: </span><span class="si">{</span><span class="n">sample</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">transformed_sample</span> <span class="o">=</span> <span class="n">poly_features_multi</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Transformed features (degree </span><span class="si">{</span><span class="n">degree_multi</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">transformed_sample</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># These correspond to: [x1, x2, x1^2, x1*x2, x2^2]</span>

<span class="c1"># --- Visualizing Polynomial Regression with Two Features (Conceptual - Hard to plot directly in 2D) ---</span>
<span class="c1"># To visualize this properly, you would need a 3D scatter plot for the data points</span>
<span class="c1"># and a 3D surface for the regression plane (in the transformed space, which maps</span>
<span class="c1"># back to a more complex surface in the original x1-x2 space).</span>

<span class="c1"># The core idea remains: PolynomialFeatures expands the feature space, allowing a</span>
<span class="c1"># linear model to fit non-linear relationships in the original feature space.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Polynomial Features with a Single Feature ---

Degree: 1
Original features (first 5 samples):
 [[-0.75275929]
 [ 2.70428584]
 [ 1.39196365]
 [ 0.59195091]
 [-2.06388816]]
Polynomial features (first 5 samples):
 [[-0.75275929]
 [ 2.70428584]
 [ 1.39196365]
 [ 0.59195091]
 [-2.06388816]]
Feature names after transformation: [&#39;x&#39;]

Degree: 2
Original features (first 5 samples):
 [[-0.75275929]
 [ 2.70428584]
 [ 1.39196365]
 [ 0.59195091]
 [-2.06388816]]
Polynomial features (first 5 samples):
 [[-0.75275929  0.56664654]
 [ 2.70428584  7.3131619 ]
 [ 1.39196365  1.93756281]
 [ 0.59195091  0.35040587]
 [-2.06388816  4.25963433]]
Feature names after transformation: [&#39;x&#39; &#39;x^2&#39;]

Degree: 3
Original features (first 5 samples):
 [[-0.75275929]
 [ 2.70428584]
 [ 1.39196365]
 [ 0.59195091]
 [-2.06388816]]
Polynomial features (first 5 samples):
 [[-0.75275929  0.56664654 -0.42654845]
 [ 2.70428584  7.3131619  19.77688015]
 [ 1.39196365  1.93756281  2.697017  ]
 [ 0.59195091  0.35040587  0.20742307]
 [-2.06388816  4.25963433 -8.79140884]]
Feature names after transformation: [&#39;x&#39; &#39;x^2&#39; &#39;x^3&#39;]
</pre></div>
</div>
<img alt="_images/d13e7f64c7caf7f0369c0b4b80f3c457701d0917bbbe8d08cc65a4541222deb0.png" src="_images/d13e7f64c7caf7f0369c0b4b80f3c457701d0917bbbe8d08cc65a4541222deb0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Polynomial Features with Multiple Features ---

Degree: 2
Original features (first 5 samples):
 [[-0.50183952  1.80285723]
 [ 0.92797577  0.39463394]
 [-1.37592544 -1.37602192]
 [-1.76766555  1.46470458]
 [ 0.40446005  0.83229031]]
Polynomial features (first 5 samples):
 [[-0.50183952  1.80285723  0.25184291 -0.90474501  3.25029418]
 [ 0.92797577  0.39463394  0.86113902  0.36621073  0.15573594]
 [-1.37592544 -1.37602192  1.89317081  1.89330356  1.89343632]
 [-1.76766555  1.46470458  3.1246415  -2.58910783  2.14535952]
 [ 0.40446005  0.83229031  0.16358793  0.33662818  0.69270716]]
Feature names after transformation: [&#39;x1&#39; &#39;x2&#39; &#39;x1^2&#39; &#39;x1 x2&#39; &#39;x2^2&#39;]

Example for a single data point: [-0.50183952  1.80285723]
Transformed features (degree 2): [[-0.50183952  1.80285723  0.25184291 -0.90474501  3.25029418]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">class</span><span class="w"> </span><span class="nc">PolynomialRegressionGD</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">degree</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">=</span> <span class="n">degree</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>  <span class="c1"># Stopping tolerance</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_polynomial_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate polynomial features with better numerical stability.&quot;&quot;&quot;</span>
        <span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))]</span>  <span class="c1"># Start with bias term</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span> <span class="o">**</span> <span class="n">d</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_standard_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Standard scaling (mean=0, std=1) with numerical safeguards.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">std_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># Avoid division by zero and maintain numerical stability</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">std_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">std_</span> <span class="o">&lt;</span> <span class="mf">1e-8</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># Threshold for small std dev</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">std_</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># Generate polynomial features (includes bias term)</span>
        <span class="n">X_poly</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_polynomial_features</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># Scale features (excluding bias term)</span>
        <span class="n">X_poly</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_standard_scale</span><span class="p">(</span><span class="n">X_poly</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>

        <span class="n">m</span> <span class="o">=</span> <span class="n">X_poly</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X_poly</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="n">prev_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">):</span>
            <span class="c1"># Predictions and error (with numerical safeguards)</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">X_poly</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e100</span><span class="p">,</span> <span class="mf">1e100</span><span class="p">)</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span>

            <span class="c1"># Calculate MSE loss only (no regularization)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">error</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">error</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>

            <span class="c1"># Check for convergence</span>
            <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">prev_loss</span> <span class="o">-</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">prev_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># Calculate gradients (no regularization terms)</span>
            <span class="n">gradients</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">X_poly</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">error</span>

            <span class="c1"># Adaptive learning rate for stability</span>
            <span class="n">effective_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">-=</span> <span class="n">effective_lr</span> <span class="o">*</span> <span class="n">gradients</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">X_poly</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_polynomial_features</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X_poly</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_poly</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">std_</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">X_poly</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e100</span><span class="p">,</span> <span class="mf">1e100</span><span class="p">)</span>  <span class="c1"># Numerical safeguard</span>

<span class="c1"># Generate data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="o">-</span> <span class="mi">3</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">X</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="k">for</span> <span class="n">style</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">degree</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">&quot;r-+&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;b--&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;g-&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;m-&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;c-&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mf">0.0005</span><span class="p">)</span>
<span class="p">]:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">PolynomialRegressionGD</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_new</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">degree</span><span class="si">}</span><span class="s2"> degree&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;k.&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Polynomial Regression via Gradient Descent (No Regularization)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/93/7lt42x5j7m39kz7wxbcghvrm0000gn/T/ipykernel_1061/2382199848.py:39: RuntimeWarning: divide by zero encountered in matmul
  y_pred = np.clip(X_poly @ self.theta, -1e100, 1e100)
/var/folders/93/7lt42x5j7m39kz7wxbcghvrm0000gn/T/ipykernel_1061/2382199848.py:39: RuntimeWarning: overflow encountered in matmul
  y_pred = np.clip(X_poly @ self.theta, -1e100, 1e100)
/var/folders/93/7lt42x5j7m39kz7wxbcghvrm0000gn/T/ipykernel_1061/2382199848.py:39: RuntimeWarning: invalid value encountered in matmul
  y_pred = np.clip(X_poly @ self.theta, -1e100, 1e100)
/var/folders/93/7lt42x5j7m39kz7wxbcghvrm0000gn/T/ipykernel_1061/2382199848.py:51: RuntimeWarning: divide by zero encountered in matmul
  gradients = (2 / m) * X_poly.T @ error
/var/folders/93/7lt42x5j7m39kz7wxbcghvrm0000gn/T/ipykernel_1061/2382199848.py:51: RuntimeWarning: overflow encountered in matmul
  gradients = (2 / m) * X_poly.T @ error
/var/folders/93/7lt42x5j7m39kz7wxbcghvrm0000gn/T/ipykernel_1061/2382199848.py:51: RuntimeWarning: invalid value encountered in matmul
  gradients = (2 / m) * X_poly.T @ error
/var/folders/93/7lt42x5j7m39kz7wxbcghvrm0000gn/T/ipykernel_1061/2382199848.py:60: RuntimeWarning: divide by zero encountered in matmul
  return np.clip(X_poly @ self.theta, -1e100, 1e100)  # Numerical safeguard
/var/folders/93/7lt42x5j7m39kz7wxbcghvrm0000gn/T/ipykernel_1061/2382199848.py:60: RuntimeWarning: overflow encountered in matmul
  return np.clip(X_poly @ self.theta, -1e100, 1e100)  # Numerical safeguard
/var/folders/93/7lt42x5j7m39kz7wxbcghvrm0000gn/T/ipykernel_1061/2382199848.py:60: RuntimeWarning: invalid value encountered in matmul
  return np.clip(X_poly @ self.theta, -1e100, 1e100)  # Numerical safeguard
</pre></div>
</div>
<img alt="_images/706c013adf2ae043c460836aeb3109de58a2cc1d004ef605067bca9699941242.png" src="_images/706c013adf2ae043c460836aeb3109de58a2cc1d004ef605067bca9699941242.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Generate some sample data for demonstration</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">3</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">X</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Create data for plotting the predictions</span>
<span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="k">for</span> <span class="n">style</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">degree</span> <span class="ow">in</span> <span class="p">((</span><span class="s2">&quot;r-+&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;b--&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;g-&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;m&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)):</span>
    <span class="n">polybig_features</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">std_scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
    <span class="n">lin_reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">polynomial_regression</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">polybig_features</span><span class="p">,</span> <span class="n">std_scaler</span><span class="p">,</span> <span class="n">lin_reg</span><span class="p">)</span>
    <span class="n">polynomial_regression</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">y_newbig</span> <span class="o">=</span> <span class="n">polynomial_regression</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
    <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">degree</span><span class="si">}</span><span class="s2"> degree</span><span class="si">{</span><span class="s1">&#39;s&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">degree</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_new</span><span class="p">,</span> <span class="n">y_newbig</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="n">width</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;b.&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$y$&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Polynomial Regression with Different Degrees&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7985823efbd953494349b73ed609d27a57fccce8e95eedf9cbc67fa2a697c6e7.png" src="_images/7985823efbd953494349b73ed609d27a57fccce8e95eedf9cbc67fa2a697c6e7.png" />
</div>
</div>
<p>References of github</p>
<ol class="arabic simple">
<li><p>cornell-cs5785-2024-applied-ml</p></li>
<li><p>girafe-ai/ml-course</p></li>
<li><p>ageron/handson-ml3</p></li>
<li><p>maykulkarni/Machine-Learning-Notebooks</p></li>
</ol>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="symbols.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Common Math Symbols and Notations in Machine Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="2_Regularization.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Generalization, Overfitting, Regularization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Linear Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contents">Contents</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model-class-linear-model-family">Model Class: Linear Model Family</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation">Explanation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-mathematical-notations-in-markdown-latex">Examples of Mathematical Notations in Markdown (<span class="math notranslate nohighlight">\( LaTeX \)</span>)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-equations-brief-overview">Normal Equations (Brief Overview)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-calculation">Memory Calculation:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-gradient-descent">Why Gradient Descent?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-gradient-descent">Introduction to Gradient Descent</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differentiation-in-gradient-descent">1. Differentiation in Gradient Descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#squared-error-cost-function">2. Squared Error Cost Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-gradient-descent-and-squared-error">3. Combining Gradient Descent and Squared Error</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-of-the-squared-error">Derivative of the Squared Error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quick-intro-calculus-key-concepts-at-a-glance">💡 <strong>Quick Intro:</strong> Calculus: Key concepts at a glance…</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limits">1. Limits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuity">2. Continuity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#differentiation">3. Differentiation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minima-and-maxima">4. Minima and Maxima</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">5. Gradient Descent</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#position-velocity-acceleration">1. Position, Velocity, Acceleration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2. Limits</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">3. Continuity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">4. Differentiation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-continuous-example">5. Non-Continuous Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-connection">6. Gradient Descent Connection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-vs-limit-as-t-to-infty">7. Derivative vs. Limit as <span class="math notranslate nohighlight">\(t\to\infty\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-code-to-visualize-gradient-descent-on-j-t">8. Python Code to Visualize Gradient Descent on <span class="math notranslate nohighlight">\(J(t)\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#business-calculus-with-a-profit-function-example">Business Calculus with a Profit Function Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Limits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Continuity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-continuous-discontinuous-functions">Non-Continuous (Discontinuous) Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#differentiation-marginal-profit">Differentiation (Marginal Profit)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integration">Integration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Gradient Descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-derivative-is-a-local-slope">🌄 The Derivative Is a Local Slope</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-know-which-direction-leads-to-the-minimum">🧭 How Do We Know Which Direction Leads to the Minimum?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-example-walk-on-a-curve">🚶 Simple Example: Walk on a Curve</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-small-intervals">🔍 Why Small Intervals?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">📌 Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#origin-of-the-power-rule-for-differentiation">Origin of the Power Rule for Differentiation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#origin-of-the-power-rule-for-integration">Origin of the Power Rule for Integration</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Gradient descent</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#an-objective-mean-squared-error">An Objective: Mean Squared Error</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-partial-derivatives">Mean Squared Error: Partial Derivatives</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-the-gradient">Mean Squared Error: The Gradient</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#begin-align-small-tiny-nabla-theta-j-i-theta-begin-bmatrix-frac-partial-j-i-theta-partial-theta-0-frac-partial-j-i-theta-partial-theta-1-vdots-frac-partial-j-i-theta-partial-theta-d-end-bmatrix">\begin{align*}
\small
{\tiny \nabla_\theta J^{(i)} (\theta)} = \begin{bmatrix}
\frac{\partial J^{(i)}(\theta)}{\partial \theta_0} \
\frac{\partial J^{(i)}(\theta)}{\partial \theta_1} \
\vdots \
\frac{\partial J^{(i)}(\theta)}{\partial \theta_d}
\end{bmatrix}</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#begin-bmatrix-left-f-theta-x-i-y-i-right-cdot-x-i-0-left-f-theta-x-i-y-i-right-cdot-x-i-1-vdots-left-f-theta-x-i-y-i-right-cdot-x-i-d-end-bmatrix">\begin{bmatrix}
\left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}<em>0 \
\left( f</em>\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}<em>1 \
\vdots \
\left( f</em>\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}_d
\end{bmatrix}</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#begin-align-nabla-theta-j-theta-begin-bmatrix-frac-partial-j-theta-partial-theta-0-frac-partial-j-theta-partial-theta-1-vdots-frac-partial-j-theta-partial-theta-d-end-bmatrix">\begin{align*}
\nabla_\theta J (\theta) = \begin{bmatrix}
\frac{\partial J(\theta)}{\partial \theta_0} \
\frac{\partial J(\theta)}{\partial \theta_1} \
\vdots \
\frac{\partial J(\theta)}{\partial \theta_d}
\end{bmatrix}</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#frac-1-n-sum-i-1-n-begin-bmatrix-frac-partial-j-i-theta-partial-theta-0-frac-partial-j-i-theta-partial-theta-1-vdots-frac-partial-j-i-theta-partial-theta-d-end-bmatrix">\frac{1}{n}\sum_{i=1}^n
\begin{bmatrix}
\frac{\partial J^{(i)}(\theta)}{\partial \theta_0} \
\frac{\partial J^{(i)}(\theta)}{\partial \theta_1} \
\vdots \
\frac{\partial J^{(i)}(\theta)}{\partial \theta_d}
\end{bmatrix}</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-least-squares">Ordinary Least Squares</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#notation-design-matrix">Notation: Design Matrix</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#notation-target-vector">Notation: Target Vector</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#squared-error-in-matrix-form">Squared Error in Matrix Form</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gradient-of-the-squared-error">The Gradient of the Squared Error</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-equations">Normal Equations</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-square-the-difference">Why square the difference?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-frac-1-2m-instead-of-frac-1-m"> Why <span class="math notranslate nohighlight">\(\frac{1}{2m}\)</span> instead of <span class="math notranslate nohighlight">\(\frac{1}{m}\)</span> ?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimising-the-cost-function">Minimising the cost function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#displaystyle-operatorname-argmin-theta-j-theta">$<span class="math notranslate nohighlight">\(\displaystyle \operatorname*{argmin}_\theta J(\theta)\)</span>$</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theta-theta-alpha-frac-partial-partial-theta-j-theta">$<span class="math notranslate nohighlight">\(\theta = \theta - \alpha \frac{\partial}{\partial \theta} J(\theta)\)</span>$</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differentiating-the-loss-function">Differentiating the loss function:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">$<span class="math notranslate nohighlight">\(\theta = \theta - \alpha \frac{\partial}{\partial \theta} J(\theta)\)</span>$</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarly-for-theta-1">Similarly, for <span class="math notranslate nohighlight">\(\theta_1\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#repeat-till-convergence">Repeat till convergence:</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#theta-0-theta-0-frac-1-m-sum-i-0-m-h-theta-x-i-y-i">1.       <span class="math notranslate nohighlight">\(\theta_0= \theta_0 -  \frac{1}{m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)} \)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#theta-1-theta-1-frac-1-m-sum-i-0-m-h-theta-x-i-y-i-x">2.       <span class="math notranslate nohighlight">\(\theta_1= \theta_1 -  \frac{1}{m}\sum_{i=0}^m{(h_\theta(x_i) - y_i)}(x) \)</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-least-squares">Non-Linear Least Squares</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-functions">Polynomial Functions</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chandravesh Chaudhari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>