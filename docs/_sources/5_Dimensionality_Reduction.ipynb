{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dimensionality Reduction: PCA",
   "id": "fa101a91b5a85b34"
  },
  {
   "cell_type": "markdown",
   "id": "857466e0-1b76-4c02-b1d2-bb528bde6e22",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border-width:0; color:gray; background-color:gray\">\n",
    "\n",
    "<table width=\"100%\">\n",
    "  <tr>\n",
    "    <td align=\"left\" width=\"60%\">\n",
    "      <h2 style=\"color: #2c3e50; font-family: Arial, sans-serif;\">\n",
    "        Machine Learning for Business\n",
    "      </h2>\n",
    "      <p>\n",
    "        <strong>Chandravesh Chaudhari</strong><br>\n",
    "        Assistant Professor<br>\n",
    "        School of Business and Management<br>\n",
    "        <a href=\"mailto:chandraveshchaudhari@gmail.com\" style=\"color: #2980b9; text-decoration: none;\">\n",
    "          chandraveshchaudhari@gmail.com\n",
    "        </a>\n",
    "      </p>\n",
    "    </td>\n",
    "    <td align=\"right\" width=\"40%\">\n",
    "      <img src=\"logo.jpg\" alt=\"Christ University\" width=\"250\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<hr style=\"height:2px; border-width:0; color:gray; background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf5966a-bec9-49d6-8557-39695e90c2cd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\t• Variance Maximization & Eigen Decomposition\n",
    "\t• Singular Value Decomposition (SVD)\n",
    "\t• Reconstruction Error\n",
    "\t• Python: PCA using SVD\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dimensionality Reduction: PCA (Principal Component Analysis)\n",
    "PCA is a technique for reducing the dimensionality of a dataset while preserving as much variability (information) as possible. It transforms the data into a new coordinate system where the axes (principal components) are orthogonal and aligned with the directions of maximum variance.\n",
    "\n",
    "1. Variance Maximization & Eigen Decomposition\n",
    "Objective: Find the directions (principal components) in which the data has maximum variance.\n",
    "\t•\tSteps:\n",
    "\t1\tCenter the data by subtracting the mean of each feature.\n",
    "\t2\tCompute the covariance matrix of the centered data.\n",
    "\t3\tPerform eigen decomposition on the covariance matrix to obtain eigenvalues and eigenvectors.\n",
    "\t▪\tEigenvectors: Represent the directions of the principal components.\n",
    "\t▪\tEigenvalues: Represent the amount of variance explained by each principal component.\n",
    "\t4\tSort eigenvectors by their corresponding eigenvalues in descending order.\n",
    "\t5\tSelect the top ( k ) eigenvectors to form a projection matrix.\n",
    "\t6\tProject the data onto the new subspace to reduce dimensionality.\n",
    "\t•\tVariance Maximization:\n",
    "\t◦\tThe first principal component captures the direction of maximum variance.\n",
    "\t◦\tEach subsequent component captures the maximum remaining variance, orthogonal to the previous components.\n",
    "\t•\tMathematical Insight: Let ( X ) be the centered data matrix (( n \\times p ), where ( n ) is the number of samples and ( p ) is the number of features). The covariance matrix is: [ C = \\frac{1}{n-1} X^T X ] Eigen decomposition of ( C ) gives: [ C = V \\Lambda V^T ] where ( V ) is the matrix of eigenvectors, and ( \\Lambda ) is a diagonal matrix of eigenvalues.\n",
    "\n",
    "2. Singular Value Decomposition (SVD)\n",
    "SVD provides an alternative way to perform PCA, often more numerically stable and efficient, especially for large datasets.\n",
    "\t•\tSVD Decomposition: For a centered data matrix ( X ), SVD decomposes it as: [ X = U \\Sigma V^T ] where:\n",
    "\t◦\t( U ): Left singular vectors (( n \\times n )).\n",
    "\t◦\t( \\Sigma ): Diagonal matrix of singular values (( n \\times p )).\n",
    "\t◦\t( V ): Right singular vectors (( p \\times p )), which correspond to the principal component directions.\n",
    "\t◦\tSingular values in ( \\Sigma ) are related to the eigenvalues of the covariance matrix: ( \\sigma_i^2 / (n-1) = \\lambda_i ).\n",
    "\t•\tPCA via SVD:\n",
    "\t◦\tThe principal components are the columns of ( V ).\n",
    "\t◦\tThe variance of the data along each principal component is given by the squared singular values.\n",
    "\t◦\tTo reduce dimensionality, select the top ( k ) columns of ( V ) and project the data: [ X_{\\text{reduced}} = X V_k ] where ( V_k ) contains the first ( k ) columns of ( V ).\n",
    "\t•\tAdvantages of SVD:\n",
    "\t◦\tWorks directly on the data matrix, avoiding the need to compute the covariance matrix.\n",
    "\t◦\tNumerically stable for high-dimensional or sparse data.\n",
    "\n",
    "3. Reconstruction Error\n",
    "Reconstruction error measures how much information is lost when reducing the dimensionality of the data.\n",
    "\t•\tReconstruction: After projecting the data onto the top ( k ) principal components, the reconstructed data is: [ \\tilde{X} = X_{\\text{reduced}} V_k^T = X V_k V_k^T ] where ( V_k ) is the matrix of the top ( k ) eigenvectors.\n",
    "\t•\tReconstruction Error: The error is the difference between the original data ( X ) and the reconstructed data ( \\tilde{X} ): [ \\text{Error} = | X - \\tilde{X} |_F^2 ] where ( | \\cdot |_F ) is the Frobenius norm.\n",
    "\t•\tRelation to Variance: The reconstruction error is equal to the sum of the variances of the discarded principal components (i.e., the sum of the eigenvalues corresponding to the discarded eigenvectors): [ \\text{Error} = \\sum_{i=k+1}^p \\lambda_i ]\n",
    "\t•\tChoosing ( k ): Select ( k ) such that a sufficient percentage of the total variance (e.g., 95%) is retained: [ \\text{Explained Variance Ratio} = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^p \\lambda_i} ]\n",
    "\n",
    "4. Python: PCA using SVD\n",
    "Below is a Python implementation of PCA using SVD with NumPy and scikit-learn for comparison.\n",
    "Implementation from Scratch\n",
    "import numpy as np\n",
    "\n",
    "def pca_svd(X, k):\n",
    "    # Center the data\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    \n",
    "    # Perform SVD\n",
    "    U, Sigma, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "    \n",
    "    # Select top k components\n",
    "    V_k = Vt[:k, :]  # Principal component directions\n",
    "    X_reduced = X_centered @ V_k.T  # Project data onto k components\n",
    "    \n",
    "    # Explained variance\n",
    "    explained_variance = (Sigma**2) / (X.shape[0] - 1)\n",
    "    explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "    \n",
    "    # Reconstruct data\n",
    "    X_reconstructed = X_reduced @ V_k\n",
    "    \n",
    "    # Reconstruction error\n",
    "    reconstruction_error = np.sum((X_centered - X_reconstructed)**2)\n",
    "    \n",
    "    return X_reduced, explained_variance_ratio[:k], reconstruction_error\n",
    "\n",
    "# Example usage\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 5)  # 100 samples, 5 features\n",
    "k = 2  # Reduce to 2 dimensions\n",
    "X_reduced, explained_variance_ratio, reconstruction_error = pca_svd(X, k)\n",
    "\n",
    "print(\"Reduced Data Shape:\", X_reduced.shape)\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "print(\"Reconstruction Error:\", reconstruction_error)\n",
    "Using scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=k)\n",
    "X_reduced_sklearn = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Explained variance and reconstruction error\n",
    "explained_variance_ratio_sklearn = pca.explained_variance_ratio_\n",
    "X_reconstructed_sklearn = X_reduced_sklearn @ pca.components_ + pca.mean_\n",
    "reconstruction_error_sklearn = np.sum((X_scaled - X_reconstructed_sklearn)**2)\n",
    "\n",
    "print(\"scikit-learn Reduced Data Shape:\", X_reduced_sklearn.shape)\n",
    "print(\"scikit-learn Explained Variance Ratio:\", explained_variance_ratio_sklearn)\n",
    "print(\"scikit-learn Reconstruction Error:\", reconstruction_error_sklearn)\n",
    "Output Explanation\n",
    "\t•\tReduced Data: The transformed data in the lower-dimensional space (( n \\times k )).\n",
    "\t•\tExplained Variance Ratio: The proportion of total variance captured by each principal component.\n",
    "\t•\tReconstruction Error: The Frobenius norm of the difference between the original and reconstructed data.\n",
    "Notes\n",
    "\t•\tStandardizing the data (zero mean, unit variance) is recommended before applying PCA to ensure features are on the same scale.\n",
    "\t•\tSVD is more efficient than eigen decomposition for large datasets, as it avoids computing the covariance matrix explicitly.\n",
    "\t•\tThe scikit-learn implementation is optimized and handles edge cases, making it preferable for production use.\n",
    "\n",
    "Summary\n",
    "\t•\tVariance Maximization: PCA finds orthogonal directions that maximize variance, computed via eigen decomposition of the covariance matrix.\n",
    "\t•\tSVD: An efficient alternative to eigen decomposition, directly decomposing the data matrix.\n",
    "\t•\tReconstruction Error: Measures information loss, minimized by retaining more components.\n",
    "\t•\tPython Implementation: SVD-based PCA can be implemented from scratch or using scikit-learn for robustness.\n",
    "Let me know if you need further clarification or additional details!\n"
   ],
   "id": "c3c361c75ee1525e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd749c-8e72-4f3a-992e-2d0a76dd414e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here’s your content converted to Markdown with $$ for LaTeX math equations, suitable for use in Jupyter Notebook:\n",
    "\n",
    "⸻\n",
    "\n",
    "Absolutely! Let’s walk through a simple 2D example to understand:\n",
    "\t•\tWhat vectors are\n",
    "\t•\tWhat eigenvectors and eigenvalues are\n",
    "\t•\tHow they relate to PCA\n",
    "\n",
    "⸻\n",
    "\n",
    "1. What is a Vector?\n",
    "\n",
    "A vector is just an arrow in space. In 2D:\n",
    "\n",
    "$$\n",
    "\\vec{v} = \\begin{bmatrix} 3 \\ 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This vector goes 3 units right and 2 units up from the origin.\n",
    "\n",
    "⸻\n",
    "\n",
    "2. What is a Matrix Transformation?\n",
    "\n",
    "A matrix can transform vectors by stretching, rotating, or squashing them.\n",
    "\n",
    "Let’s take a matrix:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 2 & 1 \\ 1 & 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now apply it to a vector:\n",
    "\n",
    "$$\n",
    "A \\cdot \\vec{v} = \\begin{bmatrix} 2 & 1 \\ 1 & 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 \\ 2 \\end{bmatrix}\n",
    "= \\begin{bmatrix} (2)(3) + (1)(2) \\ (1)(3) + (2)(2) \\end{bmatrix}\n",
    "= \\begin{bmatrix} 8 \\ 7 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So the vector was transformed to a new direction and length.\n",
    "\n",
    "⸻\n",
    "\n",
    "3. What is an Eigenvector?\n",
    "\n",
    "An eigenvector of a matrix is a special vector that doesn’t change direction when the matrix is applied. It only gets stretched or shrunk.\n",
    "\n",
    "In math:\n",
    "\n",
    "$$\n",
    "A \\vec{x} = \\lambda \\vec{x}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\t•\t$A$ = matrix\n",
    "\t•\t$\\vec{x}$ = eigenvector\n",
    "\t•\t$\\lambda$ = eigenvalue (scalar stretch)\n",
    "\n",
    "⸻\n",
    "\n",
    "4. Easy Eigenvector Example\n",
    "\n",
    "Let’s find the eigenvectors of:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 2 & 1 \\ 1 & 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Step 1: Solve the characteristic equation\n",
    "\n",
    "$$\n",
    "\\text{det}(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{vmatrix} 2 - \\lambda & 1 \\ 1 & 2 - \\lambda \\end{vmatrix} = 0\n",
    "\\Rightarrow (2 - \\lambda)^2 - 1 = 0\n",
    "\\Rightarrow \\lambda^2 - 4\\lambda + 3 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \\lambda = 1, 3\n",
    "$$\n",
    "\n",
    "So the eigenvalues are 1 and 3.\n",
    "\n",
    "⸻\n",
    "\n",
    "Step 2: Find eigenvectors\n",
    "\n",
    "For $\\lambda = 3$:\n",
    "\n",
    "Solve:\n",
    "\n",
    "$$\n",
    "(A - 3I) \\vec{x} = 0 \\Rightarrow\n",
    "\\begin{bmatrix} -1 & 1 \\ 1 & -1 \\end{bmatrix}\n",
    "\\begin{bmatrix} x \\ y \\end{bmatrix}\n",
    "= \\begin{bmatrix} 0 \\ 0 \\end{bmatrix}\n",
    "\\Rightarrow x = y\n",
    "$$\n",
    "\n",
    "So one eigenvector is:\n",
    "\n",
    "$$\n",
    "\\vec{v}_1 = \\begin{bmatrix} 1 \\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For $\\lambda = 1$:\n",
    "\n",
    "$$\n",
    "(A - I) = \\begin{bmatrix} 1 & 1 \\ 1 & 1 \\end{bmatrix}\n",
    "\\Rightarrow x = -y\n",
    "$$\n",
    "\n",
    "So another eigenvector is:\n",
    "\n",
    "$$\n",
    "\\vec{v}_2 = \\begin{bmatrix} 1 \\ -1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "⸻\n",
    "\n",
    "5. What Does It Mean Visually?\n",
    "\t•\tMatrix $A$ stretches vectors along $[1, 1]$ and $[1, -1]$ directions.\n",
    "\t•\tThose directions are the eigenvectors.\n",
    "\t•\tIt stretches by a factor of 3 along $[1, 1]$, and 1 along $[1, -1]$.\n",
    "\n",
    "⸻\n",
    "\n",
    "Why Does PCA Use Eigenvectors?\n",
    "\n",
    "PCA finds directions (eigenvectors of the covariance matrix) that:\n",
    "\t•\tCapture the most variance\n",
    "\t•\tAre uncorrelated (orthogonal)\n",
    "\t•\tReduce dimensions while preserving information\n",
    "\n",
    "⸻\n",
    "\n",
    "Would you like this visualized in Jupyter using NumPy and Matplotlib?\n",
    "\n",
    "⸻\n",
    "\n"
   ],
   "id": "d43a4ecb911253f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ed4d4-cb5c-41fb-a684-31f3b9032776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a 2x2 matrix A\n",
    "A = np.array([[2, 1],\n",
    "              [1, 2]])\n",
    "\n",
    "# Define two eigenvectors of A\n",
    "eigenvectors = np.array([[1, 1],   # eigenvector for λ = 3\n",
    "                         [1, -1]]) # eigenvector for λ = 1\n",
    "\n",
    "# Apply the transformation A to each eigenvector\n",
    "transformed = A @ eigenvectors\n",
    "\n",
    "# Plot setup\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axhline(0, color='gray', lw=1)\n",
    "plt.axvline(0, color='gray', lw=1)\n",
    "plt.grid(True)\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "# Plot original eigenvectors\n",
    "plt.quiver(0, 0, eigenvectors[0, 0], eigenvectors[1, 0], color='blue', angles='xy', scale_units='xy', scale=1, label='Eigenvector λ=3')\n",
    "plt.quiver(0, 0, eigenvectors[0, 1], eigenvectors[1, 1], color='green', angles='xy', scale_units='xy', scale=1, label='Eigenvector λ=1')\n",
    "\n",
    "# Plot transformed eigenvectors\n",
    "plt.quiver(0, 0, transformed[0, 0], transformed[1, 0], color='blue', linestyle='dashed', angles='xy', scale_units='xy', scale=1)\n",
    "plt.quiver(0, 0, transformed[0, 1], transformed[1, 1], color='green', linestyle='dashed', angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "plt.legend()\n",
    "plt.title('Eigenvectors and Their Transformation by Matrix A')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here’s your content converted to Jupyter Notebook Markdown with properly formatted LaTeX equations using $ signs for rendering:\n",
    "\n",
    "⸻\n",
    "\n",
    "Orthogonal vs Orthonormal Vectors\n",
    "\n",
    "⸻\n",
    "\n",
    "1. Orthogonal Vectors\n",
    "\n",
    "Two vectors are orthogonal if their dot product is zero:\n",
    "\n",
    "$$\n",
    "\\vec{u} \\cdot \\vec{v} = 0\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "\\vec{u} = \\begin{bmatrix} 1 \\ 2 \\end{bmatrix}, \\quad\n",
    "\\vec{v} = \\begin{bmatrix} 2 \\ -1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{u} \\cdot \\vec{v} = 1 \\cdot 2 + 2 \\cdot (-1) = 2 - 2 = 0\n",
    "$$\n",
    "\n",
    "So $\\vec{u}$ and $\\vec{v}$ are orthogonal.\n",
    "\n",
    "⸻\n",
    "\n",
    "2. Orthonormal Vectors\n",
    "\n",
    "Vectors are orthonormal if:\n",
    "\t•\tThey are orthogonal: dot product $= 0$\n",
    "\t•\tEach vector has unit length:\n",
    "\n",
    "$$\n",
    "|\\vec{u}| = |\\vec{v}| = 1\n",
    "$$\n",
    "\n",
    "Normalize:\n",
    "\n",
    "$$\n",
    "\\hat{u} = \\frac{\\vec{u}}{|\\vec{u}|}, \\quad\n",
    "\\hat{v} = \\frac{\\vec{v}}{|\\vec{v}|}\n",
    "$$\n",
    "\n",
    "Then $\\hat{u}$ and $\\hat{v}$ are orthonormal.\n",
    "\n",
    "⸻\n",
    "\n",
    "Python Code: Visualizing Orthogonal & Orthonormal Vectors\n",
    "\n",
    "(You can include a matplotlib-based visualization here if needed)\n",
    "\n",
    "⸻\n",
    "\n",
    "Summary\n",
    "\n",
    "Property\tOrthogonal\tOrthonormal\n",
    "Dot product\t$= 0$\t$= 0$\n",
    "Length\tCan be any\tMust be $1$\n",
    "Use case\tPCA bases\tOrthonormal basis in QR\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "Want to see Gram-Schmidt to convert arbitrary vectors into an orthonormal basis?\n",
    "\n",
    "⸻\n",
    "\n",
    "Let me know if you’d like the visualization code or the Gram-Schmidt process in Markdown too!"
   ],
   "id": "4c1de8ff4ea99ebc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "⸻\n",
    "\n",
    "Dot Product of Vectors\n",
    "\n",
    "The dot product (also called the scalar product) is a way of multiplying two vectors that results in a single number (a scalar).\n",
    "\n",
    "⸻\n",
    "\n",
    "Definition:\n",
    "\n",
    "For two vectors\n",
    "$\\vec{a} = [a_1, a_2, \\dots, a_n], \\quad \\vec{b} = [b_1, b_2, \\dots, b_n]$\n",
    "the dot product is:\n",
    "\n",
    "$$\n",
    "\\vec{a} \\cdot \\vec{b} = a_1b_1 + a_2b_2 + \\dots + a_n b_n\n",
    "$$\n",
    "\n",
    "⸻\n",
    "\n",
    "Geometric Interpretation:\n",
    "\n",
    "$$\n",
    "\\vec{a} \\cdot \\vec{b} = |\\vec{a}| , |\\vec{b}| , \\cos(\\theta)\n",
    "$$\n",
    "\t•\t$|\\vec{a}|$ and $|\\vec{b}|$ are magnitudes (lengths) of the vectors.\n",
    "\t•\t$\\theta$ is the angle between them.\n",
    "\n",
    "⸻\n",
    "\n",
    "Interpretation of the Result:\n",
    "\n",
    "1. If $\\vec{a} \\cdot \\vec{b} = 0$:\n",
    "\t•\tVectors are perpendicular (orthogonal), since $\\cos(90^\\circ) = 0$.\n",
    "\n",
    "2. If $\\vec{a} \\cdot \\vec{b} > 0$:\n",
    "\t•\tVectors point in a similar direction (angle $< 90^\\circ$).\n",
    "\n",
    "3. If $\\vec{a} \\cdot \\vec{b} < 0$:\n",
    "\t•\tVectors point in opposite directions (angle $> 90^\\circ$).\n",
    "\n",
    "4. If $\\vec{a} \\cdot \\vec{b} = 1$:\n",
    "\t•\tThis can only happen if both vectors are unit vectors (length = 1) and point in exactly the same direction.\n",
    "\n",
    "⸻\n",
    "\n",
    "Example:\n",
    "\n",
    "Let\n",
    "$\\vec{a} = [1, 2, 3], \\quad \\vec{b} = [4, -5, 6]$\n",
    "\n",
    "Then:\n",
    "$$\n",
    "\\vec{a} \\cdot \\vec{b} = (1)(4) + (2)(-5) + (3)(6) = 4 - 10 + 18 = 12\n",
    "$$\n",
    "\n"
   ],
   "id": "934b942c69e2d9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d119e1-288e-471e-a0ae-714f402dca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define orthogonal vectors\n",
    "u = np.array([1, 2])\n",
    "v = np.array([2, -1])\n",
    "\n",
    "# Check orthogonality\n",
    "dot_product = np.dot(u, v)\n",
    "print(\"Dot product (should be 0):\", dot_product)\n",
    "\n",
    "# Normalize vectors (make them unit length)\n",
    "u_norm = u / np.linalg.norm(u)\n",
    "v_norm = v / np.linalg.norm(v)\n",
    "\n",
    "# Plot both sets\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axhline(0, color='gray')\n",
    "plt.axvline(0, color='gray')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.grid(True)\n",
    "\n",
    "# Original orthogonal vectors\n",
    "plt.quiver(0, 0, u[0], u[1], color='blue', angles='xy', scale_units='xy', scale=1, label='u (Orthogonal)')\n",
    "plt.quiver(0, 0, v[0], v[1], color='green', angles='xy', scale_units='xy', scale=1, label='v (Orthogonal)')\n",
    "\n",
    "# Orthonormal vectors (unit length), using alpha to distinguish\n",
    "plt.quiver(0, 0, u_norm[0], u_norm[1], color='blue', alpha=0.5, angles='xy', scale_units='xy', scale=1, label='u_norm (Orthonormal)')\n",
    "plt.quiver(0, 0, v_norm[0], v_norm[1], color='green', alpha=0.5, angles='xy', scale_units='xy', scale=1, label='v_norm (Orthonormal)')\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.title(\"Orthogonal and Orthonormal Vectors\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "⸻\n",
    "\n",
    "What is PCA?\n",
    "\n",
    "PCA is a dimensionality reduction technique that:\n",
    "\t1.\tFinds directions (called principal components) along which the data varies the most.\n",
    "\t2.\tProjects data onto a smaller number of these components, while retaining as much variance (information) as possible.\n",
    "\n",
    "⸻\n",
    "\n",
    "Steps Involved\n",
    "\t1.\tStandardize the data\n",
    "Remove the mean (and optionally scale to unit variance).\n",
    "\t2.\tCompute the covariance matrix\n",
    "Captures how features vary with each other.\n",
    "\t3.\tGet eigenvectors and eigenvalues\n",
    "These represent principal components and their importance.\n",
    "\t4.\tSort and select top k eigenvectors\n",
    "Select directions with the most variance.\n",
    "\t5.\tProject data\n",
    "Reduce dimensions by projecting onto these top directions.\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "e7a660cc2b52eb17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5343da5-e0e9-4de1-88a9-981ce8c31caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def pca(X, num_components):\n",
    "    # 1. Standardize\n",
    "    X_meaned = X - np.mean(X, axis=0)\n",
    "\n",
    "    # 2. Covariance matrix\n",
    "    cov_matrix = np.cov(X_meaned, rowvar=False)\n",
    "\n",
    "    # 3. Eigen decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # 4. Sort eigenvalues & eigenvectors\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # 5. Select top components\n",
    "    eigenvectors_subset = eigenvectors[:, :num_components]\n",
    "\n",
    "    # 6. Project data\n",
    "    X_reduced = np.dot(X_meaned, eigenvectors_subset)\n",
    "\n",
    "    return X_reduced, eigenvalues[:num_components], eigenvectors_subset\n",
    "\n",
    "# Load data\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "target_names = data.target_names\n",
    "\n",
    "# Apply PCA\n",
    "X_pca, _, _ = pca(X, num_components=2)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "for target, label in enumerate(target_names):\n",
    "    plt.scatter(X_pca[y == target, 0], X_pca[y == target, 1], label=label)\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA of Iris Dataset\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Explanation of the Plot\n",
    "\t•\tEach point represents an Iris flower, projected from 4D to 2D.\n",
    "\t•\tThe axes are the top 2 principal components.\n",
    "\t•\tThe clusters show that PCA captures meaningful variance — e.g., Setosa is clearly separable from others.\n",
    "\n",
    "⸻"
   ],
   "id": "fdb5383948bc6fb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Got it! Here’s your PCA explanation converted to Jupyter Notebook Markdown format using LaTeX equations with dollar signs ($) so it renders properly in Jupyter. Just copy and paste it into a Markdown cell:\n",
    "\n",
    "⸻\n",
    "\n",
    "Principal Component Analysis (PCA) - Mathematical Explanation\n",
    "\n",
    "⸻\n",
    "\n",
    "1. Data Representation\n",
    "\n",
    "Let the dataset have $n$ samples and $d$ features.\n",
    "We represent the data as a matrix:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "\\uparrow & \\uparrow & & \\uparrow \\\n",
    "\\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_n \\\n",
    "\\downarrow & \\downarrow & & \\downarrow \\\n",
    "\\end{bmatrix}^\\top\n",
    "\\in \\mathbb{R}^{n \\times d}\n",
    "$$\n",
    "\n",
    "Each row $\\mathbf{x}_i \\in \\mathbb{R}^d$ is a data point (feature vector).\n",
    "\n",
    "⸻\n",
    "\n",
    "2. Centering the Data\n",
    "\n",
    "PCA requires zero-centered data.\n",
    "\n",
    "Let $\\boldsymbol{\\mu} \\in \\mathbb{R}^d$ be the mean vector:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "Then, we subtract the mean:\n",
    "\n",
    "$$\n",
    "X_{\\text{centered}} = X - \\boldsymbol{\\mu}\n",
    "$$\n",
    "\n",
    "⸻\n",
    "\n",
    "3. Covariance Matrix\n",
    "\n",
    "The covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$ tells us how features vary with respect to each other:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n - 1} X_{\\text{centered}}^\\top X_{\\text{centered}}\n",
    "$$\n",
    "\n",
    "This is a symmetric and positive semi-definite matrix.\n",
    "\n",
    "⸻\n",
    "\n",
    "4. Eigenvalues and Eigenvectors\n",
    "\n",
    "We perform eigen decomposition:\n",
    "\n",
    "$$\n",
    "\\Sigma \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\t•\t$\\lambda_i \\in \\mathbb{R}$ is the eigenvalue\n",
    "\t•\t$\\mathbf{v}_i \\in \\mathbb{R}^d$ is the corresponding eigenvector\n",
    "\n",
    "Properties:\n",
    "\t•\tEigenvectors $\\mathbf{v}_i$ are orthogonal (can be made orthonormal)\n",
    "\t•\tEigenvalues represent the variance captured along that direction\n",
    "\n",
    "⸻\n",
    "\n",
    "5. Sorting and Selecting Top Components\n",
    "\n",
    "Sort eigenvalues and eigenvectors in descending order of $\\lambda_i$.\n",
    "Select the top $k$ eigenvectors:\n",
    "\n",
    "$$\n",
    "W_k = \\left[ \\mathbf{v}_1 ; \\mathbf{v}_2 ; \\cdots ; \\mathbf{v}_k \\right] \\in \\mathbb{R}^{d \\times k}\n",
    "$$\n",
    "\n",
    "⸻\n",
    "\n",
    "6. Projecting the Data\n",
    "\n",
    "Now project the original data to the new $k$-dimensional subspace:\n",
    "\n",
    "$$\n",
    "Z = X_{\\text{centered}} \\cdot W_k, \\quad \\text{where} \\quad Z \\in \\mathbb{R}^{n \\times k}\n",
    "$$\n",
    "\n",
    "This is the reduced-dimensional representation of the data.\n",
    "\n",
    "⸻\n",
    "\n",
    "7. Summary of PCA Steps\n",
    "\t1.\tStandardize the data\n",
    "\t2.\tCompute covariance matrix $\\Sigma$\n",
    "\t3.\tCompute eigenvalues and eigenvectors\n",
    "\t4.\tSort and select top $k$ eigenvectors\n",
    "\t5.\tProject data onto new axes\n",
    "\n",
    "⸻\n",
    "\n",
    "Let me know if you want a version with accompanying code snippets or visualization examples next.\n"
   ],
   "id": "b1207416aed09278"
  },
  {
   "cell_type": "markdown",
   "id": "0c7be822-bb40-4ad3-bcd0-78f948fe356a",
   "metadata": {},
   "source": [
    "# Singular value decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b49d450-71fc-4982-b849-613fc5fedced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_eigenvalues(A, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Find eigenvalues by solving the characteristic polynomial det(A - λI) = 0\n",
    "    \"\"\"\n",
    "    print(\"=== Step 1: Find Eigenvalues ===\")\n",
    "    print(\"Solving det(A - λI) = 0\\n\")\n",
    "\n",
    "    # Characteristic polynomial coefficients\n",
    "    if A.shape == (2, 2):\n",
    "        # For 2x2 matrix: λ² - (tr(A))λ + det(A) = 0\n",
    "        trace = A[0,0] + A[1,1]\n",
    "        det = A[0,0]*A[1,1] - A[0,1]*A[1,0]\n",
    "        print(f\"Characteristic equation: λ² - {trace}λ + {det} = 0\")\n",
    "\n",
    "        # Quadratic formula\n",
    "        discriminant = trace**2 - 4*det\n",
    "        λ1 = (trace + np.sqrt(discriminant)) / 2\n",
    "        λ2 = (trace - np.sqrt(discriminant)) / 2\n",
    "        eigenvalues = [λ1, λ2]\n",
    "\n",
    "    else:\n",
    "        # For larger matrices - much more complex (this is simplified)\n",
    "        print(\"For larger matrices, we'd typically use iterative methods\")\n",
    "        print(\"But we'll use numpy's roots for demonstration:\")\n",
    "        char_poly = np.poly(A)\n",
    "        eigenvalues = np.roots(char_poly)\n",
    "\n",
    "    print(\"\\nEigenvalues found:\", eigenvalues)\n",
    "    return np.array(eigenvalues)\n",
    "\n",
    "def find_eigenvectors(A, eigenvalues, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Find eigenvectors by solving (A - λI)v = 0 for each eigenvalue\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Step 2: Find Eigenvectors ===\")\n",
    "    eigenvectors = []\n",
    "\n",
    "    for i, λ in enumerate(eigenvalues):\n",
    "        print(f\"\\nFor eigenvalue λ{i+1} = {λ}:\")\n",
    "        M = A - λ * np.eye(A.shape[0])\n",
    "        print(f\"A - λ{i+1}I:\\n{M}\")\n",
    "\n",
    "        # Find null space (simplified approach)\n",
    "        if abs(M[0,0]) > tol and abs(M[1,1]) > tol:\n",
    "            # Assume eigenvector is [1, -M[0,0]/M[0,1]]\n",
    "            v = np.array([1, -M[0,0]/M[0,1]])\n",
    "        else:\n",
    "            v = np.array([-M[0,1]/M[0,0], 1])\n",
    "\n",
    "        # Normalize\n",
    "        v = v / np.linalg.norm(v)\n",
    "        print(f\"Eigenvector v{i+1}: {v}\")\n",
    "        eigenvectors.append(v)\n",
    "\n",
    "    return np.array(eigenvectors).T\n",
    "\n",
    "def demonstrate_eigen_calculation(A):\n",
    "    print(\"=== Original Matrix ===\")\n",
    "    print(A)\n",
    "\n",
    "    print(\"\\n=== Calculating Determinant ===\")\n",
    "    if A.shape == (2, 2):\n",
    "        det = A[0,0]*A[1,1] - A[0,1]*A[1,0]\n",
    "        print(f\"det(A) = {A[0,0]}*{A[1,1]} - {A[0,1]}*{A[1,0]} = {det}\")\n",
    "    else:\n",
    "        det = np.linalg.det(A)\n",
    "        print(f\"det(A) = {det}\")\n",
    "\n",
    "    eigenvalues = find_eigenvalues(A)\n",
    "    eigenvectors = find_eigenvectors(A, eigenvalues)\n",
    "\n",
    "    print(\"\\n=== Verification ===\")\n",
    "    for i, λ in enumerate(eigenvalues):\n",
    "        v = eigenvectors[:, i]\n",
    "        print(f\"A @ v{i+1}: {A @ v}\")\n",
    "        print(f\"λ{i+1} * v{i+1}: {λ * v}\\n\")\n",
    "\n",
    "# Example usage\n",
    "A = np.array([[4, 2],\n",
    "              [1, 3]])\n",
    "\n",
    "demonstrate_eigen_calculation(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a30acc-0573-4adc-ba6a-330dbde8ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reconstruct_from_eigen(A):\n",
    "    print(\"=== Original Matrix ===\")\n",
    "    print(\"A:\\n\", A)\n",
    "\n",
    "    # Step 1: Find eigenvalues and eigenvectors\n",
    "    print(\"\\n=== Step 1: Eigen Decomposition ===\")\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "    print(\"Eigenvalues (λ):\", eigenvalues)\n",
    "    print(\"Eigenvectors (columns):\\n\", eigenvectors)\n",
    "\n",
    "    # Step 2: Verify Av = λv\n",
    "    print(\"\\n=== Step 2: Verify Eigenproperties ===\")\n",
    "    for i in range(len(eigenvalues)):\n",
    "        v = eigenvectors[:, i]\n",
    "        λ = eigenvalues[i]\n",
    "        print(f\"A @ v{λ}: {A @ v}\")\n",
    "        print(f\"λ * v{λ}: {λ * v}\\n\")\n",
    "\n",
    "    # Step 3: Reconstruct original matrix\n",
    "    print(\"\\n=== Step 3: Matrix Reconstruction ===\")\n",
    "    print(\"Using A = V Λ V⁻¹\")\n",
    "\n",
    "    Λ = np.diag(eigenvalues)\n",
    "    V = eigenvectors\n",
    "    V_inv = np.linalg.inv(V)\n",
    "\n",
    "    print(\"Diagonal matrix Λ:\\n\", Λ)\n",
    "    print(\"Eigenvector matrix V:\\n\", V)\n",
    "    print(\"Inverse of V:\\n\", V_inv)\n",
    "\n",
    "    A_reconstructed = V @ Λ @ V_inv\n",
    "    print(\"\\nReconstructed A:\\n\", A_reconstructed)\n",
    "\n",
    "    # Step 4: Verification\n",
    "    print(\"\\n=== Verification ===\")\n",
    "    print(\"Original A:\\n\", A)\n",
    "    print(\"Reconstructed A:\\n\", A_reconstructed)\n",
    "    print(\"\\nReconstruction successful?\", np.allclose(A, A_reconstructed))\n",
    "\n",
    "# Example usage\n",
    "A = np.array([[4, 2],\n",
    "              [1, 3]])\n",
    "reconstruct_from_eigen(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa202d8-e08b-45b8-a5a2-01337ad84d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "def demonstrate_svd_calculation(X, print_steps=True):\n",
    "    \"\"\"\n",
    "    Demonstrate how U, s, Vt are calculated in SVD decomposition\n",
    "    \"\"\"\n",
    "    if print_steps:\n",
    "        print(\"=== Original Matrix ===\")\n",
    "        print(\"X:\\n\", X)\n",
    "        print(f\"Shape: {X.shape}\\n\")\n",
    "\n",
    "    # Step 1: Compute X^T X and its eigenvalues/vectors\n",
    "    XTX = X.T @ X\n",
    "    if print_steps:\n",
    "        print(\"=== Step 1: Compute X^T X ===\")\n",
    "        print(\"X^T X:\\n\", XTX)\n",
    "        print(f\"Shape: {XTX.shape}\\n\")\n",
    "\n",
    "    # Step 2: Find eigenvalues and eigenvectors of X^T X\n",
    "    eigenvalues, V = np.linalg.eig(XTX)\n",
    "    eigenvalues = np.real(eigenvalues)  # Keep real part (for numerical stability)\n",
    "    V = np.real(V)\n",
    "\n",
    "    # Sort eigenvalues in descending order\n",
    "    idx = eigenvalues.argsort()[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    V = V[:, idx]\n",
    "\n",
    "    if print_steps:\n",
    "        print(\"=== Step 2: Eigen Decomposition of X^T X ===\")\n",
    "        print(\"Eigenvalues (s^2):\\n\", eigenvalues)\n",
    "        print(\"Eigenvectors (columns of V):\\n\", V)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Step 3: Compute singular values (s) and Vt\n",
    "    s = np.sqrt(np.maximum(eigenvalues, 0))  # Ensure non-negative\n",
    "    Vt = V.T\n",
    "\n",
    "    if print_steps:\n",
    "        print(\"=== Step 3: Compute Singular Values and Vt ===\")\n",
    "        print(\"Singular values (s):\\n\", s)\n",
    "        print(\"Vt (transpose of V):\\n\", Vt)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Step 4: Compute U\n",
    "    U = X @ V\n",
    "    for i in range(len(s)):\n",
    "        if s[i] > 1e-10:  # Avoid division by zero\n",
    "            U[:, i] = U[:, i] / s[i]\n",
    "\n",
    "    if print_steps:\n",
    "        print(\"=== Step 4: Compute U ===\")\n",
    "        print(\"U (before orthogonalization):\\n\", U)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Step 5: Ensure U is orthogonal (QR decomposition to fix numerical issues)\n",
    "    Q, R = np.linalg.qr(U)\n",
    "    U = Q\n",
    "\n",
    "    if print_steps:\n",
    "        print(\"=== Step 5: Orthogonalize U ===\")\n",
    "        print(\"U (after QR orthogonalization):\\n\", U)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Step 6: Construct full SVD matrices\n",
    "    if print_steps:\n",
    "        print(\"=== Final SVD Components ===\")\n",
    "        print(\"U:\\n\", U)\n",
    "        print(\"s:\\n\", s)\n",
    "        print(\"Vt:\\n\", Vt)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return U, s, Vt\n",
    "\n",
    "# Example usage with a simple matrix\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"==== SVD CALCULATION DEMONSTRATION ====\\n\")\n",
    "\n",
    "    # Create a sample matrix\n",
    "    X = np.array([[1, 2],\n",
    "                  [3, 4],\n",
    "                  [5, 6]], dtype=float)\n",
    "\n",
    "    # Our implementation\n",
    "    print(\"=== Our Implementation ===\")\n",
    "    U_our, s_our, Vt_our = demonstrate_svd_calculation(X)\n",
    "\n",
    "    # Compare with scipy's implementation\n",
    "    print(\"\\n=== Scipy's Implementation ===\")\n",
    "    U_scipy, s_scipy, Vt_scipy = svd(X, full_matrices=False)\n",
    "    print(\"U:\\n\", U_scipy)\n",
    "    print(\"s:\\n\", s_scipy)\n",
    "    print(\"Vt:\\n\", Vt_scipy)\n",
    "\n",
    "    # Verification\n",
    "    print(\"\\n=== Verification ===\")\n",
    "    print(\"Reconstructed matrix (our):\\n\", U_our @ np.diag(s_our) @ Vt_our)\n",
    "    print(\"Reconstructed matrix (scipy):\\n\", U_scipy @ np.diag(s_scipy) @ Vt_scipy)\n",
    "    print(\"Original matrix:\\n\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c8e1b-c6ef-4249-a435-6746d3e96255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "def svd_linear_regression(X, y, print_steps=True):\n",
    "    \"\"\"\n",
    "    Demonstrate SVD-based linear regression with explanatory print statements\n",
    "    \"\"\"\n",
    "    # Step 1: Add bias term (column of ones)\n",
    "    X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "    if print_steps:\n",
    "        print(\"=== Step 1: Design Matrix (with bias column) ===\")\n",
    "        print(\"X shape:\", X.shape)\n",
    "        print(X[:3], \"...\\n\")  # Show first 3 rows\n",
    "\n",
    "    # Step 2: Compute SVD\n",
    "    U, s, Vt = svd(X, full_matrices=False)\n",
    "    if print_steps:\n",
    "        print(\"=== Step 2: Singular Value Decomposition ===\")\n",
    "        print(\"U shape:\", U.shape)\n",
    "        print(\"Singular values (s):\", s)\n",
    "        print(\"Vt shape:\", Vt.shape, \"\\n\")\n",
    "\n",
    "    # Step 3: Determine numerical rank\n",
    "    tol = s.max() * max(X.shape) * np.finfo(s.dtype).eps\n",
    "    rank = np.sum(s > tol)\n",
    "    if print_steps:\n",
    "        print(\"=== Step 3: Determine Numerical Rank ===\")\n",
    "        print(\"Tolerance:\", tol)\n",
    "        print(\"Effective rank:\", rank, \"\\n\")\n",
    "\n",
    "    # Step 4: Compute pseudoinverse\n",
    "    s_inv = np.zeros_like(s)\n",
    "    s_inv[:rank] = 1 / s[:rank]\n",
    "    Sigma_pinv = np.diag(s_inv)\n",
    "    if print_steps:\n",
    "        print(\"=== Step 4: Compute Pseudoinverse ===\")\n",
    "        print(\"Reciprocal of non-zero singular values:\")\n",
    "        print(s_inv)\n",
    "        print(\"Σ⁺ (pseudoinverse of diagonal matrix):\")\n",
    "        print(Sigma_pinv, \"\\n\")\n",
    "\n",
    "    # Step 5: Calculate coefficients θ = VΣ⁺Uᵀy\n",
    "    theta = Vt.T @ Sigma_pinv @ U.T @ y\n",
    "    if print_steps:\n",
    "        print(\"=== Step 5: Calculate Coefficients θ ===\")\n",
    "        print(\"θ = VΣ⁺Uᵀy\")\n",
    "        print(\"Final coefficients:\")\n",
    "        print(theta.flatten())\n",
    "\n",
    "    return theta\n",
    "\n",
    "# Example usage with simple data\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"==== DEMONSTRATION: SVD FOR LINEAR REGRESSION ====\\n\")\n",
    "\n",
    "    # Sample data (2 features + noise)\n",
    "    np.random.seed(42)\n",
    "    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "    y = np.array([[3], [7], [11], [15]]) + np.random.randn(4, 1)*0.5\n",
    "\n",
    "    print(\"Original data:\")\n",
    "    print(\"X:\", X)\n",
    "    print(\"y:\", y, \"\\n\")\n",
    "\n",
    "    # Run our SVD regression\n",
    "    theta = svd_linear_regression(X, y)\n",
    "\n",
    "    # Compare with numpy's least squares\n",
    "    lstq_solution = np.linalg.lstsq(np.column_stack([np.ones(X.shape[0]), X]), y, rcond=None)[0]\n",
    "    print(\"\\n=== Verification ===\")\n",
    "    print(\"Our SVD θ:\", theta.flatten())\n",
    "    print(\"Numpy lstsq θ:\", lstq_solution.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8dec6-cf1e-421a-910a-1d78f7781b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Principal Component Analysis (PCA): Deeper Mathematical Foundations\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Variance as a Quadratic Form\n",
    "\n",
    "The variance of data projected onto a unit vector $\\mathbf{w} \\in \\mathbb{R}^d$ is:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\mathbf{w}) = \\frac{1}{n-1} \\sum_{i=1}^n (\\mathbf{w}^\\top \\mathbf{x}_i)^2 = \\mathbf{w}^\\top \\Sigma \\mathbf{w}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\Sigma$ is the covariance matrix.\n",
    "- $\\mathbf{w}$ is a direction vector.\n",
    "- We want to find the direction $\\mathbf{w}$ that maximizes the projected variance.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Optimization Problem in PCA\n",
    "\n",
    "PCA can be formulated as a constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_{\\mathbf{w}} \\quad \\mathbf{w}^\\top \\Sigma \\mathbf{w} \\quad \\text{subject to} \\quad \\|\\mathbf{w}\\| = 1\n",
    "$$\n",
    "\n",
    "This constraint ensures we are choosing a unit direction vector.\n",
    "\n",
    "We solve this using **Lagrange multipliers**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}, \\lambda) = \\mathbf{w}^\\top \\Sigma \\mathbf{w} - \\lambda (\\mathbf{w}^\\top \\mathbf{w} - 1)\n",
    "$$\n",
    "\n",
    "Taking the gradient and setting it to zero:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\mathcal{L} = 2\\Sigma \\mathbf{w} - 2\\lambda \\mathbf{w} = 0\n",
    "\\Rightarrow \\Sigma \\mathbf{w} = \\lambda \\mathbf{w}\n",
    "$$\n",
    "\n",
    "Thus, $\\mathbf{w}$ must be an **eigenvector** of $\\Sigma$, and $\\lambda$ is the corresponding **eigenvalue**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Geometric Interpretation\n",
    "\n",
    "- Each eigenvector of $\\Sigma$ defines a new axis (principal component).\n",
    "- The corresponding eigenvalue tells us the **amount of variance** along that direction.\n",
    "- PCA finds a **rotated coordinate system** aligned with directions of **maximum variance**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Orthonormality of Eigenvectors\n",
    "\n",
    "Because $\\Sigma$ is symmetric, all its eigenvectors are:\n",
    "- **Orthogonal**: $\\mathbf{v}_i^\\top \\mathbf{v}_j = 0$ for $i \\ne j$\n",
    "- Can be made **orthonormal**: $\\mathbf{v}_i^\\top \\mathbf{v}_i = 1$\n",
    "\n",
    "Let $V \\in \\mathbb{R}^{d \\times d}$ be the matrix of eigenvectors:\n",
    "\n",
    "$$\n",
    "V^\\top V = I \\quad \\Rightarrow \\quad V^{-1} = V^\\top\n",
    "$$\n",
    "\n",
    "This allows diagonalization of $\\Sigma$:\n",
    "\n",
    "$$\n",
    "\\Sigma = V \\Lambda V^\\top\n",
    "$$\n",
    "\n",
    "Where $\\Lambda$ is the diagonal matrix of eigenvalues.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Dimensionality Reduction with PCA\n",
    "\n",
    "Let:\n",
    "- $V_k \\in \\mathbb{R}^{d \\times k}$ be the matrix of top $k$ eigenvectors.\n",
    "- $X_{\\text{centered}} \\in \\mathbb{R}^{n \\times d}$ be the mean-centered data.\n",
    "\n",
    "Then the reduced representation is:\n",
    "\n",
    "$$\n",
    "Z = X_{\\text{centered}} V_k \\in \\mathbb{R}^{n \\times k}\n",
    "$$\n",
    "\n",
    "This reduces the data from $d$ dimensions to $k$ dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Reconstruction from PCA\n",
    "\n",
    "To reconstruct the original (approximate) data from $Z$:\n",
    "\n",
    "$$\n",
    "\\hat{X} = Z V_k^\\top + \\boldsymbol{\\mu}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $Z$: compressed data\n",
    "- $V_k^\\top$: maps back to original space\n",
    "- $\\boldsymbol{\\mu}$: the mean added back\n",
    "\n",
    "Note: This reconstruction is **lossy** but preserves the most important structure.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Total Variance and Explained Variance\n",
    "\n",
    "Let the eigenvalues be $\\lambda_1, \\lambda_2, \\dots, \\lambda_d$.\n",
    "\n",
    "- **Total variance** in data:\n",
    "\n",
    "$$\n",
    "\\text{Total variance} = \\sum_{i=1}^d \\lambda_i = \\text{trace}(\\Sigma)\n",
    "$$\n",
    "\n",
    "- **Explained variance ratio** for top $k$ components:\n",
    "\n",
    "$$\n",
    "\\text{Explained variance ratio} = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{j=1}^d \\lambda_j}\n",
    "$$\n",
    "\n",
    "This ratio helps us decide how many principal components to keep.\n",
    "\n",
    "---"
   ],
   "id": "e157ac431cb9a2e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Great! Let’s break this down so you can easily explain Lagrange multipliers, differentiation, and optimization in the context of PCA, using simple intuitive examples before diving into the math.\n",
    "\n",
    "⸻\n",
    "\n",
    "1. What is an Optimization Problem?\n",
    "\n",
    "At its core, optimization is about finding the best solution under some constraints.\n",
    "\n",
    "Easy example:\n",
    "\n",
    "Maximize area of a rectangle with a fixed perimeter of 100 units.\n",
    "\n",
    "Let sides be x and y.\n",
    "\t•\tObjective: Maximize A = x * y\n",
    "\t•\tConstraint: 2x + 2y = 100\n",
    "\n",
    "This is a perfect candidate for Lagrange multipliers, where we want to maximize a function under a constraint.\n",
    "\n",
    "⸻\n",
    "\n",
    "2. Lagrange Multipliers Intuition\n",
    "\n",
    "Visual analogy:\n",
    "\n",
    "Imagine you’re on a mountain (height = objective function), but you must walk along a path (constraint curve). The highest point on the path is where the gradient of the mountain is tangent (parallel) to the gradient of the path.\n",
    "\n",
    "⸻\n",
    "\n",
    "3. How Lagrange Multipliers Work\n",
    "\n",
    "If you want to maximize a function f(x, y) subject to a constraint g(x, y) = c, you define a new function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x, y, \\lambda) = f(x, y) - \\lambda (g(x, y) - c)\n",
    "$$\n",
    "\n",
    "Then, take partial derivatives and set them equal to zero:\n",
    "\n",
    "$$\n",
    "\\nabla_x \\mathcal{L} = 0, \\quad \\nabla_y \\mathcal{L} = 0, \\quad \\nabla_\\lambda \\mathcal{L} = 0\n",
    "$$\n",
    "\n",
    "⸻\n",
    "\n",
    "4. A Simple, Teachable Example\n",
    "\n",
    "Problem:\n",
    "\n",
    "Maximize f(x, y) = xy subject to x + y = 10.\n",
    "\n",
    "Step 1: Define Lagrangian\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x, y, \\lambda) = xy - \\lambda (x + y - 10)\n",
    "$$\n",
    "\n",
    "Step 2: Take derivatives and set to zero\n",
    "\t•\t\\frac{\\partial \\mathcal{L}}{\\partial x} = y - \\lambda = 0\n",
    "\t•\t\\frac{\\partial \\mathcal{L}}{\\partial y} = x - \\lambda = 0\n",
    "\t•\t\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -(x + y - 10) = 0\n",
    "\n",
    "Step 3: Solve\n",
    "\n",
    "From first two:\n",
    "\t•\ty = \\lambda\n",
    "\t•\tx = \\lambda → So x = y\n",
    "\n",
    "Plug into constraint: x + x = 10 \\Rightarrow x = 5, y = 5\n",
    "\n",
    "So the max value of xy under the constraint is 25, achieved when x = y = 5.\n",
    "\n",
    "⸻\n",
    "\n",
    "5. Applying This to PCA\n",
    "\n",
    "Now let’s return to PCA:\n",
    "\n",
    "We want to maximize variance in the direction of w (projection vector), but we also want w to be a unit vector (length = 1):\n",
    "\n",
    "Optimization form:\n",
    "\n",
    "$$\n",
    "\\max_{\\mathbf{w}} \\mathbf{w}^\\top \\Sigma \\mathbf{w} \\quad \\text{subject to} \\quad \\mathbf{w}^\\top \\mathbf{w} = 1\n",
    "$$\n",
    "\n",
    "This is just like our earlier problem!\n",
    "\n",
    "Step 1: Create Lagrangian\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}, \\lambda) = \\mathbf{w}^\\top \\Sigma \\mathbf{w} - \\lambda (\\mathbf{w}^\\top \\mathbf{w} - 1)\n",
    "$$\n",
    "\n",
    "Step 2: Take derivative and set to 0\n",
    "\n",
    "We use vector calculus:\n",
    "\t•\t\\nabla_{\\mathbf{w}}(\\mathbf{w}^\\top \\Sigma \\mathbf{w}) = 2\\Sigma \\mathbf{w}\n",
    "\t•\t\\nabla_{\\mathbf{w}}(\\lambda(\\mathbf{w}^\\top \\mathbf{w})) = 2\\lambda \\mathbf{w}\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "2\\Sigma \\mathbf{w} - 2\\lambda \\mathbf{w} = 0 \\Rightarrow \\Sigma \\mathbf{w} = \\lambda \\mathbf{w}\n",
    "$$\n",
    "\n",
    "This tells us: w must be an eigenvector of the covariance matrix Σ, and λ is the corresponding eigenvalue (which also represents variance in that direction).\n",
    "\n",
    "⸻\n",
    "\n",
    "6. Summary for Teaching\n",
    "\n",
    "Concept\tExample\tKey Idea\n",
    "Optimization\tMaximize rectangle area with fixed perimeter\tFinding best value under a condition\n",
    "Lagrange multipliers\tSolve xy with x + y = 10\tIntroduce a λ to handle constraint\n",
    "PCA application\tMaximize wᵀΣw with wᵀw = 1\tOptimize variance under unit vector constraint\n",
    "Result\tEigenvector equation: Σw = λw\tFind principal directions (PCA axes)\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "Would you like me to make this into a visual-friendly Jupyter Notebook or Markdown version for class?"
   ],
   "id": "b3d1a3459a6cec0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4a218-bcc3-40cd-9b20-52f1fbe6c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://youtu.be/fkf4IBRSeEc?si=rkWEW-W7eZjTxqb0\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename='Pca 1 Small.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eeb91b-f325-4978-8a0b-9b35071764f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# 1. Generate 2D synthetic data\n",
    "X, _ = make_blobs(n_samples=100, centers=1, cluster_std=2.5, random_state=42)\n",
    "X = X @ [[0.6, -0.8], [0.8, 0.6]]  # rotate the blob to make PCA interesting\n",
    "\n",
    "# 2. Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "# 3. Plot original data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.5, label='Data Points')\n",
    "\n",
    "# 4. Plot the PCA-1 and PCA-2 directions\n",
    "origin = np.mean(X, axis=0)  # mean of the data\n",
    "for i, (component, variance) in enumerate(zip(pca.components_, pca.explained_variance_)):\n",
    "    vector = component * 3 * np.sqrt(variance)  # scale for visibility\n",
    "    plt.quiver(*origin, *vector, angles='xy', scale_units='xy', scale=1,\n",
    "               color=['r', 'g'][i], label=f'PCA-{i+1}')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.title('PCA - Showing PCA-1 and PCA-2 directions')\n",
    "plt.legend()\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6786471-7a2f-4320-b543-95e526670659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Variance as a Quadratic Form\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your CSV (ensure it's in the same directory)\n",
    "df = pd.read_csv(\"stock_prices_2022_2023.csv\", index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "# Mean-center the data\n",
    "X = df.values\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# Compute the covariance matrix\n",
    "cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "# Choose a random direction w (unit vector)\n",
    "w = np.random.rand(X_centered.shape[1])\n",
    "w = w / np.linalg.norm(w)\n",
    "\n",
    "# Projected variance along w\n",
    "projected_variance = w.T @ cov_matrix @ w\n",
    "# This value is what PCA tries to maximize\n",
    "\n",
    "# Step 2: Optimization Problem (Eigen decomposition)\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Sort eigenvectors by descending eigenvalue\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Top principal component\n",
    "principal_vector = eigenvectors[:, 0]\n",
    "principal_variance = eigenvalues[0]\n",
    "\n",
    "# Step 3: Geometric Interpretation (Plotting top 2 PCs)\n",
    "Z = X_centered @ eigenvectors[:, :2]  # Project to 2D space\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(Z[:, 0], Z[:, 1], alpha=0.7)\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"Projection onto First 2 Principal Components\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Orthonormality of Eigenvectors\n",
    "orthonormal_check = np.allclose(eigenvectors.T @ eigenvectors, np.eye(eigenvectors.shape[1]))\n",
    "# This should return True\n",
    "\n",
    "# Diagonalization\n",
    "Sigma_diag = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n",
    "diagonalization_check = np.allclose(Sigma_diag, cov_matrix)\n",
    "# This should return True\n",
    "\n",
    "# Step 5: Dimensionality Reduction\n",
    "k = 2  # reduce to 2 dimensions\n",
    "V_k = eigenvectors[:, :k]\n",
    "Z_k = X_centered @ V_k  # Reduced data (n x k)\n",
    "\n",
    "# Step 6: Reconstruction from PCA\n",
    "X_approx = Z_k @ V_k.T + X.mean(axis=0)\n",
    "\n",
    "# Step 7: Explained Variance\n",
    "total_variance = np.sum(eigenvalues)\n",
    "explained_variance_ratio = np.cumsum(eigenvalues) / total_variance\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o')\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
    "plt.title(\"Explained Variance vs Number of Components\")\n",
    "plt.grid(True)\n",
    "plt.axhline(0.95, color='r', linestyle='--', label='95% threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Now you've implemented and visualized all 7 steps of PCA from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2761e3a-91ca-408d-ac09-206846f3a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load Iris dataset\n",
    "X = load_iris().data\n",
    "print(\"Original Data Shape:\", X.shape)  # (150, 4)\n",
    "\n",
    "# Step 1: Mean Centering\n",
    "mean_vector = np.mean(X, axis=0)\n",
    "X_centered = X - mean_vector\n",
    "print(\"\\nMean Vector Shape:\", mean_vector.shape)  # (4,)\n",
    "print(\"Centered Data Shape:\", X_centered.shape)  # (150, 4)\n",
    "\n",
    "# Step 2: Covariance Matrix\n",
    "cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "print(\"\\nCovariance Matrix Shape:\", cov_matrix.shape)  # (4, 4)\n",
    "print(\"Covariance Matrix:\\n\", np.round(cov_matrix, 2))\n",
    "\n",
    "# Step 3: Eigen Decomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "print(\"\\nUnsorted Eigenvalues:\\n\", eigenvalues)\n",
    "print(\"Unsorted Eigenvectors Shape:\", eigenvectors.shape)  # (4, 4)\n",
    "\n",
    "# Step 4: Sort Eigenvalues and Eigenvectors (Descending)\n",
    "sorted_idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[sorted_idx]\n",
    "eigenvectors = eigenvectors[:, sorted_idx]\n",
    "print(\"\\nSorted Eigenvalues:\\n\", eigenvalues)\n",
    "\n",
    "# Step 5: Select Top-k Eigenvectors (e.g., k=2)\n",
    "k = 2\n",
    "W = eigenvectors[:, :k]\n",
    "print(\"\\nProjection Matrix W Shape:\", W.shape)  # (4, 2)\n",
    "print(\"Top-2 Eigenvectors (W):\\n\", W)\n",
    "\n",
    "# Step 6: Project Data\n",
    "Z = np.dot(X_centered, W)\n",
    "print(\"\\nProjected Data Shape (Z):\", Z.shape)  # (150, 2)\n",
    "print(\"First 5 Projected Samples:\\n\", np.round(Z[:5], 3))\n",
    "\n",
    "# Step 7 (Optional): Reconstruct Approximate Original Data\n",
    "X_reconstructed = np.dot(Z, W.T) + mean_vector\n",
    "print(\"\\nReconstructed Data Shape:\", X_reconstructed.shape)\n",
    "print(\"First 5 Reconstructed Samples (Approx.):\\n\", np.round(X_reconstructed[:5], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468fca22-8b65-456d-bcff-a95cefe4ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Scree Plot – Explained Variance Ratio\n",
    "\n",
    "# This helps decide how many principal components to retain.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = eigenvalues / np.sum(eigenvalues)\n",
    "\n",
    "# Scree plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.arange(1, len(eigenvalues) + 1), explained_variance_ratio, 'o-', color='purple')\n",
    "plt.title('Scree Plot: Explained Variance Ratio')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xticks(np.arange(1, len(eigenvalues) + 1))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Cumulative variance\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.arange(1, len(eigenvalues) + 1), cumulative_variance, 'o-', color='green')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance')\n",
    "plt.xticks(np.arange(1, len(eigenvalues) + 1))\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0.95, color='red', linestyle='--', label='95% Variance Threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Original vs Reconstructed (2D Scatter Plot)\n",
    "\n",
    "# For a 2D PCA example, plot the first two original features vs reconstructed ones.\n",
    "\n",
    "# Only compare the first 2 features (just for 2D visualization)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], color='blue', alpha=0.5, label='Original')\n",
    "plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], color='red', alpha=0.5, label='Reconstructed (k=2)')\n",
    "plt.title('Original vs Reconstructed (First 2 Features)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These visuals will show:\n",
    "-\tHow many components are needed to retain most variance.\n",
    "-   How close the reconstruction is to the original data using reduced dimensions.\n",
    "\n"
   ],
   "id": "b8351390cd3abf44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cacee4-ea92-4743-ac7e-b1c5c260cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Step 1: Load Stock Data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Choose some stocks (diverse sectors)\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'JPM', 'TSLA', 'META', 'NVDA', 'XOM', 'UNH']\n",
    "\n",
    "# Download adjusted close prices\n",
    "data = pd.read_csv(\"stock_prices_2022_2023.csv\", index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "print(\"Price Data Shape:\", data.shape)\n",
    "data = data.dropna()\n",
    "print(\"Price Data Shape:\", data.shape)\n",
    "\n",
    "# Compute daily returns\n",
    "returns = data.pct_change().dropna()\n",
    "print(\"Daily Returns Shape:\", returns.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Mean Centering\n",
    "\n",
    "returns_centered = returns - returns.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Covariance Matrix and PCA\n",
    "\n",
    "# Covariance matrix\n",
    "cov_matrix = np.cov(returns_centered.T)\n",
    "\n",
    "# Eigen decomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Sort\n",
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "# Explained variance\n",
    "explained_variance_ratio = eigenvalues / np.sum(eigenvalues)\n",
    "\n",
    "# Project returns to top-2 components\n",
    "k = 2\n",
    "W = eigenvectors[:, :k]\n",
    "returns_pca = returns_centered @ W\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Scree Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.arange(1, len(eigenvalues) + 1), explained_variance_ratio, 'o-', color='darkorange')\n",
    "plt.title('Scree Plot: Explained Variance from Stock Returns')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d83900-ec5a-4da0-b17a-0e1ffdc08318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: 2D PCA Plot of Stock Clusters\n",
    "\n",
    "# This shows how stocks cluster by return behavior.\n",
    "\n",
    "# Transpose eigenvectors to get stock coordinates in 2D PCA space\n",
    "stock_coords_2d = W.T\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i, ticker in enumerate(tickers):\n",
    "    x, y = stock_coords_2d[0, i], stock_coords_2d[1, i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.text(x + 0.01, y + 0.01, ticker, fontsize=12)\n",
    "\n",
    "plt.title('Stocks in 2D PCA Space (Based on Return Covariance)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Step 6 (Optional): Reconstruct Returns\n",
    "\n",
    "# Approximate returns from top-k PCs\n",
    "returns_reconstructed = returns_pca @ W.T + returns.mean().values\n",
    "\n",
    "# Compare first few reconstructed vs original\n",
    "print(\"Original:\\n\", np.round(returns.iloc[:5, :3], 4))\n",
    "print(\"\\nReconstructed:\\n\", np.round(pd.DataFrame(returns_reconstructed, columns=returns.columns).iloc[:5, :3], 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Interpretation\n",
    "\t•\tThe scree plot shows how many components capture the majority of market movement.\n",
    "\t•\tThe 2D stock PCA plot groups stocks with similar return profiles.\n",
    "\t•\tThe reconstruction shows how well the market structure is retained with reduced dimensions.\n",
    "\n",
    "⸻\n",
    "\n",
    "What This Tells You:\n",
    "\t•\tEach cluster groups stocks that moved similarly over the date range.\n",
    "\t•\tPCA extracts dominant return patterns, and KMeans groups those patterns.\n",
    "\t•\tYou’ll likely see tech stocks cluster together, or energy stocks cluster separately.\n",
    "\n",
    "⸻\n",
    "\n"
   ],
   "id": "d34bf3f950607051"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa310c-cb4a-45c1-9d0c-60f90dd29852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f199afa-5c62-45ee-ac88-bb6079ccbf47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af33bff-3229-4eaa-91e9-00197f5116a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a8eee-51ad-417a-903d-484c233ad1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1374f05-d488-462e-a282-11d78aeb9e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f8e4f2-fde6-4db9-a7c0-8a5d2215e7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60332ea-1ba3-4779-841c-74bc62bf01d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
